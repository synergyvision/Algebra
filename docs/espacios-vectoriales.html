<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Álgebra Lineal</title>
  <meta name="description" content="Álgebra Lineal">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Álgebra Lineal" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://synergy.vision/Algebra/" />
  <meta property="og:image" content="http://synergy.vision/Algebra/images/cover.png" />
  
  <meta name="github-repo" content="synergyvision/Algebra/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Álgebra Lineal" />
  
  
  <meta name="twitter:image" content="http://synergy.vision/Algebra/images/cover.png" />

<meta name="author" content="Synergy Vision">


<meta name="date" content="2018-11-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="vectores.html">
<link rel="next" href="matrices-1.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="images/logovision-black.png" width="160"></img></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#por-que-leer-este-libro"><i class="fa fa-check"></i>¿Por qué leer este libro?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#estructura-del-libro"><i class="fa fa-check"></i>Estructura del libro</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#informacion-sobre-los-programas-y-convenciones"><i class="fa fa-check"></i>Información sobre los programas y convenciones</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practicas-interactivas-con-r"><i class="fa fa-check"></i>Prácticas interactivas con R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agradecimientos"><i class="fa fa-check"></i>Agradecimientos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acerca-del-autor.html"><a href="acerca-del-autor.html"><i class="fa fa-check"></i>Acerca del Autor</a></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html"><i class="fa fa-check"></i><b>2</b> Estructuras algebraicas</a><ul>
<li class="chapter" data-level="2.1" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#conjuntos"><i class="fa fa-check"></i><b>2.1</b> Conjuntos</a><ul>
<li class="chapter" data-level="2.1.1" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#definiciones-iniciales"><i class="fa fa-check"></i><b>2.1.1</b> Definiciones Iniciales</a></li>
<li class="chapter" data-level="2.1.2" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#operaciones-entre-conjuntos"><i class="fa fa-check"></i><b>2.1.2</b> Operaciones entre conjuntos</a></li>
<li class="chapter" data-level="2.1.3" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#producto-cartesiano"><i class="fa fa-check"></i><b>2.1.3</b> Producto cartesiano</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#relaciones-de-equivalencia"><i class="fa fa-check"></i><b>2.2</b> Relaciones de Equivalencia</a></li>
<li class="chapter" data-level="2.3" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#funciones"><i class="fa fa-check"></i><b>2.3</b> Funciones</a></li>
<li class="chapter" data-level="2.4" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#cardinales"><i class="fa fa-check"></i><b>2.4</b> Cardinales</a><ul>
<li class="chapter" data-level="2.4.1" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#ejercicios"><i class="fa fa-check"></i><b>2.4.1</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#teoria-de-grupos"><i class="fa fa-check"></i><b>2.5</b> Teoría de Grupos</a><ul>
<li class="chapter" data-level="2.5.1" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#ejercicios-1"><i class="fa fa-check"></i><b>2.5.1</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#anillos"><i class="fa fa-check"></i><b>2.6</b> Anillos</a><ul>
<li class="chapter" data-level="2.6.1" data-path="estructuras-algebraicas.html"><a href="estructuras-algebraicas.html#ejercicios-2"><i class="fa fa-check"></i><b>2.6.1</b> Ejercicios</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="vectores.html"><a href="vectores.html"><i class="fa fa-check"></i><b>3</b> Vectores</a><ul>
<li class="chapter" data-level="3.1" data-path="vectores.html"><a href="vectores.html#representacion-geometrica"><i class="fa fa-check"></i><b>3.1</b> Representación geométrica</a></li>
<li class="chapter" data-level="3.2" data-path="vectores.html"><a href="vectores.html#operaciones-entre-vectores"><i class="fa fa-check"></i><b>3.2</b> Operaciones entre vectores</a><ul>
<li class="chapter" data-level="3.2.1" data-path="vectores.html"><a href="vectores.html#producto-escalar-y-producto-vectorial"><i class="fa fa-check"></i><b>3.2.1</b> Producto escalar y producto vectorial</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="vectores.html"><a href="vectores.html#rectas-y-planos"><i class="fa fa-check"></i><b>3.3</b> Rectas y Planos</a><ul>
<li class="chapter" data-level="3.3.1" data-path="vectores.html"><a href="vectores.html#rectas-en-el-espacio"><i class="fa fa-check"></i><b>3.3.1</b> Rectas en el espacio</a></li>
<li class="chapter" data-level="3.3.2" data-path="vectores.html"><a href="vectores.html#rectas-en-el-plano"><i class="fa fa-check"></i><b>3.3.2</b> Rectas en el plano</a></li>
<li class="chapter" data-level="3.3.3" data-path="vectores.html"><a href="vectores.html#planos"><i class="fa fa-check"></i><b>3.3.3</b> Planos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="espacios-vectoriales.html"><a href="espacios-vectoriales.html"><i class="fa fa-check"></i><b>4</b> Espacios vectoriales</a><ul>
<li class="chapter" data-level="4.1" data-path="espacios-vectoriales.html"><a href="espacios-vectoriales.html#ecuaciones-lineales"><i class="fa fa-check"></i><b>4.1</b> Ecuaciones lineales</a></li>
<li class="chapter" data-level="4.2" data-path="espacios-vectoriales.html"><a href="espacios-vectoriales.html#matrices"><i class="fa fa-check"></i><b>4.2</b> Matrices</a></li>
<li class="chapter" data-level="4.3" data-path="espacios-vectoriales.html"><a href="espacios-vectoriales.html#ecuaciones-lineales-y-matrices"><i class="fa fa-check"></i><b>4.3</b> Ecuaciones lineales y matrices</a></li>
<li class="chapter" data-level="4.4" data-path="espacios-vectoriales.html"><a href="espacios-vectoriales.html#teoria-de-espacios-vectoriales"><i class="fa fa-check"></i><b>4.4</b> Teoría de espacios vectoriales</a><ul>
<li class="chapter" data-level="4.4.1" data-path="espacios-vectoriales.html"><a href="espacios-vectoriales.html#espacio-cociente"><i class="fa fa-check"></i><b>4.4.1</b> Espacio cociente</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="espacios-vectoriales.html"><a href="espacios-vectoriales.html#ejercicios-3"><i class="fa fa-check"></i><b>4.5</b> Ejercicios:</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="matrices-1.html"><a href="matrices-1.html"><i class="fa fa-check"></i><b>5</b> Matrices</a></li>
<li class="chapter" data-level="6" data-path="autovalores-y-autovectores.html"><a href="autovalores-y-autovectores.html"><i class="fa fa-check"></i><b>6</b> Autovalores y autovectores</a></li>
<li class="chapter" data-level="7" data-path="calculo-en-vectores-y-matrices.html"><a href="calculo-en-vectores-y-matrices.html"><i class="fa fa-check"></i><b>7</b> Cálculo en vectores y matrices</a></li>
<li class="chapter" data-level="8" data-path="transformaciones-lineales.html"><a href="transformaciones-lineales.html"><i class="fa fa-check"></i><b>8</b> Transformaciones lineales</a></li>
<li class="chapter" data-level="9" data-path="producto-escalar.html"><a href="producto-escalar.html"><i class="fa fa-check"></i><b>9</b> Producto escalar</a></li>
<li class="appendix"><span><b>Apéndice</b></span></li>
<li class="chapter" data-level="A" data-path="software-tools.html"><a href="software-tools.html"><i class="fa fa-check"></i><b>A</b> Software Tools</a><ul>
<li class="chapter" data-level="A.1" data-path="software-tools.html"><a href="software-tools.html#r-and-r-packages"><i class="fa fa-check"></i><b>A.1</b> R and R packages</a></li>
<li class="chapter" data-level="A.2" data-path="software-tools.html"><a href="software-tools.html#pandoc"><i class="fa fa-check"></i><b>A.2</b> Pandoc</a></li>
<li class="chapter" data-level="A.3" data-path="software-tools.html"><a href="software-tools.html#latex"><i class="fa fa-check"></i><b>A.3</b> LaTeX</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Álgebra Lineal</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="espacios-vectoriales" class="section level1">
<h1><span class="header-section-number">Capítulo 4</span> Espacios vectoriales</h1>
<p>En este capítulo estudiaremos los espacios vectoriales, un concepto abstrato de una estructura cerrada bajo combinaciones lineales. Veremos que las matrices son objetos muy útiles para trabajar en espacios vectoriales. Por esta razón estudiaremos algunos aspectos básicos de estos objetos. Como motivación para el estudio de las matrices iniciaremos el capítulo estudiando las ecuaciones lineales y poco a poco iremos acercándonos al estudio de los espacios vectoriales.</p>
<div id="ecuaciones-lineales" class="section level2">
<h2><span class="header-section-number">4.1</span> Ecuaciones lineales</h2>
<p>En este sección estudiaremos las ecuaciones lineales como motivación para el estudio de los conceptos iniciales de matrices. Además estudiaremos las operaciones básicas de suma y producto por un escalar así como el producto de matrices. Luego estudiaremos el concepto de invertibilidad de matrices, para ello definiremos las operaciones elementales por filas.</p>
Sea <span class="math inline">\(\mathbb{F}\)</span> un cuerpo. Supongamos que queremos hallar <span class="math inline">\(n\)</span> escalares (elementos del cuerpo <span class="math inline">\(\mathbb{F}\)</span>) <span class="math inline">\(x_{1},x_{2}, \cdots, x_{n}\)</span> que satisfagan las condiciones:
<span class="math display" id="eq:sistemalineal">\[\begin{equation}
    \begin{array}{ccccc}
        A_{11}x_{1}+&amp;A_{12}x_{2}+&amp;\cdots +&amp;A_{1n}x_{n}=&amp;b_{1}\\
        A_{21}x_{1}+&amp;A_{22}x_{2}+&amp;\cdots +&amp;A_{2n}x_{n}=&amp;b_{2}\\
        \vdots&amp; &amp;\ddots&amp; \vdots&amp; \vdots \\
        A_{m1}x_{1}+&amp;A_{m2}x_{2}+&amp;\cdots +&amp;A_{mn}x_{n}=&amp;b_{m}
    \end{array}
    \tag{4.1}
\end{equation}\]</span>
<p>donde <span class="math inline">\(b_{i}\in\mathbb{F}\)</span> así como <span class="math inline">\(A_{ij}\in \mathbb{F}\)</span> para todo <span class="math inline">\(1\leq i\leq m\)</span> y <span class="math inline">\(1\leq j\leq n\)</span>. Al conjunto de ecuaciones expresadas en <a href="espacios-vectoriales.html#eq:sistemalineal">(4.1)</a> se le llama <em>sistema de <span class="math inline">\(m\)</span> ecuaciones lineales con <span class="math inline">\(n\)</span> incognitas</em>. A los elementos <span class="math inline">\(A_{ij}\)</span> se les conoce como <em>coeficientes</em> del sistema de ecuaciones, siendo específicamente el coeficiente de la <span class="math inline">\(i\)</span>-ésima fila y la <span class="math inline">\(j\)</span>-ésima columna. Una <em>solución</em> del sistema es una <span class="math inline">\(n\)</span>-tupla <span class="math inline">\((x_{1},x_{2},\cdots,x_{n})\)</span> (un vector del espacio <span class="math inline">\(\mathbb{R}^{n}\)</span>) que satisfaga las ecuaciones <a href="espacios-vectoriales.html#eq:sistemalineal">(4.1)</a>. Cuando <span class="math inline">\(b_{1}=b_{2}=\cdots b_{m}=0\)</span> se dice que el <em>sistema de ecuaciones es homogéneo</em> (cada ecuación es homogénea).</p>
<p>Una forma de resolver un sistema de ecuaciones es con la técnica de eliminación de incógnitas, el cual consiste en multiplicar algunas de las ecuaciones por un escalar de forma que al sumar las ecuaciones se elimine algunas de las incógnitas. Veamos esto con un ejemplo.</p>

<div class="example">
<span id="exm:unnamed-chunk-211" class="example"><strong>Ejemplo 4.1  </strong></span> Dado el siguiente sistema homogéneo, sobre el cuerpo de los números reales <span class="math inline">\(\mathbb{R}\)</span>
<span class="math display">\[\begin{eqnarray*}
    \begin{array}{cc}
        x_{1}-4x_{2}+x_{3}&amp;=0\\
        3x_{1}-11x_{2}+2x_{3}&amp;=0
    \end{array}
    \end{eqnarray*}\]</span>
<p>Multiplicamos la primera ecuación por el escalar <span class="math inline">\(-3\)</span> y la sumamos a la segunda ecuación, para obtener <span class="math inline">\(-3(x_{1}-4x_{2}+x_{3})+(3x_{1}-11x_{2}+2x_{3})=0\)</span>, lo que queda como la siguiente <span class="math inline">\(-3x_{1}+12x_{2}-3x_{3}+3x_{1}-11x_{2}+2x_{3}=0\)</span>, sumando términos independientes, obtenemos <span class="math inline">\(x_{2}-x_{3}=0\)</span> por lo tanto <span class="math inline">\(x_{2}=x_{3}\)</span>. Ahora, multiplicando por <span class="math inline">\(-2\)</span> la primera ecuación y sumándola a la segunda, se obtiene <span class="math inline">\(-2(x_{1}-4x_{2}+x_{3})+(3x_{1}-11x_{2}+2x_{3})=0\)</span> lo que equivale a <span class="math inline">\(-2x_{1}+8x_{2}-2x_{3}+3x_{1}-11x_{2}+2x_{3}=0\)</span> al sumar los términos semejantes se obtiene que <span class="math inline">\(x_{1}-3x_{2}=0\)</span> por lo que <span class="math inline">\(x_{1}=3x_{2}\)</span>. Luego cualquier vector de la forma <span class="math inline">\((3\lambda,\lambda,\lambda)\)</span> con <span class="math inline">\(\lambda\in\mathbb{R}\)</span> es una solución del sistema homogéneo.</p>
</div>

En general, este método para resolver un sistema de ecuaciones como <a href="espacios-vectoriales.html#eq:sistemalineal">(4.1)</a> consiste en multiplicar por <span class="math inline">\(m\)</span> escalares <span class="math inline">\(c_{1}, c_{2},\cdots ,c_{m}\)</span> cada ecuación del sistema y sumarlas entre si para obtener una <em>combinación lineal</em> de las ecuaciones:
<span class="math display">\[\begin{equation}
\begin{array}{ccccccc}
&amp;c_{1}(A_{11}x_{1}&amp;+&amp;\cdots &amp;+&amp;A_{1n}x_{n})=&amp;c_{1}b_{1}\\
&amp;c_{2}(A_{21}x_{1}&amp;+&amp;\cdots &amp;+&amp;A_{2n}x_{n})=&amp;c_{2}b_{2}\\
+&amp;\vdots&amp; &amp;\ddots&amp; &amp;\vdots&amp; \vdots  \\
&amp;c_{m}(A_{m1}x_{1}&amp;+&amp;\cdots &amp;+&amp;A_{mn}x_{n})=&amp;c_{m}b_{m}\\
\cline{2-7}
&amp;(c_{1}A_{11}+\cdots+c_{m}A_{m1})x_{1}&amp;+&amp;\cdots&amp;+&amp;(c_{1}A_{1n}+\cdots +c_{m}A_{mn})x_{n}&amp;=c_{1}b_{1}+\cdots+c_{m}b_{m}
\end{array}
\end{equation}\]</span>
Es claro que cualquier solución del sistema <a href="espacios-vectoriales.html#eq:sistemalineal">(4.1)</a> es solución de la combinación lineal antes descrita. Ahora bien, si formamos un sistema de <span class="math inline">\(k\)</span> ecuaciones lineales en las que cada una de ellas es una combinación lineal de las <span class="math inline">\(m\)</span> ecuaciones del sistema original, como sigue:
<span class="math display" id="eq:sistema2">\[\begin{equation}
\begin{array}{ccccc}
B_{11}x_{1}+&amp;B_{12}x_{2}+&amp;\cdots +&amp;B_{1n}x_{n}=&amp;d_{1}\\
B_{21}x_{1}+&amp;B_{22}x_{2}+&amp;\cdots +&amp;B_{2n}x_{n}=&amp;d_{2}\\
\vdots&amp; &amp;\ddots&amp; \vdots&amp; \vdots \\
B_{k1}x_{1}+&amp;B_{k2}x_{2}+&amp;\cdots +&amp;B_{kn}x_{n}=&amp;d_{k}
\end{array}
\end{equation}\]</span>
<p>\tag{4.2}</p>
<p>se tiene que <span class="math inline">\((x_{1},x_{2},\cdots, x_{n})\)</span> es solución de <a href="espacios-vectoriales.html#eq:sistema2">(4.2)</a> si lo es del sistema <a href="espacios-vectoriales.html#eq:sistemalineal">(4.1)</a>. Lo contrario no es necesariamente cierto, sin embargo si las ecuaciones de <a href="espacios-vectoriales.html#eq:sistemalineal">(4.1)</a> son combinación lineal de las ecuaciones <a href="espacios-vectoriales.html#eq:sistema2">(4.2)</a> entonces podemos estar seguros que toda solución del sistema <a href="espacios-vectoriales.html#eq:sistema2">(4.2)</a> es también solución de <a href="espacios-vectoriales.html#eq:sistemalineal">(4.1)</a>. En este caso diremos que <em>son sistemas de ecuaciones equivalentes</em>. Y la observación podemos señalarla así:</p>

<div class="theorem">
<span id="thm:unnamed-chunk-212" class="theorem"><strong>Teorema 4.1  </strong></span> Sistemas de ecuaciones lineales equivalentes tienen exactamente las mismas soluciones.
</div>

<p>Lo anterior nos permite buscar las soluciones de cualquier sistema de ecuaciones lineales, buscando un sistema equivalente que sea mas fácil de resolver (por excelencia, trivial). Este método lo explicaremos en la siguiente sección.</p>
</div>
<div id="matrices" class="section level2">
<h2><span class="header-section-number">4.2</span> Matrices</h2>
<p>En la sección anterior vimos que cuando realizamos combinaciones lineales de ecuaciones lineales, lo que importa son los coeficientes de las ecuaciones originales, siendo las incógnitas prescindibles. Esto nos permite trabajar directamente con los coeficientes para hallar un nuevo sistema lineal equivalente mas sencillo. Por esta razón arreglaremos tales coeficientes en forma rectángular para trabajar con ellos de forma directa. Estos objetos se llaman <em>matrices</em>. En la primera parte de esta sección se dará una definición formal y mas general de estos objetos, se definiran algunas operaciones con estos objetos. Para luego volver al problema original, la resolución de sistemas de ecuaciones lineales.</p>

<div class="definition">
<span id="def:unnamed-chunk-213" class="definition"><strong>Definición 4.1  </strong></span> Para enteros positivos <span class="math inline">\(m\)</span> y <span class="math inline">\(n\)</span>, <em>una matriz <span class="math inline">\(m\times n\)</span> sobre el cuerpo <span class="math inline">\(\mathbb{F}\)</span></em> es una función <span class="math inline">\(A\)</span> del conjunto de los pares <span class="math inline">\((i,j)\in\{1,2,\cdots m\}\times\{1,2,\cdots n\}\)</span> en el cuerpo <span class="math inline">\(\mathbb{F}\)</span>. El <em>orden de la matriz <span class="math inline">\(A\)</span></em> es <span class="math inline">\(m\times n\)</span>. Los <em>elementos de la matriz <span class="math inline">\(A\)</span></em> son los escalares <span class="math inline">\(A(i,j)=a_{ij}\)</span>. Suele representarse como un arreglo rectángular de <span class="math inline">\(m\)</span> filas y <span class="math inline">\(n\)</span> columnas, donde el elemento <span class="math inline">\(a_{ij}\)</span> ocupa el puesto en la fila <span class="math inline">\(i\)</span> y la columna <span class="math inline">\(j\)</span> del arreglo, como sigue
<span class="math display">\[\begin{equation*}
        \left[ \begin{array}{cccc}
        a_{11}&amp;a_{12}&amp;\cdots &amp;a_{1n}\\
        a_{21}&amp;a_{22}&amp;\cdots &amp;a_{2n}\\
        \vdots&amp; \ddots&amp; \vdots&amp; \vdots\\
        a_{m1}&amp;a_{m2}&amp;\cdots &amp;a_{mn}
        \end{array}\right] 
    \end{equation*}\]</span>
<p>La <em><span class="math inline">\(i\)</span>-ésima fila de la matriz <span class="math inline">\(A\)</span></em> es el arreglo <span class="math inline">\(A_{i*}=[a_{i1}\; a_{i2}\;\cdots\; a_{in}]\)</span> (puede entenderse como un vector de <span class="math inline">\(\mathbb{R}^{n}\)</span>) y la  es el arreglo <span class="math inline">\(A_{*j}=\left[ \begin{array}{c}a_{1j}\\  a_{2j}\\  \vdots\\  a_{mj}  \end{array}\right]\)</span>.</p>
</div>


<div class="definition">
<span id="def:unnamed-chunk-214" class="definition"><strong>Definición 4.2  </strong></span> Dadas las matrices <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> de orden <span class="math inline">\(n\times m\)</span>, sobre el cuerpo <span class="math inline">\(\mathbb{F}\)</span>, <em>la suma de las matrices <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span></em>, es la matriz <span class="math inline">\(A+B\)</span> formada por los elementos <span class="math inline">\((a+b)_{ij}=a_{ij}+b_{ij}\)</span> (la función suma <span class="math inline">\((A+B)(i,j)=A(i,j)+B(i,j)\)</span>). También se puede expresar como:
<span class="math display">\[\begin{equation*}
        \left[ \begin{array}{cccc}
            a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
            a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
            a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
        \end{array}\right]+
        \left[ \begin{array}{cccc}
        b_{11} &amp; b_{12} &amp; \cdots &amp; b_{1n}\\
        b_{21} &amp; b_{22} &amp; \cdots &amp; b_{2n}\\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
        b_{m1} &amp; b_{m2} &amp; \cdots &amp; b_{mn}\\
        \end{array}\right]=
        \left[ \begin{array}{cccc}
        a_{11} + b_{11}&amp; a_{12} + b_{12}&amp; \cdots &amp; a_{1n}+ b_{1n}\\
        a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; \cdots &amp; a_{2n} + b_{2n}\\
        \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
        a_{m1} + b_{m1} &amp; a_{m2} + b_{m2} &amp; \cdots &amp; a_{mn} + b_{mn}\\
        \end{array}\right]
    \end{equation*}\]</span>
</div>
 
<div class="definition">
<span id="def:unnamed-chunk-215" class="definition"><strong>Definición 4.3  </strong></span> Dada una matriz <span class="math inline">\(A\)</span> de orden <span class="math inline">\(m\times n\)</span> sobre el cuerpo <span class="math inline">\(\mathbb{F}\)</span> y un escalar <span class="math inline">\(\lambda\in\mathbb{F}\)</span>. El <em>producto de la matriz <span class="math inline">\(A\)</span> por el escalar <span class="math inline">\(\lambda\)</span></em> es la matriz <span class="math inline">\(\lambda A\)</span>, donde cada elemento <span class="math inline">\([\lambda A]_{ij}=\lambda a_{ij}\)</span>, es decir:
<span class="math display">\[\begin{equation*}
        \lambda A=
        \lambda \left[ \begin{array}{cccc}
            a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
            a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
            a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}\\
        \end{array}\right]=
        \left[ \begin{array}{cccc}
            \lambda a_{11}&amp; \lambda a_{12}&amp; \cdots &amp; \lambda a_{1n}\\
            \lambda a_{21}&amp; \lambda a_{22}&amp; \cdots &amp; \lambda a_{2n}\\
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
            \lambda a_{m1}&amp; \lambda a_{m2}&amp; \cdots &amp; \lambda a_{mn}\\
        \end{array}\right]
    \end{equation*}\]</span>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-216" class="definition"><strong>Definición 4.4  </strong></span> Sean <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> matrices de orden <span class="math inline">\(m\times n\)</span> y <span class="math inline">\(n\times p\)</span> respectivamente, el <em>producto <span class="math inline">\(AB\)</span></em> es la matriz <span class="math inline">\(C\)</span> de orden <span class="math inline">\(m\times p\)</span> cuyos elementos <span class="math inline">\(ij\)</span> son <span class="math inline">\([C]_{ij}=\sum_{k=1}^{n} A_{ik}B_{kj}\)</span>.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-217" class="example"><strong>Ejemplo 4.2  </strong></span> Dadas las matrices <span class="math inline">\(A=\left[ \begin{array}{cc}  1 &amp; 0 \\  -3 &amp; 1  \end{array}\right]\)</span>, <span class="math inline">\(B=\left[ \begin{array}{ccc}  5 &amp; -1 &amp; 2 \\  15 &amp; 4 &amp; 8  \end{array}\right]\)</span> y <span class="math inline">\(C=\left[ \begin{array}{ccc}  -2 &amp; 1 &amp; 5\\  -1 &amp; -5 &amp; 1  \end{array}\right]\)</span>, entonces el producto de la matriz <span class="math inline">\(C\)</span> por el escalar <span class="math inline">\(-2\)</span> es: <span class="math display">\[-2 C=\left[ \begin{array}{ccc}
    (-2)(-2) &amp; (-2)(1) &amp; (-2)(5)\\
    (-2)(-1) &amp; (-2)(-5) &amp; (-2)(1)
    \end{array}\right]=\left[ \begin{array}{ccc}
    4 &amp; -2 &amp; -10\\
    2 &amp; 10 &amp; -2
    \end{array}\right]\]</span>. El producto <span class="math inline">\(AB\)</span> es la matriz: <span class="math display">\[\left[ \begin{array}{cc}
    1 &amp; 0 \\
    -3 &amp; 1
    \end{array}\right]\left[ \begin{array}{ccc}
    5 &amp; -1 &amp; 2 \\
    15 &amp; 4 &amp; 8
    \end{array}\right]=\left[ \begin{array}{ccc}
        5 &amp; -1 &amp; 2\\
        0 &amp; 7 &amp; 2
    \end{array}\right]\]</span> Y la suma <span class="math display">\[AB+C=\left[ \begin{array}{ccc}
        3 &amp; 0 &amp; 7\\
        -1 &amp; 2 &amp; 3
    \end{array}\right]\]</span> Note que no podemos realizar el producto <span class="math inline">\(BA\)</span>, pues no está definido. Para realizar un producto de matrices, el número de columnas del primer factor debe ser igual al número de filas del segundo factor.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-218" class="example"><strong>Ejemplo 4.3  </strong></span> Dadas las matrices <span class="math inline">\(A=\left[ \begin{array}{cc}  1 &amp; -2 \\  0 &amp; 1 \\  -2 &amp; 0  \end{array}\right]\)</span> y <span class="math inline">\(B=\left[ \begin{array}{ccc}  1 &amp; 0 &amp; 1 \\  0 &amp; 2 &amp; 1  \end{array}\right]\)</span>, entonces los productos: <span class="math inline">\(AB=\left[ \begin{array}{ccc}  1 &amp; -4 &amp; -1\\  0 &amp; 2 &amp; 1 \\  -2 &amp; 0 &amp; -2  \end{array}\right]\)</span> y <span class="math inline">\(BA=\left[ \begin{array}{cc}  -1 &amp; -2 \\  -2 &amp; 2  \end{array}\right]\)</span>.\ Por otro lado, si se tienen las matrices <span class="math inline">\(C=\left[ \begin{array}{cc}  1 &amp; -2 \\  3 &amp; -1 \\  \end{array}\right]\)</span> y <span class="math inline">\(D=\left[ \begin{array}{cc}  1 &amp; -2 \\  -1 &amp; 2  \end{array}\right]\)</span>, entonces los productos: <span class="math inline">\(CD=\left[ \begin{array}{cc}  3 &amp; -6 \\  4 &amp; -7  \end{array}\right]\)</span> y <span class="math inline">\(DC=\left[ \begin{array}{cc}  -5 &amp; 0 \\  5 &amp; 0  \end{array}\right]\)</span></p>
</div>


<div class="remark">
<p> <span class="remark"><em>Nota. </em></span>  En los casos en que expresemos el producto <span class="math inline">\(AB\)</span> sin detallar el orden de las matrices, supondremos que el producto está bien definido. De los ejemplos anteriores podemos ver que aunque los productos <span class="math inline">\(AB\)</span> y <span class="math inline">\(BA\)</span> esten bien definidos, no necesariamente se tiene que <span class="math inline">\(AB=BA\)</span>, esto es, el producto de matrices no es conmutativo.</p>
</div>

<p>Ahora estudiaremos las <em>operaciones elementales de filas</em> que pueden aplicarse a una matriz, el fin de aplicar estas operaciones es obtener una matriz equivalente a la original (para obtener sistemas de ecuaciones lineales equivalentes) que corresponda a los coeficientes de un sistema lineal sencillo de resolver.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-220" class="definition"><strong>Definición 4.5  </strong></span> Una matriz <span class="math inline">\(R\)</span> de orden <span class="math inline">\(m\times n\)</span> se llama una <em>matriz escalón reducida por filas</em> si:</p>
<pre><code>    (1) El primer elemento no nulo de cada fila no nula es igual a $1$, al cual llamaremos *pivote*.

    (2) las columnas que contienen a un pivote (de cualquier fila), tienen el restos de sus elemento igual a cero.

    (3) toda fila nula de $R$, está debajo de las filas con elementos no nulos.

    (4) suponiendo que las filas no nulas de $R$ son las filas $1,2,\cdots, r$ y que el pivote de la fila $i\leq r$ está en la columna $k_{i}$, entonces $k_{1}&lt; k_{2}&lt; \cdots &lt; k_{r}$.</code></pre>
</div>


<div class="remark">
<p> <span class="remark"><em>Nota. </em></span>  Una matriz que cumpla las primeras dos condiciones se llama <em>matriz reducida por filas</em>.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-222" class="example"><strong>Ejemplo 4.4  </strong></span> La <em>matriz identidad</em> <span class="math inline">\(n\times n\)</span> (cuadrada), definida por la función <span class="math display">\[[I]_{ij}=delta_{ij}=\left\{ \begin{array}{cc}
    1 &amp;\mbox{ si } i=j\\
    0 &amp;\mbox{ si } i\neq j
    \end{array}\right. \]</span> La función <span class="math inline">\(\delta_{ij}\)</span> es conocida como la <em>delta de Kronecker</em>. Es de hacer notar que la matriz identidad se define como una matriz de cualquier orden, mientras que sea cuadrada, es decir, el número de filas es igual al número de columnas.Por ejemplo la matriz identidad de orden <span class="math inline">\(4\times 4\)</span> luce así: <span class="math display">\[\left[\begin{array}{cccc}
    1 &amp; 0 &amp; 0 &amp; 0\\
    0 &amp; 1 &amp; 0 &amp; 0\\
    0 &amp; 0 &amp; 1 &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 1
    \end{array} \right]\]</span> Las matrices <span class="math display">\[\left[\begin{array}{cccc}
    0 &amp; 1 &amp; 0 &amp; 4\\
    0 &amp; 0 &amp; 1 &amp; 5\\
    0 &amp; 0 &amp; 0 &amp; 0
    \end{array} \right] \mbox{ y }
    \left[\begin{array}{ccccc}
    0 &amp; 1 &amp; -3 &amp; 0 &amp; \frac{1}{2}\\
    0 &amp; 0 &amp; 0 &amp; 1 &amp; 2\\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
    \end{array} \right] \]</span> son matrices escalonadas reducidas. Pero estas matrices<br />
<span class="math display">\[\left[\begin{array}{cccc}
    0 &amp; 1 &amp; 0 &amp; 4\\
    0 &amp; 0 &amp; 1 &amp; 5\\
    0 &amp; 0 &amp; 0 &amp; 2
    \end{array} \right] \mbox{ y }\left[\begin{array}{cccc}
    0 &amp; 1 &amp; 0 &amp; \frac{4}{3}\\
    0 &amp; 0 &amp; 1 &amp; 5\\
    1 &amp; 0 &amp; 0 &amp; -1
    \end{array} \right]\]</span> no lo son. La primera no lo es ya que el primer elemento no nulo de la última fila no es <span class="math inline">\(1\)</span>. La segunda matriz no cumple la definición, el pivote de la tercera fila está en la columna <span class="math inline">\(1\)</span>, mientras que el de la primera primera fila está en la columna <span class="math inline">\(2\)</span> (<span class="math inline">\(2\nless 1\)</span>).</p>
</div>

<p>Podremos ver mas adelante que los sistemas de ecuaciones asociadas a matrices escalonadas reducidas son mas fáciles de resolver. Entonces es conveniente hallar una matriz escalonada reducida que tenga un sistema de ecuaciones equivalente al original para así resolver el sistema fácilmente.</p>
</div>
<div id="ecuaciones-lineales-y-matrices" class="section level2">
<h2><span class="header-section-number">4.3</span> Ecuaciones lineales y matrices</h2>
Dado un sistema de ecuaciones lineales como <a href="espacios-vectoriales.html#eq:sistemalineal">(4.1)</a>
<span class="math display">\[\begin{equation}
\begin{array}{ccccc}
A_{11}x_{1}+&amp;A_{12}x_{2}+&amp;\cdots +&amp;A_{1n}x_{n}=&amp;b_{1}\\
A_{21}x_{1}+&amp;A_{22}x_{2}+&amp;\cdots +&amp;A_{2n}x_{n}=&amp;b_{2}\\
\vdots&amp; &amp;\ddots&amp; \vdots&amp; \vdots \\
A_{m1}x_{1}+&amp;A_{m2}x_{2}+&amp;\cdots +&amp;A_{mn}x_{n}=&amp;b_{m}
\end{array}
\end{equation}\]</span>
Podemos representar este sistema de ecuaciones como un sistema matricial <span class="math inline">\(AX=B\)</span>, donde <span class="math inline">\(A\)</span> es la matriz de los coeficientes del sistema de ecuaciones, <span class="math inline">\(X\)</span> es una matriz de incógnitas y <span class="math inline">\(B\)</span> una matriz de términos independientes de la siguiente forma:
<span class="math display">\[\begin{equation}
    \left[ \begin{array}{cccc}
        A_{11}&amp;A_{12}&amp;\cdots &amp;A_{1n}\\
        A_{21}&amp;A_{22}&amp;\cdots &amp;A_{2n}\\
        \vdots&amp; \ddots&amp; \vdots&amp; \vdots\\
        A_{m1}&amp;A_{m2}&amp;\cdots &amp;A_{mn}
    \end{array}\right] 
    \cdot
    \left[ \begin{array}{c}
        x_{1}\\
        x_{2}\\
        \vdots\\
        x_{n}
    \end{array}\right] =
    \left[ \begin{array}{c}
        b_{1}\\
        b_{2}\\
        \vdots\\
        b_{n}
    \end{array}\right] 
\end{equation}\]</span>

<div class="theorem">
<p><span id="thm:unnamed-chunk-223" class="theorem"><strong>Teorema 4.2  </strong></span> Sean <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> y <span class="math inline">\(C\)</span> matrices sobre el cuerpo de escalares <span class="math inline">\(\mathbb{F}\)</span>. Supongamos que los productos <span class="math inline">\(BC\)</span> y <span class="math inline">\(A(BC)\)</span> están definidos. Entonces, <span class="math inline">\(AB\)</span> y <span class="math inline">\((AB)C\)</span> también están definidos y <span class="math inline">\(A(BC)=(AB)C\)</span>.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Como <span class="math inline">\(BC\)</span> y <span class="math inline">\(A(BC)\)</span> están definidos, se tiene que el número de columnas de <span class="math inline">\(B\)</span> es igual al número de filas de <span class="math inline">\(C\)</span> y que el número de columnas de <span class="math inline">\(A\)</span> es igual al número de filas de <span class="math inline">\(BC\)</span> (y por lo tanto, igual al número de filas de <span class="math inline">\(B\)</span>). Supongamos que <span class="math inline">\(B\)</span> es de orden <span class="math inline">\(n\times p\)</span>, <span class="math inline">\(C\)</span> es de orden <span class="math inline">\(p\times q\)</span> y <span class="math inline">\(A\)</span> de orden <span class="math inline">\(m\times n\)</span>, así <span class="math inline">\(A(BC)\)</span> es de orden <span class="math inline">\(m\times q\)</span>. Claramente, <span class="math inline">\(AB\)</span> está definida y será de orden <span class="math inline">\(m\times p\)</span>, como <span class="math inline">\(C\)</span> es de orden <span class="math inline">\(p\times q\)</span>, <span class="math inline">\((AB)C\)</span> está bien definido y será de orden <span class="math inline">\(m\times q\)</span>. Ahora veamos que los productos <span class="math inline">\(A(BC)\)</span> y <span class="math inline">\((AB)C\)</span> además de tener el mismo orden, coinciden en cada elemento. <span class="math inline">\(\begin{array}{rl}  [A(BC)]_{ij} &amp; =\sum_{r=1}^{n} [A]_{ir}[BC]_{rj}\\  &amp; =\sum_{r=1}^{n} [A]_{ir} \sum_{s=1}^{p} [B]_{rs} [C]_{sj}\\  &amp; =\sum_{r=1}^{n}\sum_{s=1}^{p} [A]_{ir}[B]_{rs} [C]_{sj}\\  &amp; =\sum_{s=1}^{p}\sum_{r=1}^{n} [A]_{ir}[B]_{rs} [C]_{sj}\\  &amp; =\sum_{s=1}^{p}(\sum_{r=1}^{n} [A]_{ir}[B]_{rs}) [C]_{sj}\\  &amp; =\sum_{s=1}^{p} [AB]_{is} [C]_{sj}\\  &amp; =[(AB)C]_{ij}  \end{array}\)</span></p>
</div>


<div class="remark">
<p> <span class="remark"><em>Nota. </em></span>  El producto de una matriz cuadrada <span class="math inline">\(A\)</span> de orden <span class="math inline">\(n\times n\)</span>, consigo misma, se puede denotar por <span class="math inline">\(A^{2}=AA\)</span>. Note que <span class="math inline">\(A^{2}\)</span> es de orden <span class="math inline">\(n\times n\)</span>, luego el producto <span class="math inline">\(AA^{2}\)</span> está defindo y se denotará <span class="math inline">\(A^{3}\)</span>. En general, el producto de cualquier matriz cuadrada consigo misma, <span class="math inline">\(r\)</span> veces, está definida y se denota <span class="math inline">\(A^{r}=AA\cdots A\)</span>.</p>
</div>

<p>Ahora veremos las operaciones elementales por filas que corresponden a hacer combinaciones lineales entre las filas de la matriz de coeficientes (equivalente a hacerlo con las ecuaciones del sistema). Las <em>\textit{</em>operaciones elementales por filas* son tres:</p>
<pre><code>(1) Multiplicar una fila de la matriz $A$ por un escalar no nulo $\lambda$.

(2) Intercambio de dos filas de la matriz $A$.

(3) Sustituír la $i$-\&#39;esima fila de la matriz $A$, por la suma de la fila $r$ mas un múltiplo de la fila $s$-ésima.</code></pre>
<p>Podemos denotar en forma de función (entre fila) las operaciones elementales por fila del siguiente modo. Si <span class="math inline">\(A\)</span> es una matriz <span class="math inline">\(m\times n\)</span>, una operación elemental de filas es una función <span class="math inline">\(e\)</span> que se le aplica a la matriz <span class="math inline">\(A\)</span>, asociándole la matriz <span class="math inline">\(e(A)\)</span>, que corresponde al resultado de alguna de las operaciones antes descritas. esto es:</p>
<pre><code>(1) Denotaremos $\lambda e_{r}$ a la operación 
$[e(A)]_{ij}=\left\{ \begin{array}{cc}
\lambda [A]_{ij} &amp; \mbox{ si } i=r \\
\left[A\right]_{ij} &amp; \mbox{ si } i\neq r
\end{array}\right.$, con $r\leq m$ y $\lambda\neq 0$.

(2) Denotaremos $e_{rs}$ a la operación 
$[e(A)]_{ij}=\left\{ \begin{array}{cc}
[A]_{sj} &amp; \mbox{ si } i=r \\
\left[ A\right]_{rj} &amp; \mbox{ si } i=s \\
\left[A\right]_{ij} &amp; \mbox{ en otro caso } 
\end{array}\right.$, con $r\neq s \leq m$.

(3) Denotaremos $\lambda e_{rs}$ a la operación 
$[e(A)]_{ij}=\left\{ \begin{array}{cc}
[A]_{ij}+\lambda [A]_{sj} &amp; \mbox{ si } i=r \\
\left[A\right]_{ij} &amp; \mbox{ si } i\neq r
\end{array}\right.$, con $r\neq s \leq m$.</code></pre>
<p>Note que cualquiera de las tres operaciones elementales por filas se puede “revertir” con una operación del mismo tipo. Para el primer tipo, basta con multiplicar la misma fila por el inverso de <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\frac{1}{\lambda}\)</span>. Para el intercambio de las filas <span class="math inline">\(r\)</span> y <span class="math inline">\(s\)</span> basta volver a intercambiar las filas. Para el tercer tipo de operación, <span class="math inline">\(\lambda A_{rs}\)</span>, debemos aplicar <span class="math inline">\(-\lambda A_{rs}\)</span> y regresaremos a la matriz original. Esto es la demostración del siguiente teorema.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-226" class="theorem"><strong>Teorema 4.3  </strong></span> Para cada operación elemental de filas <span class="math inline">\(e\)</span> existe una operación elemental de filas <span class="math inline">\(e_{1}\)</span> del mismo tipo tal que <span class="math inline">\(e_{1}(e(A))=e(e_{1}(A))=A\)</span>. Es decir, cada operación elemental de filas, tiene una operación inversa del mismo tipo.</p>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-227" class="definition"><strong>Definición 4.6  </strong></span> Si <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> son dos matrices del mismo orden sobre el mismo cuerpo de escalares y <span class="math inline">\(B\)</span> se obtiene de aplicar una cantidad finita de operaciones elementales por filas a la matriz <span class="math inline">\(A\)</span>, entonces decimos que <em><span class="math inline">\(B\)</span> es equivalente por filas a <span class="math inline">\(A\)</span></em>.</p>
</div>


<div class="remark">
<p> <span class="remark"><em>Nota. </em></span>  Del teorema anterior se puede verificar que si una matriz <span class="math inline">\(B\)</span> es equivalente por filas a otra matriz <span class="math inline">\(A\)</span>, entonces <span class="math inline">\(A\)</span> es equivalente por filas con <span class="math inline">\(B\)</span>. También se puede ver que toda matriz es equivalente por filas a si misma. Por último se puede demostrar que si <span class="math inline">\(A\)</span> es equivalente por filas a <span class="math inline">\(B\)</span> y <span class="math inline">\(B\)</span> es equivalente por filas a <span class="math inline">\(C\)</span>, entonces <span class="math inline">\(A\)</span> es equivalente por filas a <span class="math inline">\(C\)</span>. De lo anterior, se tiene que la equivalencia por filas es una relación de equivalencia.</p>
</div>
<p> Como ya lo hemos mencionado, aplicar una operación elemental por filas es equivalente a hacer combinaciones lineales con las ecuaciones del sistema, por lo tanto al obtener matrices equivalentes por filas tendremos sistemas de ecuaciones equivalentes.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-229" class="theorem"><strong>Teorema 4.4  </strong></span> Si <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> son matrices equivalentes por filas, los sistemas homogeneos de ecuaciones lineales <span class="math inline">\(AX=0\)</span> y <span class="math inline">\(BX=0\)</span> tinen exactamente las mismas soluciones.</p>
</div>
 
<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Basta suponer que <span class="math inline">\(B\)</span> se obtiene de aplicar una operación elemental <span class="math inline">\(e\)</span> a la matriz <span class="math inline">\(A\)</span>. Luego, las ecuaciones del sistema <span class="math inline">\(BX=0\)</span> son combinaciones lineales de las ecuaciones del sistema <span class="math inline">\(AX=0\)</span>, por lo que cada solución de <span class="math inline">\(AX=0\)</span> es solución de <span class="math inline">\(BX=0\)</span>. Análogamente, cada solución de <span class="math inline">\(BX=0\)</span> es solución de <span class="math inline">\(AX=0\)</span>, ya que <span class="math inline">\(A\)</span> se obtiene al aplicar la operación elemental inversa de <span class="math inline">\(e\)</span> a la matriz <span class="math inline">\(B\)</span>.</p>
</div>
 
<div class="example">
<p><span id="exm:unnamed-chunk-231" class="example"><strong>Ejemplo 4.5  </strong></span> Dada la matriz de coeficientes <span class="math inline">\(A=\left[ \begin{array}{cccc}  2 &amp; -1 &amp; 3 &amp; 2 \\  1 &amp; 4 &amp; 0 &amp; -1 \\  2 &amp; 6 &amp; -1 &amp; 5  \end{array}\right]\)</span> podemos hallar una matriz escalonada reducida por fila equivalente a <span class="math inline">\(A\)</span>, de la siguiente forma:</p>
<pre><code>$\left[  \begin{array}{cccc}
2 &amp; -1 &amp; 3 &amp; 2 \\
1 &amp; 4 &amp; 0 &amp; -1 \\
2 &amp; 6 &amp; -1 &amp; 5
\end{array}\right]   \stackrel{-2 e_{21}}{\longrightarrow}
\left[  \begin{array}{cccc}
0 &amp; -9 &amp; 3 &amp; 4 \\
1 &amp; 4 &amp; 0 &amp; -1 \\
2 &amp; 6 &amp; -1 &amp; 5
\end{array}\right]  \stackrel{-2 e_{23}}{\longrightarrow}
\left[ 
\begin{array}{cccc}
0 &amp; -9 &amp; 3 &amp; 4 \\
1 &amp; 4 &amp; 0 &amp; -1 \\
0 &amp; -2 &amp; -1 &amp; 7
\end{array}\right] \stackrel{\frac{-1}{2} e_{3}}{\longrightarrow}
\left[ 
\begin{array}{cccc}
    0 &amp; -9 &amp; 3 &amp; 4 \\
    1 &amp; 4 &amp; 0 &amp; -1 \\
    0 &amp; 1 &amp; \frac{1}{2} &amp; \frac{-7}{2}
\end{array}\right] \stackrel{-4 e_{32}}{\longrightarrow}
\left[ 
\begin{array}{cccc}
    0 &amp; -9 &amp; 3 &amp; 4 \\
    1 &amp; 0 &amp; -2 &amp; 13 \\
    0 &amp; 1 &amp; \frac{1}{2} &amp; \frac{-7}{2}
\end{array}\right] \stackrel{9 e_{31}}{\longrightarrow}
\left[ 
\begin{array}{cccc}
    0 &amp; 0 &amp; \frac{15}{2} &amp; \frac{-55}{2} \\
    1 &amp; 0 &amp; -2 &amp; 13 \\
    0 &amp; 1 &amp; \frac{1}{2} &amp; \frac{-7}{2}
\end{array}\right] \stackrel{\frac{2}{15} e_{1}}{\longrightarrow}</code></pre>
<p>    $</p>
<p>De donde se tiene que los sistemas de ecuaciones lineales</p>
$
<span class="math display">\[\begin{array}{ccccc}
2x_{1} &amp; -x_{2} &amp; +3x_{3} &amp; +2x_{4} &amp; =0\\
x_{1} &amp; +4x_{2} &amp;  &amp; -x_{4} &amp;=0\\
2x_{1} &amp; +6x_{2} &amp; -x_{3} &amp; +5x_{4} &amp; =0
\end{array}\]</span>
<p>. $ y</p>
$ 
<span class="math display">\[\begin{array}{ccccc}
    x_{1} &amp;  &amp;  &amp; +\frac{17}{3}x_{4} &amp;=0 \\
 &amp; x_{2} &amp;  &amp; +\frac{-5}{3}x_{4} &amp;=0 \\
  &amp;  &amp; x_{3} &amp; +\frac{-11}{3}x_{4} &amp;=0 
\end{array}\]</span>
<p>. $ son equivalentes y por lo tanto tienen las mismas soluciones. Del segundo sistema salta a la vista que una solución es de la forma <span class="math inline">\((\frac{-17}{3}\lambda,\frac{5}{3}\lambda,\frac{11}{3}\lambda,\lambda)\)</span> para cualquier número real <span class="math inline">\(\lambda\)</span>.</p>
</div>


<div class="example">
<span id="exm:unnamed-chunk-232" class="example"><strong>Ejemplo 4.6  </strong></span> Dada la ecuación con coeficientes en el cuerpo de los números complejos <span class="math inline">\(\mathbb{C}\)</span>: $ 
<span class="math display">\[\begin{array}{ccc}
    -x_{1} &amp; +ix_{2} &amp;=0\\
    -ix_{1} &amp; +3x_{2} &amp;=0\\
    x_{1} &amp; +2x_{2} &amp; =0
    \end{array}\]</span>
<p>. $</p>
<pre><code>La matriz de coeficientes es $A=\left[  \begin{array}{cc}
-1 &amp; i \\
-i &amp; 3 \\
1 &amp; 2 
\end{array}\right]$$ 
Hallemos una matriz escalonada reducida por fila equivalente a $A$:
$$\left[  \begin{array}{cc}
-1 &amp; i \\
-i &amp; 3 \\
1 &amp; 2 
\end{array}\right]   \stackrel{e_{13}}{\longrightarrow}
\left[  \begin{array}{cc}
1 &amp; 2 \\
-i &amp; 3 \\
-1 &amp; i 
\end{array}\right]  \stackrel{i e_{12}}{\longrightarrow}
\left[ 
\begin{array}{cc}
1 &amp; 2 \\
0 &amp; 3+2i \\
-1 &amp; i 
\end{array}\right] \stackrel{1 e_{13}}{\longrightarrow}
\left[ 
\begin{array}{cc}
1 &amp; 2 \\
0 &amp; 3+2i \\
0 &amp; 2+i 
\end{array}\right] \stackrel{\frac{1}{3+2i} e_{2}}{\longrightarrow}$

$\left[ 
\begin{array}{cc}
1 &amp; 2 \\
0 &amp; 1 \\
0 &amp; 2+i 
\end{array}\right] \stackrel{-2 e_{21}}{\longrightarrow}
\left[ 
\begin{array}{cc}
1 &amp; 0 \\
0 &amp; 1 \\
0 &amp; 2+i 
\end{array}\right] \stackrel{-2-i e_{23}}{\longrightarrow}
\left[ 
\begin{array}{cc}
1 &amp; 0 \\
0 &amp; 1 \\
0 &amp; 2+i 
\end{array}\right]$</code></pre>
<p>De donde se tiene que la solución del sistema <span class="math inline">\(AX=0\)</span> es la trivial <span class="math inline">\((0,0,0)\)</span>, ya que <span class="math inline">\(AX=0\)</span> es equivalente al sistema <span class="math inline">\(\left[ \begin{array}{cc} 1 &amp; 0 \\ 0 &amp; 1 \\ 0 &amp; 2+i \end{array}\right] \cdot \left[ \begin{array}{c} x_{1} \\ x_{2} \end{array}\right] = \left[ \begin{array}{c} 0 \\ 0 \end{array}\right]\)</span></p>
</div>

<p>Dada cualquier matriz <span class="math inline">\(A\)</span> de orden <span class="math inline">\(m\times n\)</span>, podemos hallar una matriz equivalente por filas que sea escalonda reducida por filas, realizando un número finito de operaciones por filas según el siguiente algoritmo. Toda fila nula de la matriz se mueven hacia abajo de la matriz por medio de la operación intercambio de filas, de forma que todas ellas queden en las últimas filas de la matriz, es decir, supongamos que existen <span class="math inline">\(r\leq m\)</span> filas no nulas, entonces las últimas filas <span class="math inline">\(r+1, r+2, \cdots, m\)</span> serán las filas de ceros, de forma que el bloque superior <span class="math inline">\(1,2,\cdots, r\)</span> serán las filas no nulas. Luego, considerando esta nueva matriz (la llamaremos <span class="math inline">\(A\)</span>, por comodidad). Sea <span class="math inline">\(a_{1k_{1}}\)</span> el primer elemento no nulo de la primera fila (<span class="math inline">\(k_{1}\)</span> es la columna donde aparece el primer elemento no nulo de la fila <span class="math inline">\(1\)</span>), si <span class="math inline">\(a_{1k_{1}}=1\)</span>, se cumple la condición (1), si no es así, aplicamos la operación <span class="math inline">\(\frac{1}{a_{1k_{1}}} e_{1}\)</span> para hacer que el pivote sea <span class="math inline">\(1\)</span>. Ahora, debemos hacer que todo elemento en esa columna sea cero si está en otra fila (condición (2)), para esto aplicamos la operación <span class="math inline">\(-a_{ik_{1}}e_{i}\)</span> para cada fila <span class="math inline">\(1\neq i\leq r\)</span>. Pasamos a la siguiente fila, <span class="math inline">\(A_{2\ast}\)</span>, consideramos el primer elemento no nulo de dicha fila, <span class="math inline">\(a_{2k_{2}}\)</span>, donde <span class="math inline">\(k_{2}\)</span> es la columna que ocupa. Si <span class="math inline">\(a_{2k_{2}}=1\)</span>, no haremos nada, en otro caso, aplicamos la operación <span class="math inline">\(\frac{1}{a_{2k_{2}}} e_{2}\)</span>, con esto se cumple la condición (1) para esta fila. Ahora aplicamos <span class="math inline">\(-a_{ik_{2}}e_{i}\)</span> para cada fila <span class="math inline">\(2\neq i\leq r\)</span>, con esto se cumple la condición (2). Repetimos este proceso para cada una de las filas no nulas de <span class="math inline">\(A\)</span>, es decir, para las filas <span class="math inline">\(1,2,\cdots, r\)</span>. Ahora ordenamos las filas no nulas intercambiando filas para lograr que se cumpla la condición (4), es decir, que se cumpla que <span class="math inline">\(p_{1}&lt; p_{2}&lt; \cdots &lt; p_{r}\)</span>, llamando <span class="math inline">\(p_{i}\)</span> a la columna del pivote de la fila <span class="math inline">\(i\)</span> luego de aplicar las operaciones elementales por filas. Es claro que estas operaciones son siempre posibles de aplicar a cualquier matriz, en un número finito de pasos. De lo anterior podemos concluir que siempre podemos reducir una matriz a una escalonada. Podemos expresarlo como un teorema, cuya demostración es el algoritmo anterior.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-233" class="theorem"><strong>Teorema 4.5  </strong></span> Toda matriz <span class="math inline">\(m\times n\)</span> sobre el cuerpo <span class="math inline">\(\mathbb{F}\)</span> es equivalente por filas a una matriz escalonada reducida por filas.</p>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-234" class="theorem"><strong>Teorema 4.6  </strong></span> Si <span class="math inline">\(A\)</span> es una matriz <span class="math inline">\(m\times n\)</span> con <span class="math inline">\(m&lt;n\)</span>, el sistema homogéneo de ecuaciones lineales <span class="math inline">\(AX=0\)</span> tiene una solución no trivial.</p>
</div>
 
<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sea <span class="math inline">\(R\)</span> una matriz escalón reducida por filas equivalente por filas a la matriz <span class="math inline">\(A\)</span>. Entonces <span class="math inline">\(AX=0\)</span> y <span class="math inline">\(RX=0\)</span> tienen exactamente las mismas soluciones. Supongamos que <span class="math inline">\(R\)</span> tiene <span class="math inline">\(r\)</span> filas no nulas, luego <span class="math inline">\(r&lt;n\)</span>, luego el sistema de ecuaciones <span class="math inline">\(RX=0\)</span> consta de <span class="math inline">\(r\)</span> ecuaciones no triviales, a saber, suponiendo que <span class="math inline">\(x_{k_{i}}\)</span> es la incógnita que aparece en la posición del pivote de la fila <span class="math inline">\(i\)</span>, <span class="math display">\[\begin{array}{ccc}
    x_{k_{1}}+&amp; \sum_{j=1}^{n-r}c_{1j}u_{j}=&amp;0\\
    \vdots &amp; &amp; \vdots\\
    x_{k_{r}}+&amp; \sum_{j=1}^{n-r}c_{rj}u_{j}=&amp;0
    \end{array}\]</span> donde las <span class="math inline">\(n-r\)</span> incógnitas diferentes de <span class="math inline">\(x_{k_{1}}, x_{k_{2}},\cdots, x_{k_{r}}\)</span> las denotamos <span class="math inline">\(u_{1}, u_{2}, \cdots, u_{n-r}\)</span>. Note que la incógnita <span class="math inline">\(x_{k_{i}}\)</span> aparece solo en la <span class="math inline">\(i\)</span>-ésima ecuación. De esta forma, podemos dar valores arbitrarios a <span class="math inline">\(u_{1}, u_{2}, \cdots, u_{n-r}\)</span> y así hallar una solución no trivial, que a su vez es solución del sistema <span class="math inline">\(AX=0\)</span>.</p>
</div>


<div class="theorem">
<p><span id="thm:teorema6" class="theorem"><strong>Teorema 4.7  </strong></span> Si <span class="math inline">\(A\)</span> es una matriz cuadrada <span class="math inline">\(n\times n\)</span>. Entonces <span class="math inline">\(A\)</span> es equivalente por filas a la matriz identidad si y solo si, el sistema de ecuaciones <span class="math inline">\(AX=0\)</span> tiene solo la solución trivial <span class="math inline">\(X=0\)</span>.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Si <span class="math inline">\(A\)</span> es equivalente por filas a la identidad, por el teorema anterior <span class="math inline">\(AX=0\)</span> y <span class="math inline">\(IX=0\)</span> tienen exactamente las mismas soluciones, por lo tanto la única solución es <span class="math inline">\(X=0\)</span>. Ahora, supongamos que <span class="math inline">\(AX=0\)</span> tiene unicamente la solución trivial; supongamos que <span class="math inline">\(R\)</span> es la matriz escalonada reducida equivalente a <span class="math inline">\(A\)</span>, y sea <span class="math inline">\(r\leq n\)</span> el número de filas no nulas; entonces <span class="math inline">\(RX=0\)</span> tiene unicamente la solución trivial (por el teorema anterior), luego <span class="math inline">\(r\geq n\)</span>, por lo tanto <span class="math inline">\(r=n\)</span>, por lo tanto <span class="math inline">\(R=I\)</span>.</p>
</div>

<p>Todos los teoremas anteriores hacen referencia a sistemas homogéneos <span class="math inline">\(AX=0\)</span>, el cual siempre tiene solución, la solución trivial <span class="math inline">\(X=0\)</span>. Cabe preguntar que sucede con los sistemas no homogéneos <span class="math inline">\(AX=B\)</span>. Un sistema no homogéneo no tiene necesariamente solución. Estudiemos esto a continuación. Dado el sistema no homogéneo <span class="math inline">\(AX=B\)</span>, con <span class="math inline">\(A\)</span> de orden <span class="math inline">\(m\times n\)</span>; consideramos la <em>matriz aumentada</em>, <span class="math inline">\(\hat{A}\)</span> de orden <span class="math inline">\(m\times (n+1)\)</span>, cuyas primeras <span class="math inline">\(n\)</span> columnas son iguales a las columnas de <span class="math inline">\(A\)</span> y la columna <span class="math inline">\(n+1\)</span> corresponde a <span class="math inline">\(B\)</span>, es decir, <span class="math inline">\(\hat{A}_{\ast j}=A_{\ast j}\)</span> para <span class="math inline">\(j\leq n\)</span> y <span class="math inline">\(\hat{A}_{\ast n+1}=B_{1}\)</span>. Se aplicará a esta matriz, <span class="math inline">\(\hat{A}\)</span> las mismas operaciones elementales por filas que se le aplican a la matriz <span class="math inline">\(A\)</span> para llevarla a una matriz escalonada reducidas por filas <span class="math inline">\(R\)</span>, y así se obtendrá una matriz <span class="math inline">\(\hat{R}\)</span> cuya última fila son los escalares <span class="math inline">\(z_{1}, z_{2},\cdots z_{m}\)</span> (que serán combinaciones lineales de los coeficientes <span class="math inline">\(b_{1}, b_{2},\cdots, b_{m}\)</span>). Es claro que los sistemas <span class="math inline">\(AX=B\)</span> y <span class="math inline">\(RX=Z\)</span> tienen las mismas soluciones (la demostración es análoga al de los sistemas homogéneos). Es fácil ver cuando el sistema <span class="math inline">\(\hat{R}X=Z\)</span> tiene solución. Si <span class="math inline">\(\hat{R}\)</span> tiene <span class="math inline">\(r\)</span> filas no nulas, donde el pivote de la fila <span class="math inline">\(i\)</span> está en la columna <span class="math inline">\(k_{i}\)</span> entonces las primeras <span class="math inline">\(r\)</span> ecuaciones expresarán las primeras <span class="math inline">\(r\)</span> incógnitas, <span class="math inline">\(x_{k_{1}}, x_{k_{2}}, \cdots, x_{k_{r}}\)</span> por las <span class="math inline">\(n-r\)</span> incógnitas restantes, <span class="math inline">\(x_{j}\)</span> y los escalares <span class="math inline">\(z_{1}, z_{2},\cdots, z_{r}\)</span>. Y las últimas <span class="math inline">\(m-r\)</span> ecuaciones son: <span class="math display">\[\begin{array}{cc}
0= &amp; z_{r+1}\\
\vdots &amp; \vdots\\
0= &amp; z_{m}
\end{array}\]</span> Por lo tanto, para que el sistema tenga solución debe suceder que <span class="math inline">\(z_{r+1}=z_{r+2}=\cdots=z_{m}=0\)</span>. Si esto ocurre entonces las soluciones del sistema se obtienen dando valores arbitrarios a la <span class="math inline">\(n-j\)</span> incógnitas <span class="math inline">\(x_{j}\)</span>.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-237" class="example"><strong>Ejemplo 4.7  </strong></span> Sea <span class="math display">\[A=\left[\begin{array}{ccc}
    1 &amp; -2 &amp; 1\\
    2 &amp; 1 &amp; 1\\
    0 &amp; 5 &amp; -1
    \end{array} \right] \]</span> la matriz de coeficientes del sistema <span class="math inline">\(AX=B\)</span>, donde <span class="math inline">\(B=\left[\begin{array}{c}  b_{1}\\  b_{2}\\  b_{3}  \end{array} \right]\)</span>. Luego la matriz extendida es <span class="math inline">\(\hat{A}=\left[\begin{array}{ccc|c}  1 &amp; -2 &amp; 1 &amp; b_{1}\\  2 &amp; 1 &amp; 1 &amp; b_{2}\\  0 &amp; 5 &amp; -1 &amp; b_{3}  \end{array} \right]\)</span></p>
<pre><code>Reducimos la matriz:

$\left[\begin{array}{ccc|c}
1 &amp; -2 &amp; 1 &amp; b_{1}\\
2 &amp; 1 &amp; 1 &amp; b_{2}\\
0 &amp; 5 &amp; -1 &amp; b_{3}
\end{array} \right] \stackrel{-2 e_{12}}{\longrightarrow} 
\left[\begin{array}{ccc|c}
1 &amp; -2 &amp; 1 &amp; b_{1}\\
0 &amp; 5 &amp; -1 &amp; b_{2}-2b_{1}\\
0 &amp; 5 &amp; -1 &amp; b_{3}
\end{array} \right] \stackrel{\frac{1}{5} e_{2}}{\longrightarrow}
\left[\begin{array}{ccc|c}
1 &amp; -2 &amp; 1 &amp; b_{1}\\
0 &amp; 1 &amp; -\frac{1}{5} &amp; \frac{1}{5}(b_{2}-2b_{1})\\
0 &amp; 5 &amp; -1 &amp; b_{3}
\end{array} \right] \stackrel{ 2e_{21}}{\longrightarrow}</code></pre>
<p> $ Luego, el sistema tiene solución solo si <span class="math inline">\(2b_{1}-b_{2}+b_{3}=0\)</span>. Si esta condición se cumple, entonces una solución para el sistema es de la forma: <span class="math inline">\(\begin{array}{cc}  x_{1}= &amp;-\frac{3}{5}x_{3}+\frac{1}{5}(b_{1}+2b_{2})\\  x_{2}= &amp;\frac{1}{5}x_{3}+\frac{1}{5}(b_{2}-2b_{1})  \end{array}\)</span> para cualquier valor de <span class="math inline">\(x_{3}\)</span>.</p>
</div>

<p>Note que la igualdad <span class="math inline">\(A(BC)=(AB)C\)</span> implica, entre otras cosas, que combinaciones lineales de combinaciones lineales de las filas de <span class="math inline">\(C\)</span>, son otra vez combinaciones lineales de las filas de <span class="math inline">\(C\)</span>. Si <span class="math inline">\(C\)</span> es una matriz que se obtiene de aplicar una operación elemental de fila a <span class="math inline">\(B\)</span>, entonces las filas de <span class="math inline">\(C\)</span> son combinación lineal de las filas de <span class="math inline">\(B\)</span>, por lo tanto debe existir una matriz <span class="math inline">\(A\)</span>, tal que <span class="math inline">\(C=AB\)</span>. Pueden existir muchas matrices con tal propiedad; veremos como escoger una de ellas.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-238" class="definition"><strong>Definición 4.7  </strong></span> Una matriz <span class="math inline">\(m\times m\)</span> es una <em>matriz elemental</em> si se obtiene de aplicar una sola operación elemental de filas a la matriz identidad (de orden <span class="math inline">\(m\times m\)</span>).</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-239" class="example"><strong>Ejemplo 4.8  </strong></span> Una matriz elementa de orden <span class="math inline">\(2\times 2\)</span> es necesariamente como alguna de las siguientes: <span class="math display">\[ \left[ \begin{array}{cc}
    0 &amp; 1\\
    1 &amp; 0
    \end{array} \right] , 
    \left[ \begin{array}{cc}
    1 &amp; \lambda\\
    0 &amp; 1
    \end{array} \right] ,
    \left[ \begin{array}{cc}
    1 &amp; 0\\
    \lambda &amp; 1
    \end{array} \right]  \mbox{ con }\lambda\in \mathbb{F}\]</span> o</p>
<pre><code>$$\left[ \begin{array}{cc}
\delta &amp; 0\\
0 &amp; 1
\end{array} \right] , 
\left[ \begin{array}{cc}
1 &amp; 0\\
0 &amp; \delta
\end{array} \right] \mbox{ con } \delta\neq 0$$</code></pre>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-240" class="theorem"><strong>Teorema 4.8  </strong></span> Sea <span class="math inline">\(E\)</span> una matriz elemental que corresponde a la operación elemental por filas <span class="math inline">\(e\)</span>. Sea <span class="math inline">\(A\)</span> una matriz <span class="math inline">\(m\times n\)</span>. Entonces <span class="math inline">\(e(A)=EA\)</span>.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Note que el elemento <span class="math inline">\([EA]_{ij}\)</span> se obtien de la <span class="math inline">\(i\)</span>-ésima fila de <span class="math inline">\(E\)</span> y la <span class="math inline">\(j\)</span>-ésima columna de <span class="math inline">\(A\)</span>. Estudiemos cada tipo de operación por separado, comenzaremos con la mas complicada y las otras dos se dejan como ejercicio. Supongamos que <span class="math inline">\(e= \lambda e_{sr}\)</span> (se sustituye la fila <span class="math inline">\(r\)</span> por la fila <span class="math inline">\(r\)</span> mas <span class="math inline">\(\lambda\)</span> veces la fila <span class="math inline">\(s\)</span>). Se tiene que <span class="math inline">\(E=\lambda e_{sr}(I)\)</span>, por lo tanto: <span class="math display">\[[E]_{ik}=
        \left\{ \begin{array}{lc}
        \delta_{ik} &amp; i\neq r\\
        \delta_{rk}+\lambda \delta_{sk} &amp; i=r
        \end{array} 
        \right.\]</span> Luego <span class="math display">\[ [EA]_{ij}=\sum_{k=1}^{m} [E]_{ik}[A]_{kj}=
    \left\{ \begin{array}{lc}
        a_{ik} &amp; i\neq r \\
        a_{rk}+\lambda a_{sk} &amp; i=r
        \end{array} 
    \right. 
\]</span> Por lo tanto <span class="math inline">\(EA=e(A)\)</span>.</p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-242" class="corollary"><strong>Corolario 4.1  </strong></span> Sean <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> matrices de orden <span class="math inline">\(m\times n\)</span> sobre el cuerpo <span class="math inline">\(\mathbb{F}\)</span>. Entonces <span class="math inline">\(B\)</span> es equivalente por filas a <span class="math inline">\(A\)</span> si y solo si <span class="math inline">\(B=PA\)</span>, donde <span class="math inline">\(P\)</span> es un producto de matrices elementales <span class="math inline">\(m\times m\)</span>.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Supongamos que <span class="math inline">\(E_{1}, E_{2},\cdots, E_{k}\)</span> matrices elementales correspondientes a las operaciones elementales por fila <span class="math inline">\(e_{1}, e_{2},\cdots, e_{k}\)</span>. Entonces <span class="math inline">\(B=e_{1}\circ e_{2}\circ\cdots \circ e_{k}(A)\)</span> si y solo si <span class="math inline">\(B=E_{1} E_{2} \cdots E_{k} A=PA\)</span>, con <span class="math inline">\(P=E_{1} E_{2} \cdots E_{k}\)</span>.</p>
</div>
<p> Si <span class="math inline">\(A\)</span> es una matriz equivalente por filas a <span class="math inline">\(B\)</span>, el corolario anterior asegura que existe <span class="math inline">\(P\)</span>, tal que <span class="math inline">\(B=PA\)</span>, donde <span class="math inline">\(P\)</span> es producto de matrices elementales. Por otro lado se tiene que <span class="math inline">\(B\)</span> es equivalente por filas a <span class="math inline">\(A\)</span>, luego existe una matriz <span class="math inline">\(Q\)</span>, producto de matrices elementales, tal que <span class="math inline">\(A=QB\)</span>. En particular, esto es cierto para la matriz identidad, es decir <span class="math inline">\(I=QB=QP\)</span>. Veremos que la existencia de la matriz <span class="math inline">\(Q\)</span>, tal que <span class="math inline">\(QP=I\)</span> es equivalente al hecho de que <span class="math inline">\(P\)</span> es producto de matrices elementales.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-244" class="definition"><strong>Definición 4.8  </strong></span> Sea <span class="math inline">\(A\)</span> una matriz (cuadrada) <span class="math inline">\(n\times n\)</span> sobre el cuerpo <span class="math inline">\(\mathbb{F}\)</span>. Una matriz <span class="math inline">\(B\)</span> <span class="math inline">\(n\times n\)</span>, tal que <span class="math inline">\(BA=I\)</span> se llama <em>inversa izquierda</em> de <span class="math inline">\(A\)</span>; una matriz <span class="math inline">\(B\)</span> <span class="math inline">\(n\times n\)</span>, tal que <span class="math inline">\(AB=I\)</span> se llama <em>inversa derecha</em> de <span class="math inline">\(A\)</span>. Si <span class="math inline">\(AB=BA=I\)</span>, entonces <span class="math inline">\(B\)</span> se llama <em>inversa bilateral</em> de <span class="math inline">\(A\)</span> y en este caso se dice que <span class="math inline">\(A\)</span> es <em>invertible</em>.</p>
</div>


<div class="lemma">
<p><span id="lem:unnamed-chunk-245" class="lemma"><strong>Lema 4.1  </strong></span> Si una matriz <span class="math inline">\(A\)</span> tiene una inversa izquierda <span class="math inline">\(B\)</span> y una inversa derecha <span class="math inline">\(C\)</span>, entonces <span class="math inline">\(B=C\)</span>.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Suponga que <span class="math inline">\(BA=I\)</span> y <span class="math inline">\(AC=I\)</span>. Entonces <span class="math inline">\(B=BI=B(AC)=(BA)C=IC=C\)</span></p>
</div>

<p>Del corolario anterior tenemos que si <span class="math inline">\(A\)</span> tiene una inversa derecha y una inversa izquierda, entonces son iguales, la llamos <em>la inversa de <span class="math inline">\(A\)</span></em> y la denotámos <span class="math inline">\(A^{-1}\)</span>.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-247" class="theorem"><strong>Teorema 4.9  </strong></span> Dadas las matrices <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> de orden <span class="math inline">\(n\times n\)</span> sobre el cuerpo <span class="math inline">\(\mathbb{F}\)</span>. Se tiene que:</p>
<pre><code>    (1) Si $A$ es invertible, entonces $A^{-1}$ también lo es y $(A^{-1})^{-1}=A$.
    (2) Si $A$ y $B$ son invertibles, entonces $AB$ también lo es y $(AB)^{-1}=B^{-1}A^{-1}$.
&lt;/div&gt;\EndKnitrBlock{theorem}</code></pre>

<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  De la simetría de la definición de inversa, se sigue la parte <span class="math inline">\(1\)</span>. Para la segunda parte se sigue de <span class="math inline">\((AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=AIA^{-1}=AA^{-1}=I\)</span> y <span class="math inline">\((B^{-1}A^{-1})(AB)=B^{-1}(A^{-1}A)B=B^{-1}IB=B^{-1}B=I\)</span>.</p>
</div>

<p>Del resultado anterior se tiene que el producto de matrices invertible, es invertible.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-249" class="theorem"><strong>Teorema 4.10  </strong></span> Toda matriz elemental es invertible.</p>
</div>
 
<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sea <span class="math inline">\(E\)</span> una matriz elemental y sea <span class="math inline">\(e\)</span> la operación elemental de filas que corresponde a <span class="math inline">\(E\)</span>, es decir, <span class="math inline">\(E=e(I)\)</span>. Sea <span class="math inline">\(\hat{e}\)</span> la operación inversa de <span class="math inline">\(e\)</span>, y <span class="math inline">\(\hat{E}=\hat{e}(I)\)</span>. Entonces <span class="math inline">\(\hat{E}E=\hat{e}(E)=\hat{e}(e(I))=(\hat{e}\circ e) (I)= I\)</span>. Análogamente se tiene que <span class="math inline">\(E\hat{E}=I\)</span>. Luego <span class="math inline">\(\hat{E}=E^{-1}\)</span>.</p>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-251" class="theorem"><strong>Teorema 4.11  </strong></span> Sea <span class="math inline">\(A\)</span> una matriz <span class="math inline">\(n\times n\)</span>. Entonces los siguientes enunciados son equivalentes:</p>
<pre><code>    (1) $A$ es invertible.
    (2) $A$ es equivalente por filas a la identidad (de orden $n\times n$).
    (3) $A$ es producto de matrices elementales.</code></pre>
</div>
 
<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sea <span class="math inline">\(R=E_{1}E_{2}\cdots E_{n}A\)</span> una matriz esacalonada reducida equivalente por filas a la matriz <span class="math inline">\(A\)</span> (donde <span class="math inline">\(E_{1}E_{2}\cdots E_{n}\)</span> son matrices elementales). Entonces, <span class="math inline">\(A=E_{n}^{-1}\cdots E_{2}^{-1}E_{1}^{-1}R\)</span>, ya que las matrices elementales son invertibles. Como el producto de matrices invertibles, es invertible, <span class="math inline">\(A\)</span> es invertible si y solo si <span class="math inline">\(R\)</span> lo es; como <span class="math inline">\(R\)</span> es una matriz cuadrada esacalonada reducida por filas, entonces <span class="math inline">\(R\)</span> es invertible si y solo si <span class="math inline">\(R\)</span> es la identidad. Luego, <span class="math inline">\(A\)</span> es invertible si y solo si <span class="math inline">\(R=I\)</span>, entonces, <span class="math inline">\(A=E_{n}^{-1}\cdots E_{2}^{-1}E_{1}^{-1}I=E_{n}^{-1}\cdots E_{2}^{-1}E_{1}^{-1}\)</span>.</p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-253" class="corollary"><strong>Corolario 4.2  </strong></span> Si <span class="math inline">\(A\)</span> es una matriz invertible <span class="math inline">\(n\times n\)</span> y si la sucesión de operaciones elementales <span class="math inline">\(e_{1}\circ e_{2}\circ \cdots \circ e_{n}\)</span> reduce a la matriz <span class="math inline">\(A\)</span> a la identidad, entonces la misma sucesión de operaciones elementales aplicadas a <span class="math inline">\(I\)</span>, nos da <span class="math inline">\(A^{-1}\)</span>, la inversa de <span class="math inline">\(A\)</span>.</p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-254" class="corollary"><strong>Corolario 4.3  </strong></span> Sean <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> dos matrices <span class="math inline">\(m\times n\)</span>. Entonces <span class="math inline">\(B\)</span> es equivalente por filas a <span class="math inline">\(A\)</span> si y solo si <span class="math inline">\(B=PA\)</span>, donde <span class="math inline">\(P\)</span> es una matriz invertible <span class="math inline">\(m\times m\)</span>.</p>
</div>
 
<div class="theorem">
<p><span id="thm:unnamed-chunk-255" class="theorem"><strong>Teorema 4.12  </strong></span> Sea <span class="math inline">\(A\)</span> una matriz <span class="math inline">\(n\times n\)</span>. Entonces los siguientes enunciados son equivalentes:</p>
<pre><code>(1) $A$ es invertible.
(2) El sistema homogéneo $AX=0$ tiene una única solución ($X=0$).
(3) El sistema de ecuaciones asociado a $AX=B$ tiene una solución $X$ para cada matriz $B$ de orden $n\times 1$.
&lt;/div&gt;\EndKnitrBlock{theorem}</code></pre>

<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  El sistema homogéneo <span class="math inline">\(AX=0\)</span> si y solo si <span class="math inline">\(A\)</span> es equivalente por filas al la identidad ( por el teorema <a href="espacios-vectoriales.html#thm:teorema6">4.7</a>. Del teorema anterior, se tiene que <span class="math inline">\((1)\)</span> y <span class="math inline">\((2)\)</span> son equivalentes. Ahora supongámos que <span class="math inline">\(A\)</span> es invertible, entonces la solución de <span class="math inline">\(AX=B\)</span> viene dada por <span class="math inline">\(X=A^{-1}\)</span>. Recíprocamente, supongamos que <span class="math inline">\(AX=B\)</span> tiene una solución para cada <span class="math inline">\(B\)</span>. Sea <span class="math inline">\(R\)</span> la matriz escalonada reducida por filas equivalente por filas a <span class="math inline">\(A\)</span>. Necesariamente <span class="math inline">\(R=I\)</span>, ya que si <span class="math inline">\(R\)</span> tiene al menos una fila identicamente igual a cero, entonces el sistema <span class="math inline">\(RX=C\)</span>, donde <span class="math display">\[C=\left[ \begin{array}{c}
    0\\
    0\\
    \vdots\\
    0\\
    1
    \end{array}\right] \]</span> no tiene solución, lo que es una contradicción. Por lo tanto se tiene que <span class="math inline">\(R=I\)</span>.</p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-257" class="corollary"><strong>Corolario 4.4  </strong></span> Una matriz cuadrada que tiene una inversa izquierda o una inversa a la derecha es invertible.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sea <span class="math inline">\(A\)</span> una matriz <span class="math inline">\(n\times n\)</span>. Supongamos que <span class="math inline">\(A\)</span> tiene una inversa a la izquierda, es decir, existe <span class="math inline">\(B\)</span> tal que <span class="math inline">\(BA=I\)</span>. Así, <span class="math inline">\(AX=0\)</span> tiene unicamente la solución trivial, ya que <span class="math inline">\(X=IX=B(AX)\)</span>. Por lo tanto <span class="math inline">\(A\)</span> es invertible. Supóngase que <span class="math inline">\(A\)</span> tiene una inversa a la derecha, es decir, existe <span class="math inline">\(C\)</span> tal que <span class="math inline">\(AC=I\)</span>. entoces <span class="math inline">\(C\)</span> tiene una inversa a la izquierda y por lo tanto es invertible, así <span class="math inline">\(A=ACC^{-1}=IC^{-1}=C^{-1}\)</span> de donde se tiene que <span class="math inline">\(A\)</span> es invertible de inversa <span class="math inline">\(C\)</span>.</p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-259" class="corollary"><strong>Corolario 4.5  </strong></span> Sea <span class="math inline">\(A=A_{1}A_{2}\cdots A_{n}\)</span>, donde <span class="math inline">\(A_{1}, A_{2}, \cdots , A_{n}\)</span> son matrices cuadradas. Entonces <span class="math inline">\(A\)</span> es invertible si y solo si cada <span class="math inline">\(A_{i}\)</span> es invertible.</p>
</div>
 
<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Supongamos que cada <span class="math inline">\(A_{i}\)</span> es invertible, como el producto de matrices invertibles es invertible, se tiene que <span class="math inline">\(A\)</span> es invertible. Recíprocamente, supongamos que <span class="math inline">\(A\)</span> es invertible, entonces <span class="math inline">\(AX=0\)</span> tiene unicamente la solución trivial. Por lo tanto, <span class="math inline">\((A_{1}A_{2}\cdots A_{n-1})A_{n}X=AX=0\)</span> tiene unicamente la solución trivial, si y solo si <span class="math inline">\(A_{n}X=0\)</span> tiene solo la solución trivial, de donde se sigue que <span class="math inline">\(A_{n}\)</span> es invertible. Luego, <span class="math inline">\(AA^{-1}=A_{1}A_{2}\cdots A_{n-1}\)</span>, considerando <span class="math inline">\((A_{1}A_{2}\cdots A_{n-2})A_{n-1}X=AA^{-1}X=0\)</span>, de forma análoga a lo anterior, se puede demostrar que <span class="math inline">\(A_{n-1}\)</span> es invertible. Continuando de esta forma se demuestra que cada <span class="math inline">\(A_{i}\)</span> es invertible.</p>
</div>


<div class="remark">
<p> <span class="remark"><em>Nota. </em></span>  Sea <span class="math inline">\(A\)</span> una matriz <span class="math inline">\(m\times n\)</span> y sea <span class="math inline">\(AX=B\)</span>, y sea <span class="math inline">\(R\)</span> la matriz escalonada reducida por filas equivalentes por fila a <span class="math inline">\(A\)</span>, entonces <span class="math inline">\(R=PA\)</span>, donde <span class="math inline">\(P\)</span> es una matriz invertible <span class="math inline">\(m\times m\)</span>; las soluciones de <span class="math inline">\(RX=PB\)</span> son las mismas que las del sistema <span class="math inline">\(AX=B\)</span>. Hallar <span class="math inline">\(PB\)</span> es equivalente a reducir por filas la matriz <span class="math inline">\(A\)</span>, esto se hace considerando la matriz aumentada <span class="math inline">\(\hat{A}\)</span> y aplicandole operaciones elementales por fila hasta reducir a <span class="math inline">\(A\)</span> a una matriz escalonada reducida, lo obtenido en la última columna será la matriz <span class="math inline">\(PB\)</span>.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-262" class="example"><strong>Ejemplo 4.9  </strong></span> Sea $A=$ la matriz de coeficientes del sistema <span class="math inline">\(AX=B\)</span>, donde <span class="math inline">\(B=\left[\begin{array}{c}  b_{1}\\  b_{2}  \end{array} \right]\)</span>. Luego la matriz extendida es</p>
<p><span class="math inline">\(\hat{A}=\left[\begin{array}{cc|c}  2 &amp; -1 &amp; b_{1}\\  1 &amp; 3 &amp; b_{2}  \end{array} \right]\)</span></p>
<pre><code>Reducimos la matriz:
$\left[\begin{array}{cc|c}
2 &amp; -1 &amp; b_{1}\\
1 &amp; 3 &amp; b_{2}
\end{array} \right] \stackrel{e_{12}}{\longrightarrow} 
\left[\begin{array}{cc|c}
1 &amp; 3 &amp; b_{2}\\
2 &amp; -1 &amp; b_{1}
\end{array} \right] \stackrel{-2 e_{12}}{\longrightarrow}
\left[\begin{array}{cc|c}
1 &amp; 3 &amp; b_{2}\\
0 &amp; -7 &amp; b_{1}-2b_{2}
\end{array} \right] \stackrel{ -\frac{1}{7}e_{2}}{\longrightarrow}
\left[\begin{array}{cc|c}
1 &amp; 3 &amp;   b_{2}\\
0 &amp; 1 &amp; \frac{1}{7}(2b_{2}-b_{1})
\end{array} \right] \stackrel{ -3e_{21}}{\longrightarrow}
\left[\begin{array}{cc|c}
1 &amp; 0 &amp;  \frac{1}{7}(b_{2}+3b_{1})\\
0 &amp; 1 &amp; \frac{1}{7}(2b_{2}-b_{1})
\end{array} \right]$

De donde se tiene que $PB=\left[ \begin{array}{c}
\frac{1}{7}(b_{2}+3b_{1})\\
\frac{1}{7}(2b_{2}-b_{1})
\end{array}\right] $, o también que $A^{-1}=\left[\begin{array}{cc}
\frac{3}{7} &amp; \frac{1}{7}\\
-\frac{1}{7} &amp; \frac{2}{7} 
\end{array} \right]$.

También podemos llegar a la inversa de $A$ aplicando las operaciones elementales antes descritas para reducir a $A$, a la identidad. Es decir:
$\left[\begin{array}{cc|cc}
2 &amp; -1 &amp; 1 &amp; 0\\
1 &amp; 3 &amp; 0 &amp; 1
\end{array} \right] \stackrel{e_{12}}{\longrightarrow} 
\left[\begin{array}{cc|cc}
1 &amp; 3 &amp; 0 &amp; 1\\
2 &amp; -1 &amp; 1 &amp; 0
\end{array} \right] \stackrel{-2 e_{12}}{\longrightarrow}
\cdots 
\left[\begin{array}{cc|cc}
1 &amp; 0 &amp;  \frac{3}{7} &amp; \frac{1}{7}\\
0 &amp; 1 &amp;  -\frac{1}{7} &amp; \frac{2}{7}
\end{array} \right]$</code></pre>
</div>

</div>
<div id="teoria-de-espacios-vectoriales" class="section level2">
<h2><span class="header-section-number">4.4</span> Teoría de espacios vectoriales</h2>
<p>En los sitemas de ecuaciones lineales vimos que las combinaciones lineales de soluciones del sistema volvía a ser una solución. Este no es el único ámbito en el que sucede y es interesante este comportamiento, el concepto de espacios vectoriales generaliza esto. Estudiaremos a continuación estas estructuras algebraicas.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-263" class="definition"><strong>Definición 4.9  </strong></span> Dado un cuerpo <span class="math inline">\(\mathbb{F}\)</span>, un <em>espacio vectorial</em> sobre el cuerpo <span class="math inline">\(\mathbb{K}\)</span> es un conjunto <span class="math inline">\(V\)</span> de objetos llamados <em>vectores</em>, dotado de dos operaciones, <em>suma</em> y <em>producto por un escalar</em>, con las siguientes propiedades:</p>
<pre><code>    (1) cerrado bajo la suma: para todo $u,v\in V$, se tiene que $u+v\in V$;
    (2) la suma es comutativa: para todo $u,v\in V$, $u+v=v+u$;
    (3) la suma es asociativa: para todo $u, v, ww\in V$, $(u+v)+w=u+(v+w)$;
    (4) existe un único vector nulo, $0\in V$, tal que para todo $u\in V$, $u+0=u$;
    (5) para cada vector $u\in V$, existe un único vector opuesto, $-u$, tal que $u+(-u)=0$;
    (6) cerrado bajo el producto por un escalar: para todo $\lambda\in\mathbb{F}$ y todo $u\in V$, se tiene que $\lambda \cdot u\in V$ (se puede abreviar la notación quitando el punto $\lambda u$);
    (7) para todo $u\in V$, $1u=u$;
    (8) para todo $u\in V$ y para todo $\lambda, \gamma\in\mathbb{F}$, $(\lambda\gamma)u=\lambda(\gamma u)$;
    (9) para todo $u, v\in V$ y $\lambda in\in\mathbb{F}$, $\lambda(u+v)=\lambda u+\lambda v$;
    (10) para todo $u\in V$ y para todo $\lambda, \gamma\in\mathbb{F}$, $(\lambda +\gamma)u=\lambda u + \gamma u$.</code></pre>
</div>

<p>Note que un espacio vectorial consta de cuatro cosas, un cuerpo, un conjunto no vacío de vectores y dos operaciones. Cuando no hay posibilidad de confusión, se hará referencia solo al espacio <span class="math inline">\(V\)</span>, omitiendo el resto de los objetos. Puede ocurrir que el mismo conjunto <span class="math inline">\(V\)</span> forme parte de dos espacios vectoriales distintos.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-264" class="example"><strong>Ejemplo 4.10  </strong></span> <em>El conjunto de las <span class="math inline">\(n\)</span>-tuplas</em>, <span class="math inline">\(\mathbb{F}^{n}\)</span>. Sea <span class="math inline">\(\mathbb{F}\)</span> un cuerpo y sea <span class="math inline">\(V\)</span> el conjunto de todas las <span class="math inline">\(n\)</span>-tuplas <span class="math inline">\(u=(x_{1}, x_{2}, \cdots , x_{n})\)</span> de escalares <span class="math inline">\(x_{i}\in\mathbb{F}\)</span>. Dados <span class="math inline">\(u=(x_{1}, x_{2}, \cdots , x_{n}), v=(y_{1}, y_{2}, \cdots , y_{n})\in V\)</span> y el escalar <span class="math inline">\(\lambda\in \mathbb{F}\)</span>, definimos: La suma <span class="math inline">\(u+v=(x_{1}, x_{2}, \cdots , x_{n})+(y_{1}, y_{2}, \cdots , y_{n})=(x_{1}+y_{1}, x_{2}+y_{2}, \cdots , x_{n}+y_{n})\)</span> y <span class="math inline">\(\lambda u=\lambda(x_{1}, x_{2}, \cdots , x_{n})=(\lambda x_{1},\lambda x_{2}, \cdots ,\lambda x_{n})\)</span>. Es fácil ver que estas operaciones así definidas cumplen con las 10 condiciones de la definición.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-265" class="example"><strong>Ejemplo 4.11  </strong></span> <em>El espacio de las matrices</em> <span class="math inline">\(m\times n\)</span>, <span class="math inline">\(\mathbb{F}^{m\times n}\)</span>. Sea <span class="math inline">\(\mathbb{F}\)</span> un cuerpo, y sean <span class="math inline">\(m\)</span> y <span class="math inline">\(n\)</span> enteros positivos. Sea el conjunto <span class="math inline">\(\mathbb{F}^{m\times n}\)</span> el conjunto de todas las matrices de orden <span class="math inline">\(m\times n\)</span> sobre el cuerpo <span class="math inline">\(\mathbb{F}\)</span>. La suma de dos vectores y el producto por un escalar se definen de la forma usual, sean <span class="math inline">\(A,B\in\mathbb{F}^{m\times n}\)</span> y <span class="math inline">\(\lambda\in\mathbb{F}\)</span>, <span class="math inline">\([A+B]_{ij}=[A]_{ij}+[B]_{ij}\)</span> y <span class="math inline">\([\lambda A]_{ij}=\lambda [A]_{ij}\)</span>. Note que, <span class="math inline">\(\mathbb{F}^{1\times n}=\mathbb{F}^{n}\)</span>.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-266" class="example"><strong>Ejemplo 4.12  </strong></span> <em>El espacio de funciones de un conjunto en un cuerpo</em>. Sea <span class="math inline">\(\mathbb{F}\)</span> y <span class="math inline">\(C\)</span> un conjunto no vacío cualquiera. Sea <span class="math inline">\(V\)</span> el conjunto de funciones de <span class="math inline">\(C\)</span> en <span class="math inline">\(\mathbb{F}\)</span>. La suma de dos vectores <span class="math inline">\(f, g\in V\)</span>, se define como la suma usual de funciones, <span class="math inline">\((f+g)(c)=f(c)+g(c)\)</span> y el producto por un escalar se define como <span class="math inline">\((\lambda f)(c)=\lambda f(c)\)</span>. Las condiciones se cumplen, ya que <span class="math inline">\(\mathbb{F}\)</span> es un cuerpo:</p>
<pre><code>    (1) claramente las operaciones así definidas, son cerradas.
    (2) como la suma en $\mathbb{F}$ es conmutativa, $(f+g)(c)=f(c)+g(c)=g(c)+f(c)=(g+f)(c)$;
    (3) como la suma en $\mathbb{F}$ es asociativa, $((f+g)+h)(c)=(f(c)+g(c))+h(c)=f(c)(g(c)+h(c))=(f+(g+h))(c)$;
    (4) el único vector nulo es la función cero, que asigna el escalar cero (del cuerpo $\mathbb{F}$) a cada elemento $c\in C$, esto es $f(c)\equiv 0$;
    (5) para toda función $f$, $-f$, definda como $(-f)(c)=-f(c)$, es el elemento opuesto;
    (6) claramente, $(1f)(c)=1f(c)=f(c)$;
    (7) $(\lambda\gamma f)(c)=(\lambda\gamma) f(c)= \lambda(\gamma f(c))=(\lambda(\gamma f))(c)$;
    (8) $(\lambda(f+g))(c)=(\lambda f)(c)+(\lambda g)(c)=((\lambda f)+(\lambda g))(c)$;
    (9) $((\lambda + \gamma)f)(c)=(\lambda f+ \gamma f)(c)$.</code></pre>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-267" class="example"><strong>Ejemplo 4.13  </strong></span> <em>El espacio de las funciones polinómicas sobre el cuerpo</em> <span class="math inline">\(\mathbb{F}\)</span>. Sea <span class="math inline">\(\mathbb{F}\)</span> un cuerpo y sea <span class="math inline">\(V\)</span> el conjunto de las funciones de <span class="math inline">\(\mathbb{F}\)</span> en <span class="math inline">\(\mathbb{F}\)</span>, de la forma <span class="math inline">\(f(x)=c_{0}+c_{1}x+\cdots+c_{n}x^{n}\)</span>, donde <span class="math inline">\(c_{0}, c_{1},\cdots c_{n}\)</span> son escalares fijos de <span class="math inline">\(\mathbb{F}\)</span> y <span class="math inline">\(n\)</span> un entero positivo cualquiera, una función de este tipo se llama <em>función polinómica</em>. Las operaciones de suman y producto por escalares se definen como en el ejemplo anterior. Es fácil ver que <span class="math inline">\(f+g\)</span> y <span class="math inline">\(\lambda f\)</span> son funciones polinómicas y cumplen con las condiciones de la definición.</p>
</div>
 
<div class="definition">
<p><span id="def:unnamed-chunk-268" class="definition"><strong>Definición 4.10  </strong></span> Una <em>combinación lineal</em> de los vectores <span class="math inline">\(u_{1}, u_{2}, \cdots, u_{n}\)</span> de un espacio vectorial <span class="math inline">\(V\)</span>, es un vector <span class="math inline">\(v\in V\)</span>, tal que <span class="math inline">\(v=\sum_{i=1}^{n} \lambda_{i}u_{i}\)</span>, para algunos escalares <span class="math inline">\(\lambda_{1}, \lambda_{2}, \cdots, \lambda_{n}\)</span>.</p>
</div>


<div class="remark">
<p> <span class="remark"><em>Nota. </em></span>  Note que una combinación lineal de vectores, es un vector del espacio gracias a que las operaciones de suma y producto por un escalar son cerradas. Una combinación lineal de combinaciones lineales de vectores, es una combinación lineal de vectores. Dados los vectores <span class="math inline">\(u_{1}, u_{2}, \cdots, u_{n}\)</span>, de las propiedades de asociatividad de la suma y las propiedades distributivas, se tiene que <span class="math inline">\(\sum_{i=1}^{n} \lambda_{i}u_{i} + \sum_{i=1}^{n} \gamma_{i}u_{i}=\sum_{i=1}^{n} (\lambda_{i}+\gamma_{i})u_{i}\)</span>.</p>
</div>

<p>Mucho del álgebra está motivado por la geometría, las palabras “espacio” y vector tienen una connotación geométrica. Los vectores en el espacio <span class="math inline">\(\mathbb{R}^{3}\)</span> son ternas <span class="math inline">\((x,y,z)\)</span> donde cada entrada representa una coordenada en el espacio. Esto se estudia en el capítulo de vectores.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-270" class="definition"><strong>Definición 4.11  </strong></span> Sea <span class="math inline">\(V\)</span> un espacio vectorial sobre el cuerpo <span class="math inline">\(\mathbb{F}\)</span>. Un <em>subespacio</em> de <span class="math inline">\(V\)</span> es un subconjunto <span class="math inline">\(W\subseteq V\)</span> que junto a las mismas operaciones de suma y producto por un escalar definidas en <span class="math inline">\(V\)</span>, es en si mismo un espacio vectorial sobre <span class="math inline">\(\mathbb{F}\)</span>. Escribiremos <span class="math inline">\(W\prec V\)</span> para decir que <span class="math inline">\(W\)</span> es un subespacio de <span class="math inline">\(V\)</span>.</p>
</div>

<p>De este modo para comprobar que un subconjunto de vectores <span class="math inline">\(W\)</span> de un espacio vectorial <span class="math inline">\(V\)</span>, es un subespacio de <span class="math inline">\(V\)</span>, debemos ver que la suma y el producto por un escalar son operaciones cerradas, es decir, para todo <span class="math inline">\(u,v\in W\)</span> se debe tener que <span class="math inline">\(u+v\in W\)</span> y <span class="math inline">\(\lambda v\in W\)</span>, para calquier escalar <span class="math inline">\(\lambda\)</span> en <span class="math inline">\(\mathbb{F}\)</span>; que el elemento neutro (para la suma) pertenece a <span class="math inline">\(W\)</span>, y que todo vector de <span class="math inline">\(W\)</span>, tiene un opuesto en <span class="math inline">\(W\)</span>, es decir para todo <span class="math inline">\(v\in W\)</span>, <span class="math inline">\(-v\in W\)</span>. Las otras propiedades se heredan del espacio vectorial <span class="math inline">\(V\)</span>. En alguna literatura se puede conseguir que la definición de subespacio vectorial es un subconjunto <span class="math inline">\(W\)</span> que tiene la siguiente propiedad: para todo <span class="math inline">\(u,v\in W\)</span> y todo <span class="math inline">\(\lambda\in\mathbb{F}\)</span>, <span class="math inline">\(\lambda u+v\in W\)</span>. Sin embargo, se puede demostrar la equivalencia entre ambas afirmaciones.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-271" class="theorem"><strong>Teorema 4.13  </strong></span> Un subconjunto no vacío <span class="math inline">\(W\)</span> de <span class="math inline">\(V\)</span> es un subespacio de <span class="math inline">\(V\)</span> si y solo si, para todo par de vectores <span class="math inline">\(u,v\)</span> de <span class="math inline">\(W\)</span> y todo escalar <span class="math inline">\(\lambda\)</span> en el cuerpo <span class="math inline">\(\mathbb{F}\)</span>, se tinen que el vector (combinación lineal) <span class="math inline">\(\lambda u + v\)</span> está en <span class="math inline">\(W\)</span>.</p>
</div>
 
<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sea <span class="math inline">\(W\)</span> un subconjunto de vectores con la propiedad de que <span class="math inline">\(\forall u,v\in W\)</span> y <span class="math inline">\(\forall \lambda\in\mathbb{F}\)</span>, <span class="math inline">\(\lambda u + v\in W\)</span>. Sea <span class="math inline">\(r\in W\)</span>, ya que <span class="math inline">\(W\)</span> no es vacío, entonces, por la propiedad del conjunto <span class="math inline">\((-1)r+r=-r+r=0\in W\)</span>, lo que indica que <span class="math inline">\(W\)</span> contiene al elemento neutro; así <span class="math inline">\((-1)r+0=-r\in W\)</span>, por lo que <span class="math inline">\(W\)</span> tiene al opuesto de todo sus vectores; en general <span class="math inline">\(\lambda u +0\in W\)</span>, es decir el producto por un escalar es cerrado; de este modo, si <span class="math inline">\(u,v\in W\)</span>, <span class="math inline">\(u+v=(1)u+v\in W\)</span>, por lo que se tiene que la suma es cerrada; de donde se concluye que <span class="math inline">\(W\)</span> es un subespacio. Recíprocamente, si <span class="math inline">\(W\)</span> es un subespacio, por definición se tiene que cumple que <span class="math inline">\(\forall u,v\in W\)</span> y <span class="math inline">\(\forall \lambda\in\mathbb{F}\)</span>, <span class="math inline">\(\lambda u + v\in W\)</span>.</p>
</div>
 
<div class="example">
<p><span id="exm:unnamed-chunk-273" class="example"><strong>Ejemplo 4.14  </strong></span> Dado un espacio vectorial <span class="math inline">\(V\)</span>, el mismo <span class="math inline">\(V\)</span> es subespacio de <span class="math inline">\(V\)</span>. El conjunto cuyo único elemento es el vector cero <span class="math inline">\(\{\vec{0}\}\)</span> es subespacio de <span class="math inline">\(V\)</span>, llamado el subespacio nulo de <span class="math inline">\(V\)</span>. Estos dos subespacios son los subespacios triviales de <span class="math inline">\(V\)</span>.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-274" class="example"><strong>Ejemplo 4.15  </strong></span> En el espacio <span class="math inline">\(\mathbb{F}^{n}\)</span>, el conjunto de las <span class="math inline">\(n\)</span>-tuplas <span class="math inline">\((x_{1}, x_{2},\cdots , x_{n})\)</span> donde <span class="math inline">\(x_{i}=0\)</span>, para algún <span class="math inline">\(i\)</span> fijo, es un subespacio de <span class="math inline">\(\mathbb{F}^{n}\)</span>. Pero el conjunto de las <span class="math inline">\(n\)</span>-tuplas, tales que <span class="math inline">\(x_{i}+kx_{j}=p\)</span> no es un subespacio.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-275" class="example"><strong>Ejemplo 4.16  </strong></span> El espacio de las funciones polinómicas es subespacio del espacio de todas las funciones.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-276" class="example"><strong>Ejemplo 4.17  </strong></span> Una matriz cuadrada <span class="math inline">\(n\times n\)</span>, es una <em>matriz simétrica</em> si <span class="math inline">\(a_{ij}=a_{ji}\)</span> para todo <span class="math inline">\(1\leq i,j\leq n\)</span>. El conjunto de las matrices simétricas es un subespacio del espacio de las matrices <span class="math inline">\(n\times n\)</span>.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-277" class="example"><strong>Ejemplo 4.18  </strong></span> Una matriz cuadrada <span class="math inline">\(n\times n\)</span> sobre el cuerpo de los números complejos <span class="math inline">\(\mathbb{C}\)</span> es una <em>matriz hermítica</em> (o <em>autoadjunta</em>) si <span class="math inline">\(a_{jk}=\bar{a_{kj}}\)</span> para todo <span class="math inline">\(1\leq j,k\leq n\)</span>, donde la barra sobre el escalar <span class="math inline">\(a_{kj}\)</span> denota la conjugación compleja. El conjunto de las matrices hermíticas es un subespacio sobre el espacio de las matrices cuadradas <span class="math inline">\(n\times n\)</span> sobre <span class="math inline">\(\mathbb{R}\)</span>.</p>
</div>
 
<div class="example">
<span id="exm:unnamed-chunk-278" class="example"><strong>Ejemplo 4.19  </strong></span> El espacio de las soluciones de un sistema homogéneo de ecuaciones. Dada una matriz <span class="math inline">\(A\)</span> <span class="math inline">\(m\times n\)</span>, el conjunto de las soluciones, de la ecuación <span class="math inline">\(AX=0\)</span>, forman un subespacio del espacio de las matrices <span class="math inline">\(n\times 1\)</span>. Para esto basta ver que dadas dos soluciones del sistema <span class="math inline">\(X_{1}\)</span> y <span class="math inline">\(X_{2}\)</span>, la matriz <span class="math inline">\(\lambda X_{1} + X_{2}\)</span>, con <span class="math inline">\(\lambda\)</span> un escalar cualquiera, es también solución del sistema. Esto es fácil de ver, ya que <span class="math display">\[\begin{array}{cl}
     [A(\lambda X_{1}+X_{2})]_{ij}&amp;=\sum_{k=1}^{n} [A]_{ik}[\lambda X_{1}+ X_{2}]_{kj}\\
                                  &amp;=\sum_{k=1}^{n} [A]_{ik}\lambda [X_{1}]_{kj}+ A_{ik}[X_{2}]_{kj}\\
                                  &amp;=\sum_{k=1}^{n} \lambda ([A]_{ik} [X_{1}]_{kj})+ ([A]_{ik}[X_{2}]_{kj})\\
                                  &amp;=\lambda \sum_{k=1}^{n} ([A]_{ik} [X_{1}]_{kj}) + \sum_{k=1}^{n} ([A]_{ik}[X_{2}]_{kj})\\
                                  &amp;=\lambda[AX_{1}]_{ij} + [AX_{2}]_{ij}
     \end{array}\]</span>
</div>

<p>De hecho, la demostración del ejemplo anterior se puede generalizar.</p>

<div class="lemma">
<p><span id="lem:unnamed-chunk-279" class="lemma"><strong>Lema 4.2  </strong></span> Sea <span class="math inline">\(A\)</span> una matriz <span class="math inline">\(m\times n\)</span> sobre el cuerpo <span class="math inline">\(\mathbb{F}\)</span>. Si <span class="math inline">\(B\)</span> y <span class="math inline">\(C\)</span> matrices <span class="math inline">\(n\times p\)</span> sobre el mismo cuerpo, y <span class="math inline">\(\lambda\)</span> un escalar. Entonces <span class="math inline">\(A(\lambda B + C)=\lambda(AB)+ AC\)</span>. Si <span class="math inline">\(B\)</span> y <span class="math inline">\(C\)</span> matrices <span class="math inline">\(q\times m\)</span> sobre el mismo cuerpo, y <span class="math inline">\(\lambda\)</span> un escalar. Entonces <span class="math inline">\((\lambda B + C)A=\lambda(BA)+ CA\)</span>.</p>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-280" class="theorem"><strong>Teorema 4.14  </strong></span> Sea <span class="math inline">\(V\)</span> un espacio vectorial. Entonces la intersección de una colección arbitraria de subespacios de <span class="math inline">\(V\)</span>, es un subespacio de <span class="math inline">\(V\)</span>.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sea <span class="math inline">\(\{W_{\alpha} \}\)</span> una colección de subespacios de <span class="math inline">\(V\)</span>. Sea <span class="math inline">\(W=\bigcap _{\alpha} W_{\alpha}\)</span> la intersección de la colección de subespacios. Note que el vector cero pertece a todo <span class="math inline">\(W_{\alpha}\)</span> ya que cada <span class="math inline">\(W_{\alpha}\)</span> es un subespacio de <span class="math inline">\(V\)</span>; por lo tanto el vector cero pertenece a la intersección <span class="math inline">\(W\)</span>, así <span class="math inline">\(W\)</span> no es vacío. Sean <span class="math inline">\(u,v\in W\)</span>, y <span class="math inline">\(\lambda\)</span> un escalar. Entonces <span class="math inline">\(u,v\in W_{\alpha}\)</span> para todo <span class="math inline">\(\alpha\)</span>, como cada <span class="math inline">\(W_{\aleph}\)</span> es un subespacio, se tiene que <span class="math inline">\(\lambda u + v\in W_{\alpha}\)</span> para todo <span class="math inline">\(\alpha\)</span>, de donde se sigue que <span class="math inline">\(\lambda u +v\in W\)</span>, lo que muestra que <span class="math inline">\(W\prec V\)</span>.</p>
</div>

<p>Del teorema anterior dado un subconjunto no vacío <span class="math inline">\(S\)</span> de un espacio vectorial, podemos construír un subespacio que contenga a <span class="math inline">\(S\)</span> y que sea mínimo (en el sentido de la contención).</p>

<div class="definition">
<p><span id="def:unnamed-chunk-282" class="definition"><strong>Definición 4.12  </strong></span> Sea <span class="math inline">\(S\)</span> un conjunto de vectores de un espacio vectorial <span class="math inline">\(V\)</span>. El <em>subespacio generado</em> por <span class="math inline">\(S\)</span> es la intersección de todos los subespacios que contienen a <span class="math inline">\(S\)</span> y se denota <span class="math inline">\(\left\langle S \right\langle\)</span>. Es decir, <span class="math inline">\(\left\langle S \right\rangle =\bigcap \{W\prec V : S\subseteq W \}\)</span>. Cuando <span class="math inline">\(S\)</span> es un conjunto finito de vectores, <span class="math inline">\(u_{1}, u_{2},\cdots u_{n}\)</span> se denota <span class="math inline">\(\left\langle u_{1}, u_{2},\cdots u_{n} \right\rangle\)</span> y dicimos simplemente que <span class="math inline">\(\left\langle u_{1}, u_{2},\cdots u_{n} \right\rangle\)</span> es el subespacio generado por los vectores <span class="math inline">\(u_{1}, u_{2},\cdots u_{n}\)</span>.</p>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-283" class="theorem"><strong>Teorema 4.15  </strong></span> Sea <span class="math inline">\(V\)</span> un espacio vectorial y <span class="math inline">\(S\subseteq V\)</span> no vacío. El subespacio generado por <span class="math inline">\(S\)</span>, <span class="math inline">\(\left\langle S \right\rangle\)</span>, es el conjunto de todas las combinaciones lineales de los vectores de <span class="math inline">\(S\)</span>.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sea <span class="math inline">\(L(S)\)</span> el conjunto de todas las combinaciones lineales de <span class="math inline">\(S\)</span>, es decir, <span class="math inline">\(L(S)=\{\sum_{i=1}^{n} \lambda_{i} u_{i}: \lambda_{i}\mbox{ es un escalar, } u_{i}\in S, n\in\mathbb{N} \}\)</span>. Es claro que <span class="math inline">\(L(S)\subseteq \left\langle S \right\rangle\)</span>, ya que toda combinación lineal de elementos de <span class="math inline">\(S\)</span> es un elemento de <span class="math inline">\(\left\langle S \right\rangle\)</span> (por definición de subespacio). Ahora, veamos la otra contención. Para esto, veamos que <span class="math inline">\(L(S)\)</span> es un subespacio vectorial que contiene a <span class="math inline">\(S\)</span> (y así tendremos que <span class="math inline">\(\left\langle S \right\rangle\subseteq L(S)\)</span>). Primero, como <span class="math inline">\(S\subseteq L(S)\)</span> se tiene que <span class="math inline">\(L(S)\)</span> no es vacío. Sean <span class="math inline">\(v, w\in L(S)\)</span>, como combinaciones lineales de combinaciones lineales, vuelven a ser combinaciones lineales (del mismo conjunto de vectores) entonces <span class="math inline">\(\gamma v + w\in L(S)\)</span>, lo que muestra que <span class="math inline">\(L(S)\)</span> es un subespacio de <span class="math inline">\(V\)</span> que contiene al conjunto <span class="math inline">\(S\)</span>, por ser <span class="math inline">\(\left\langle S \right\rangle\)</span> el menor subespacio con esta propiedad, se tiene que <span class="math inline">\(\left\langle S \right\rangle\subseteq L(S)\)</span>. Por lo que <span class="math inline">\(\left\langle S \right\rangle = L(S)\)</span>.</p>
</div>
<p> Se puede usar <span class="math inline">\(L(S)\)</span> o <span class="math inline">\(\left\langle S \right\rangle\)</span> indistintamente para denotar el subespacio generado por <span class="math inline">\(S\)</span>.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-285" class="definition"><strong>Definición 4.13  </strong></span> Sean <span class="math inline">\(S_{1}, S_{2},\cdots, S_{n}\subseteq V\)</span>, la suma de <span class="math inline">\(S_{1}, S_{2},\cdots, S_{n}\)</span> es el conjunto de todos las sumas <span class="math inline">\(v_{1}+v_{2}+\cdots + v_{n}\)</span> con <span class="math inline">\(v_{i}\in S_{i}\)</span>, para cada <span class="math inline">\(1\leq i\leq n\)</span> y se denota <span class="math inline">\(S_{1}+S_{2}+\cdots+S_{n}\)</span> o <span class="math inline">\(\sum_{i=1}^{n} S_{i}\)</span>.</p>
</div>

<p>Note que si <span class="math inline">\(W_{1}, W_{2}, \cdots, W_{n}\)</span> son subespacios de un espacio vectorial <span class="math inline">\(V\)</span>, entonces <span class="math inline">\(W=W_{1}+W_{2}+\cdots +W_{n}\)</span> es subespacio de <span class="math inline">\(V\)</span>, que contienen a cada <span class="math inline">\(W_{i}\)</span>, con <span class="math inline">\(1\leq i\leq n\)</span>. Entonces <span class="math inline">\(W=W_{1}+W_{2}+\cdots +W_{n}=\left\langle W_{1}\cup W_{2}\cdots W_{n}\right\rangle\)</span>.</p>

<div class="example">
<p><span id="exm:ejm361" class="example"><strong>Ejemplo 4.20  </strong></span> Sean <span class="math inline">\(v_{1}=(1,0,2,0,1), v_{2}=(0,1,0,3,2), v_{3}=(0,0,1,0,0)\)</span> vectores del espacio <span class="math inline">\(\mathbb{R}^{5}\)</span>. El subespacio generado por <span class="math inline">\(v_{1}, v_{2}, v_{3}\)</span> es el espacio <span class="math inline">\(\left\langle v_{1}, v_{2}, v_{3}\right\rangle =\{\lambda_{1}v_{1}+\lambda_{2}v_{2}+\lambda_{3}v_{3}: \lambda_{1}, \lambda_{2},\lambda_{3}\in\mathbb{R} \}\)</span>. Como <span class="math inline">\(\alpha=\lambda_{1}v_{1}+\lambda_{2}v_{2}+\lambda_{3}v_{3}=(\lambda_{1},\lambda_{2},2\lambda_{1}+\lambda_{3},3\lambda_{2},\lambda_{1}+2\lambda_{2})\)</span>, se tiene que <span class="math inline">\(\left\langle v_{1}, v_{2}, v_{3}\right\rangle =\{v\in\mathbb{R}^{5}:(\lambda_{1},\lambda_{2},2\lambda_{1}+\lambda_{3},3\lambda_{2},\lambda_{1}+2\lambda_{2})\mbox{ con } \lambda_{1}, \lambda_{2},\lambda_{3}\in\mathbb{R}\}\subseteq\mathbb{R}^{5}\)</span>.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-286" class="example"><strong>Ejemplo 4.21  </strong></span> Las matrices de la forma <span class="math display">\[\left[ \begin{array}{cc} 
    a &amp; b\\
    c &amp; 0
    \end{array}\right]\mbox{ con }a,b,c\in\mathbb{R}\]</span> forman un subespacio de <span class="math inline">\(\mathbb{R}^{2\times 2}\)</span>. Llamemos <span class="math inline">\(W_{1}=\{A\in\mathbb{R}^{2\times 2}: a_{22}=0 \}\)</span>. Sean <span class="math inline">\(A, B\in W_{1}\)</span>, entonces <span class="math inline">\(\lambda A + B\in W_{1}\)</span>, pues <span class="math inline">\([\lambda A + B\in W_{1}]_{ij}=\lambda a_{ij} + b_{ij}\)</span> y así, <span class="math inline">\([\lambda A + B\in W_{1}]_{22}=\lambda a_{22} + b_{22}=0\)</span>. Las matrices de la forma <span class="math display">\[\left[ \begin{array}{cc} 
    a &amp; 0\\
    0 &amp; b
    \end{array}\right] \mbox{ con } a,b,c\in\mathbb{R}\]</span> también son un subespacio de <span class="math inline">\(\mathbb{R}^{2\times 2}\)</span>. Si llamemos <span class="math inline">\(W_{2}=\{A\in\mathbb{R}^{2\times 2}: a_{12}=a_{21}=0 \}\)</span>, se tiene que <span class="math inline">\(\mathbb{R}^{2\times 2}=W_{1}+W_{2}\)</span>. También se puede ver que <span class="math inline">\(W_{1}\cap W_{2}\)</span> es el subespacio de las matrices de la forma <span class="math display">\[\left[ \begin{array}{cc} 
    a &amp; 0\\
    0 &amp; 0
    \end{array}\right]\mbox{ con } a\in\mathbb{R}\]</span>.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-287" class="example"><strong>Ejemplo 4.22  </strong></span> Sea <span class="math inline">\(A\)</span> una matriz <span class="math inline">\(m\times n\)</span>, sobre un cuerpo <span class="math inline">\(\mathbb{F}\)</span>. Los <em>vectores filas</em> de la matriz <span class="math inline">\(A\)</span> son vectores de <span class="math inline">\(\mathbb{R}^{n}\)</span> dados por <span class="math inline">\(A_{i\ast}=(a_{i1},a_{i2},\cdots, a_{in})\)</span>, con <span class="math inline">\(1\leq i\leq n\)</span>. El espacio fila de la matriz <span class="math inline">\(A\)</span> es el espacio generado por los vectores fila, es decir, $A_{1},A_{2}, A_{m} $. El subespacio visto en el ejemplo <a href="espacios-vectoriales.html#exm:ejm361">4.20</a> es exactamente el espacio fila de la matriz <span class="math display">\[\left[ \begin{array}{ccccc} 
    1 &amp; 0 &amp; 2 &amp; 0 &amp; 1 \\
    0 &amp; 1 &amp; 0 &amp; 3 &amp; 2 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0
    \end{array}\right]\]</span> Pero también es el espacio fila de la siguiente matriz <span class="math display">\[\left[ \begin{array}{ccccc} 
    1 &amp; 0 &amp; 2 &amp; 0 &amp; 1 \\
    0 &amp; 1 &amp; 0 &amp; 3 &amp; 2 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
    -2 &amp; -1 &amp; -3 &amp; -3 &amp; -4
    \end{array}\right]\]</span> Ya que la última fila es combinación lineal de las anteriores. Sim embargo, el espacio fila de la matriz <span class="math display">\[\left[ \begin{array}{ccccc} 
    1 &amp; 0 &amp; 2 &amp; 0 &amp; 1 \\
    0 &amp; 1 &amp; 0 &amp; 3 &amp; 2 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1 &amp; -1
    \end{array}\right]\]</span> es diferente, esto es porque las filas de esta matriz son linealmente independientes.</p>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-288" class="definition"><strong>Definición 4.14  </strong></span> Un subconjunto <span class="math inline">\(S\)</span> de un espacio vectorial <span class="math inline">\(V\)</span> es <em>linealmente dependiente</em> (se puede abreviar l.d.) si existen vectores <span class="math inline">\(v_{1}, v_{2},\cdots ,v_{n}\in S\)</span> distintos y escalares <span class="math inline">\(\lambda_{1}, \lambda_{2}, \cdots , \lambda_{n}\)</span> no todos nulos tales que la combinación lineal <span class="math inline">\(\lambda_{1}v_{1}+\lambda_{2}v_{2}+\cdots +\lambda_{n}v_{n}=0\)</span> sea igual al vector cero. Un conjunto que no es linealmente dependiente, se llama <em>linealmente independiente</em>. Se puede abreviar l.i.</p>
</div>

<p>Si el conjunto es finito <span class="math inline">\(S=\{v_{1}, v_{2},\cdots ,v_{n}\}\)</span> podemos decir directamente que los vectores <span class="math inline">\(v_{1}, v_{2},\cdots ,v_{n}\)</span> son l.d. o l.i. (según corresponda) obviando el nombre del conjunto <span class="math inline">\(S\)</span>.</p>

<div class="remark">
<p> <span class="remark"><em>Nota. </em></span> Se tiene que:</p>
<pre><code>    1. Todo conjunto que contiene a un conjunto l.d., es un conjunto l.d.
    2. Todo subconjunto de un conjunto l.i., es l.i.
    3. Todo conjunto que contenga el vector cero es l.d.
    4. Un conjunto $S$ es l.i. si y solo si toda combinación lineal de vectores de $S$ que es igual a cero, implica que todos los escalares de dicha combinación lineal son iguales a cero, es decir, si para todo $n\in \mathbb{N}$, $\lambda_{1}v_{1}+\lambda_{2}v_{2}+\cdots +\lambda_{n}v_{n}=0$, donde $v_{i}\in S$ para $0\leq i\leq n$, entonces $\lambda_{1}=\lambda_{2}= \cdots =\lambda_{n}=0$.
&lt;/div&gt;\EndKnitrBlock{remark}</code></pre>

<div class="definition">
<p><span id="def:unnamed-chunk-290" class="definition"><strong>Definición 4.15  </strong></span> <em>Una base</em> de un espacio vectorial <span class="math inline">\(V\)</span>, es un conjunto de vectores linealmente independientes de <span class="math inline">\(V\)</span>, cuyo espacio generado coincide con <span class="math inline">\(V\)</span>. El espacio <span class="math inline">\(V\)</span> es de <em>dimensión finita</em> si tiene una base finita.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-291" class="example"><strong>Ejemplo 4.23  </strong></span> Los vectores <span class="math inline">\(v_{1}=(1,0,2,0,1), v_{2}=(0,1,0,3,2), v_{3}=(0,0,1,0,0), v_{4}=(2,1,5,3,4)\)</span> son linealmente dependientes, ya que <span class="math inline">\(2v_{1}+v_{2}+v_{3}-v_{4}=0\)</span>. Pero <span class="math inline">\(v_{1}=(1,0,2,0,1), v_{2}=(0,1,0,3,2), v_{3}=(0,0,1,0,0)\)</span> son linealmente independientes, ya que <span class="math inline">\(\lambda v_{1}+\lambda v_{2}+\lambda v_{3}=0\)</span> solo si <span class="math inline">\(\lambda_{1}=\lambda_{2}=\lambda_{3}=0\)</span>. Note que cualquier otra combinación lineal (de dos o menos vectores) es igual a cero solo cuando sus escalares son todos cero.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-292" class="example"><strong>Ejemplo 4.24  </strong></span> Sean <span class="math inline">\(e_{1}, e_{2}, \cdots ,e_{n}\)</span> vectores de <span class="math inline">\(\mathbb{R}^{n}\)</span> tales que <span class="math display">\[[e_{i}]_{j}=\left\lbrace \begin{array}{cc}
    1 &amp; \mbox{ si } i=j\\
    0 &amp; \mbox{ si } i\neq j
    \end{array}\right. \]</span> donde <span class="math inline">\([e_{i}]_{j}\)</span> denota la <span class="math inline">\(j-\)</span>ésima posición (entrada o coordenada) del vector <span class="math inline">\(e_{i}\)</span>. Es decir, los vectores <span class="math inline">\(e_{1}, e_{2}, \cdots ,e_{n}\)</span> son <span class="math inline">\(n\)</span> vectores casi iguales a cero, salvo que en la coordenada <span class="math inline">\(i-\)</span>ésima <span class="math inline">\(e_{i}\)</span> es igual a <span class="math inline">\(1\)</span>. Serían así: <span class="math display">\[\begin{array}{cl}
    e_{1}=&amp; (1, 0, 0, \cdots , 0, 0)\\
    e_{2}=&amp; (0, 1, 0, \cdots , 0, 0)\\
    \vdots &amp; \vdots \\
    e_{i}=&amp; (0, \cdots, 1,\cdots , 0)\mbox{ , en la posición } i\\
    \vdots &amp; \vdots \\
    e_{n}=&amp; (0, 0, \cdots, 0, 0, 1)
    \end{array}\]</span> Sea <span class="math inline">\(v=(x_{1}, x_{2},\cdots, x_{n})\in \mathbb{R}^{n}\)</span> un vector cualquiera. Se tiene que <span class="math inline">\(v=(x_{1}, x_{2},\cdots, x_{n})=x_{1}e_{1}+x_{2}e_{2}+\cdots+ x_{n}e_{n}\)</span>, lo que muestra que todo vector de <span class="math inline">\(\mathbb{R}^{n}\)</span> se puede escribir como combinación lineal de los vectores <span class="math inline">\(e_{1}, e_{2}, \cdots ,e_{n}\)</span>, o lo que es igual, que estos vectores generan a todo el espacio, es decir, <span class="math inline">\(\left\langle e_{1}, e_{2}, \cdots, e_{n}\right\rangle=\mathbb{R}^{n}\)</span>. Ahora es fácil ver que <span class="math inline">\(\{e_{1}, e_{2}, \cdots ,e_{n}\}\)</span> es un conjunto l.i., por lo tanto forman una base para el espacio <span class="math inline">\(\mathbb{R}^{n}\)</span>. Lo que nos permite decir que <span class="math inline">\(\mathbb{R}^{n}\)</span> es un espacio de dimensión finita. A la base <span class="math inline">\(\{e_{1}, e_{2}, \cdots ,e_{n}\}\)</span> se le conoce con el nombre de base canónica de <span class="math inline">\(\mathbb{R}^{n}\)</span>.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-293" class="example"><strong>Ejemplo 4.25  </strong></span> Sea <span class="math inline">\(A\in \mathcal{M}_{n}(\mathbb{F})\)</span> invertible. Sean <span class="math inline">\(A_{1\ast}, A_{2\ast},\cdots, A_{n\ast}\)</span> los vectores fila de la <span class="math inline">\(A\)</span>. <span class="math inline">\(A_{1\ast}, A_{2\ast},\cdots, A_{n\ast}\)</span> son l.i. ya que <span class="math inline">\(\lambda_{1} A_{1\ast}+\lambda_{2} A_{2\ast}+\cdots+\lambda_{n} A_{n\ast}=0\)</span> si y solo si <span class="math inline">\(AX=0\)</span>, donde <span class="math inline">\(X\)</span> es la matriz <span class="math inline">\(n\times 1\)</span> formada por los <span class="math inline">\(\lambda_{i}\)</span>. Como <span class="math inline">\(AX=0\Leftrightarrow X=A^{-1}0\Leftrightarrow \lambda_{1}=\lambda_{2}=\cdots=\lambda_{n}=0\)</span>. Además forman una base del espacio de las matrices filas <span class="math inline">\(\mathcal{M}_{1\times n}(F)\)</span>: Sea <span class="math inline">\(B=(b_{11} b_{12},\cdots,b_{1n})\)</span>, sea <span class="math inline">\(X=A^{-1}B\Leftrightarrow AX=B\)</span>. Luego, el espacio de las matrices filas <span class="math inline">\(\mathcal{M}_{1\times n}(F)\)</span>, es un espacio de dimensión finita.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-294" class="example"><strong>Ejemplo 4.26  </strong></span> El espacio de las funciones polinómicas sobre <span class="math inline">\(\mathbb{R}\)</span>. Sea <span class="math inline">\(S=\{f_{k}(x)=x^{k}: k\geq 0 \}=\{1, x, x^{2},\cdots \}\)</span>. Sea <span class="math inline">\(n\)</span> un entero positivo, consideremos la combinación lineal <span class="math inline">\(f(x)=\lambda_{0}+\lambda_{1}x+\lambda_{2}x^{2}+\cdots + \lambda_{n}x^{n}\)</span>, entonces <span class="math inline">\(f(x)=0\)</span> para todo <span class="math inline">\(x\in \mathbb{F}\)</span> si y solo si <span class="math inline">\(\lambda_{1}=\lambda_{2}=\cdots=\lambda_{n}=0\)</span>. Por lo tanto <span class="math inline">\(S\)</span> es un conjunto linealmente independiente. Cualquier subconjunto finito de <span class="math inline">\(S\)</span> no genera a todo el espacio, Lo que demuestra que el espacio de las funciones polinómicas es de dimensión infinita.</p>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-295" class="theorem"><strong>Teorema 4.16  </strong></span> Sea <span class="math inline">\(V\)</span> un espacio vectorial generado por un conjunto finito de <span class="math inline">\(n\)</span> vectores <span class="math inline">\(v_{1}, v_{2},\cdots, v_{n}\)</span>. Entonces todo subconjunto de <span class="math inline">\(V\)</span> linealmente independiente no tiene mas de <span class="math inline">\(n\)</span> vectores.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Veamos que <span class="math inline">\(S=\{u_{1}, u_{2}, \cdots, u_{m} \}\)</span>, con <span class="math inline">\(m\geq n\)</span>, es linealmente dependiente. Como <span class="math inline">\(v_{1}, v_{2},\cdots, v_{n}\)</span> generan a todo el espacio <span class="math inline">\(V\)</span>, cada <span class="math inline">\(u_{i}\)</span> se escribe como combinación lineal de los <span class="math inline">\(v_{j}\)</span>, es decir, <span class="math display">\[\begin{array}{cc}
        u_{1}=&amp;\lambda_{11}v_{1}+\lambda_{12}v_{2}+\cdots +\lambda_{1n}v_{n}\\
        u_{2}=&amp;\lambda_{21}v_{1}+\lambda_{22}v_{2}+\cdots +\lambda_{2n}v_{n}\\
        \vdots&amp; \ddots\\
        u_{m}=&amp;\lambda_{m1}v_{1}+\lambda_{m2}v_{2}+\cdots +\lambda_{mn}v_{n}
    \end{array} \mbox{ donde } \lambda_{ij} \mbox{ es un escalar }\]</span> Tomemos una combinación lineal de los vectores de <span class="math inline">\(S\)</span> y la igualamos a cero <span class="math inline">\(x_{1} u_{1}+ x_{2} u_{2}+ \cdots+ x_{m} u_{m}=0\)</span>, esto es <span class="math display">\[(x_{1},x_{2},\cdots, x_{m}) \left(\left[ \begin{array}{cccc}
        \lambda_{11} &amp; \lambda_{12} &amp; \cdots &amp; \lambda_{1n}\\
        \lambda_{21} &amp; \lambda_{22} &amp; \cdots &amp; \lambda_{2n}\\
        \vdots &amp; &amp; \ddots &amp; \\
        \lambda_{m1} &amp; \lambda_{m2} &amp; \cdots &amp; \lambda_{mn}\\
    \end{array}
    \right] \left[ \begin{array}{c}
        v_{1}\\
        v_{2}\\
        \vdots\\
        v_{n}
    \end{array} \right]  \right)=0 \]</span> Es decir, <span class="math display">\[(x_{1},x_{2},\cdots, x_{m}) \left[ \begin{array}{cccc}
    \lambda_{11} &amp; \lambda_{12} &amp; \cdots &amp; \lambda_{1n}\\
    \lambda_{21} &amp; \lambda_{22} &amp; \cdots &amp; \lambda_{2n}\\
    \vdots &amp; &amp; \ddots &amp; \\
    \lambda_{m1} &amp; \lambda_{m2} &amp; \cdots &amp; \lambda_{mn}\\
    \end{array}
    \right] =0\Leftrightarrow \left[ \begin{array}{cccc}
        \lambda_{11} &amp; \lambda_{12} &amp; \cdots &amp; \lambda_{1n}\\
        \lambda_{21} &amp; \lambda_{22} &amp; \cdots &amp; \lambda_{2n}\\
        \vdots &amp; &amp; \ddots &amp; \\
        \lambda_{m1} &amp; \lambda_{m2} &amp; \cdots &amp; \lambda_{mn}\\
    \end{array}
    \right] \left[ \begin{array}{c}
    x_{1}\\
    x_{2}\\
    \vdots\\
    x_{n}
    \end{array} \right] =0\]</span> Como <span class="math inline">\(m&gt;n\)</span> tiene múltiples soluciones, esto es, existen <span class="math inline">\(x_{1},x_{2},\cdots, x_{m}\)</span> no todos ceros tales que <span class="math inline">\(x_{1} u_{1}+ x_{2} u_{2}+ \cdots+ x_{m} u_{m}=0\)</span>, por lo tanto <span class="math inline">\(S\)</span> es linealmente dependiente.</p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-297" class="corollary"><strong>Corolario 4.6  </strong></span> Si <span class="math inline">\(V\)</span> en un espacio de dimensión finita, entonces dos bases cualesquiera tienen el mismo número (finito) de elementos.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Como <span class="math inline">\(V\)</span> es de dimensión finita, tiene una base finita, digamos <span class="math inline">\(B=\{v_{1}, v_{2}, \cdots, v_{n} \}\)</span>. Sea <span class="math inline">\(B´\)</span> una base de <span class="math inline">\(V\)</span>, entonces <span class="math inline">\(B´\)</span> tiene una cantidad finita de vectores, digamos <span class="math inline">\(B´=\{u_{1}, u_{2}, \cdots, u_{m} \}\)</span> con <span class="math inline">\(m\leq n\)</span>. Análogamente, <span class="math inline">\(n\leq m\)</span>, por lo tanto <span class="math inline">\(n=m\)</span>.</p>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-299" class="definition"><strong>Definición 4.16  </strong></span> Llamaremos <em>dimensión</em> al número de vectores de una base de un espacio de dimensión finita y se denota <span class="math inline">\(\dim V\)</span>.</p>
</div>

<p>Podemos reformular el teorema anterior en términos de la definición anterior.</p>

<div class="corollary">
<p><span id="cor:coro374" class="corollary"><strong>Corolario 4.7  </strong></span> Sea <span class="math inline">\(V\)</span> un espacio de dimensión finita y sea <span class="math inline">\(n=\dim V\)</span>. Entonces:</p>
<pre><code>    (1) Cualquier conjunto $S\subseteq V$ que contenga más de $n$ vectores es l.d.
    (2) Ningún subconjunto de $V$ que contenga menos de $n$ vectores puede generar a $V$.</code></pre>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-300" class="example"><strong>Ejemplo 4.27  </strong></span><br />
(1) <span class="math inline">\(\mathbb{R}^{n}\)</span> tiene dimensión <span class="math inline">\(n\)</span>. Ya vimos antes que la base canónica de <span class="math inline">\(\mathbb{R}^{n}\)</span> está formada por los vectores <span class="math inline">\(e_{1}, e_{2}, \cdots ,e_{n}\)</span>,</p>
<p><span class="math display">\[[e_{i}]_{j}=\left\lbrace \begin{array}{cc}
        1 &amp; \mbox{ si } i=j\\
        0 &amp; \mbox{ si } i\neq j
        \end{array}\right. \]</span> Luego, toda base de <span class="math inline">\(\mathbb{R}^{n}\)</span> tiene <span class="math inline">\(n\)</span> vectores.</p>
<pre><code>    (2) El espacio de las matrices de orden $m\times n$, $\mathcal{M}_{m\times n}(\mathbb{F})$, tiene como base al conjunto de matrices $E^{ij}$ de la forma
    $$[E^{ij}]st=\left\{\begin{array}{cc}
    1 &amp; \mbox{ si } i=s \mbox{ y } j=t\\
    0 &amp; \mbox{ en otro caso }
    \end{array}\right. $$
      
    Hay $mn$ matrices de esta forma, por lo tanto la dimensión del espacio $\mathcal{M}_{m\times n}(\mathbb{F})$ es $mn$.
    
    (3) Dada una matriz $A$, el espacio soluci\ón del sistema homogéneo $AX=0$, tiene dimensión igual al número de filas no nulas de la matriz escalonada reducida por filas equivalente a $A$.
&lt;/div&gt;\EndKnitrBlock{example}</code></pre>
<p>Vamos a convenir dar dimensión cero al espacio nulo.</p>

<div class="lemma">
<p><span id="lem:unnamed-chunk-301" class="lemma"><strong>Lema 4.3  </strong></span> Sea <span class="math inline">\(S\)</span> un subconjunto linealmente independiente de un espacio vectorial <span class="math inline">\(V\)</span>. Sea <span class="math inline">\(v\in V\setminus\left\langle S \right\rangle\)</span>. Entonces <span class="math inline">\(S\cup \{v\}\)</span> es linealmente independiente.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sean <span class="math inline">\(u_{1}, u_{2},\cdots, u_{n}\in S\)</span>; tomemos <span class="math inline">\(\lambda_{1}u_{1}+\lambda_{2}u_{2}+\cdots +\lambda_{n}u_{n}+\gamma v=0\)</span>, necesariamente <span class="math inline">\(\gamma=0\)</span> porque en otro caso, <span class="math inline">\(v=\gamma^{-1}(\lambda_{1}u_{1}+\lambda_{2}u_{2}+\cdots +\lambda_{n}u_{n})=\gamma^{-1}\lambda_{1}u_{1}+\gamma^{-1}\lambda_{2}u_{2}+\cdots +\gamma^{-1}\lambda_{n}u_{n}\)</span> y así <span class="math inline">\(v\in\left\langle S \right\rangle\)</span>. Como <span class="math inline">\(S\)</span> es l.i. se tiene que si <span class="math inline">\(\lambda_{1}u_{1}+\lambda_{2}u_{2}+\cdots +\lambda_{n}u_{n}+\gamma v=0\)</span>, entonces <span class="math inline">\(\lambda_{1}u_{1}+\lambda_{2}u_{2}+\cdots +\lambda_{n}u_{n}=0\)</span>, por lo tanto, <span class="math inline">\(\lambda_{1}=\lambda_{2}=\cdots=\lambda_{n}=\gamma=0\)</span> de donde se sigue que <span class="math inline">\(S\cup \{v\}\)</span> es l.i.</p>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-303" class="theorem"><strong>Teorema 4.17  </strong></span> Sea <span class="math inline">\(V\)</span> un espacio de dimensión finita. Sea <span class="math inline">\(W\prec V\)</span>, entonces todo subconjunto <span class="math inline">\(S\subseteq W\)</span> l.i. es finito y es parte de una base de <span class="math inline">\(W\)</span> (esta base debe ser finita).</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sea <span class="math inline">\(n=\dim V\)</span>. Sea <span class="math inline">\(S=S_{0}\subseteq W\)</span> un conjunto l.i. Por el corolario , <span class="math inline">\(S_{0}\)</span> tiene a lo sumo <span class="math inline">\(n\)</span> vectores. Si <span class="math inline">\(S_{0}\)</span> genera a <span class="math inline">\(W\)</span>, esto es <span class="math inline">\(\left\langle S_{0}\right\rangle=W\)</span>, el mismo es la base de <span class="math inline">\(W\)</span>. Si no, existe <span class="math inline">\(v_{1}\in W\setminus\left\langle S_{0} \right\rangle\)</span>, y por el corolario anterior, <span class="math inline">\(S_{1}=S_{0}\cup\{v_{1}\}\)</span> es l.i. Si <span class="math inline">\(\left\langle S_{1}\right\rangle=W\)</span>, ya tenemos la base de <span class="math inline">\(W\)</span>, sino repetimos el proceso, tomamos <span class="math inline">\(v_{2}\in W\setminus\left\langle S_{1} \right\rangle\)</span> y construimos <span class="math inline">\(S_{2}=S_{1}\cup\{v_{2}\}\)</span> un conjunto l.i. Volvemos a preguntarnos si <span class="math inline">\(S_{2}\)</span> genera al subespacio <span class="math inline">\(W\)</span>, si la respuesta es afirmativa, conseguimos la base, si no, continuamos con este proceso hasta hallar <span class="math inline">\(k\)</span> vectores <span class="math inline">\(\{v_{1}, v_{2}, \cdots, v_{k}\}\)</span>, que completen una base para el subespacio <span class="math inline">\(W\)</span>, a saber <span class="math inline">\(S_{k}=S_{0}\cup \{v_{0}, v_{1}, \cdots, v_{k}\}\)</span>. Note que este proceso es finito ya que la dimensión de <span class="math inline">\(V\)</span> es finita.</p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-305" class="corollary"><strong>Corolario 4.8  </strong></span> Sea <span class="math inline">\(V\)</span> un espacio vectorial de dimensión finita. Sea <span class="math inline">\(W\)</span> un subespacio propio de <span class="math inline">\(V\)</span>. Entonces <span class="math inline">\(\dim W &lt; \dim V\)</span>, por lo tanto <span class="math inline">\(W\)</span> es de dimensión finita.</p>
</div>
 
<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Si <span class="math inline">\(W\)</span> es el subespacio nulo se tiene el resultado de forma trivial. Supongamos que <span class="math inline">\(W\)</span> no es el espacio nulo. Sea <span class="math inline">\(\vDash\in W\)</span>, entonces <span class="math inline">\(\{v\}\)</span> es l.i. y por el teorema anterior <span class="math inline">\(v\)</span> pertenece a <span class="math inline">\(B\)</span> una base finita de <span class="math inline">\(W\)</span>, y <span class="math inline">\(B\)</span> no tiene mas de <span class="math inline">\(\dim V\)</span> vectores. Por lo tanto <span class="math inline">\(\dim W\leq \dim V\)</span>. Pero como <span class="math inline">\(W\)</span> es un subespacio propio de <span class="math inline">\(V\)</span>, existe <span class="math inline">\(u\in V\setminus W\)</span>, y así <span class="math inline">\(B\cup\{u\}\)</span> es un conjunto l.i., por lo tanto cualquier base de <span class="math inline">\(V\)</span> tiene mas de <span class="math inline">\(\dim W\)</span> elementos, entonces <span class="math inline">\(\dim V &gt; \dim W\)</span>.</p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-307" class="corollary"><strong>Corolario 4.9  </strong></span> En todo espacio <span class="math inline">\(V\)</span> de dimensión finita, todo conjunto de vectores linealmente independientes es parte de una base.</p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-308" class="corollary"><strong>Corolario 4.10  </strong></span> Sea <span class="math inline">\(A\in\mathcal{M}_{n}(\mathbb{F})\)</span>. Si los vectores filas de <span class="math inline">\(A\)</span> son un conjunto linealmente independientes de <span class="math inline">\(\mathbb{F}^{n}\)</span>, entonces <span class="math inline">\(A\)</span> es invertible.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sean <span class="math inline">\(v_{1}, v_{2},\cdots, v_{n}\)</span> los vectores filas de <span class="math inline">\(A\)</span> (es decir <span class="math inline">\(v_{i}=A_{i*}\)</span> para cada <span class="math inline">\(1\leq i\leq n\)</span>) y supongamos que es un conjunto l.i. Sea <span class="math inline">\(W=\left\langle v_{1}, v_{2},\cdots, v_{n}\right\rangle\)</span>, entonces <span class="math inline">\(\dim W=n\)</span>, así <span class="math inline">\(W=\mathbb{F}^{n}\)</span>. Además, los vectores <span class="math inline">\(e_{i}\)</span>, <span class="math display">\[[e_{i}]_{j}=\left\lbrace \begin{array}{cc}
    1 &amp; \mbox{ si } i=j\\
    0 &amp; \mbox{ si } i\neq j
    \end{array}\right. \]</span> que forman la base canónica de <span class="math inline">\(\mathbb{F}^{n}\)</span> se escriben como combinación lineal de los vectores <span class="math inline">\(v_{1}, v_{2},\cdots, v_{n}\)</span>. Esto es, existen escalares <span class="math inline">\(\lambda_{ij}\in \mathbb{F}\)</span> tales que: <span class="math inline">\(e_{i}=\sum_{j=1}^{n} b_{ij}v_{j}\)</span> para cada <span class="math inline">\(1\leq i\leq n\)</span>. Es decir, <span class="math inline">\(BA=I\)</span>, donde <span class="math inline">\([B]_{ij}=b_{ij}\)</span>.</p>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-310" class="theorem"><strong>Teorema 4.18  </strong></span> Si <span class="math inline">\(W_{1}\)</span> y <span class="math inline">\(W_{2}\)</span> son subespacios de dimensión finita de un espacio vectorial <span class="math inline">\(V\)</span>, entonces <span class="math inline">\(W_{1}+W_{2}\)</span> es de dimensión finita y <span class="math inline">\(\dim W_{1}+\dim W_{2}=\dim (W_{1}\cap W_{2}) + \dim (W_{1}+W_{2})\)</span>.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Como <span class="math inline">\(W_{1}\cap W_{2}\)</span> es un subespacio vectorial y <span class="math inline">\(W_{1}\cap W_{2}\subseteq W_{1}\)</span> y <span class="math inline">\(W_{1}\cap W_{2}\subseteq W_{2}\)</span> se sigue que <span class="math inline">\(W_{1}\cap W_{2}\)</span> tiene una base finita <span class="math inline">\(B=\{v_{1}, v_{2},\cdots, v_{k} \}\)</span> que es parte de una base de <span class="math inline">\(W_{1}\)</span>, digamos <span class="math inline">\(B_{1}=\{v_{1}, v_{2},\cdots, v_{k}, u_{1}, u_{2}, \cdots, u_{m} \}\)</span> y de base de <span class="math inline">\(W_{2}\)</span>, digamos <span class="math inline">\(B_{2}=\{v_{1}, v_{2},\cdots, v_{k}, w_{1}, w_{2}, \cdots, w_{n} \}\)</span> así el subespacio <span class="math inline">\(W_{1}+W_{2}\)</span> es generado por los vectores <span class="math inline">\(v_{1}, v_{2},\cdots, v_{k}, u_{1}, u_{2}, \cdots, u_{m}, w_{1}, w_{2}, \cdots, w_{n}\)</span> los cuales forman un conjunto l.i. En efecto, tomemos la combinación lineal <span class="math inline">\(\sum_{i=1}^{k} \lambda_{i}v_{i} + \sum_{i=1}^{m} \gamma_{i}u_{1} + \sum_{i=1}^{n} \delta_{i}w_{i}=0\)</span> si y solo si <span class="math display">\[-\sum_{i=1}^{n} \delta_{i}w_{i}=\sum_{i=1}^{k} \lambda_{i}v_{i} + \sum_{i=1}^{m} \gamma_{i}u_{1}.\]</span> Luego <span class="math inline">\(-\sum_{i=1}^{n} c_{i}w_{i}\in W_{1}\)</span>. Además <span class="math inline">\(\sum_{i=1}^{n} c_{i}w_{i}\in W_{2}\)</span>, entonces <span class="math inline">\(\sum_{i=1}^{n} c_{i}w_{i}\in W_{1}\cap W_{2}\)</span> por lo que <span class="math display">\[\sum_{i=1}^{n} c_{i}w_{i}=\sum_{i=1}^{k} a_{i}v_{i},\]</span> Entonces <span class="math inline">\(\sum_{i=1}^{n} c_{i}w_{i}-\sum_{i=1}^{k} a_{i}v_{i}=0\)</span>. Como <span class="math inline">\(B_{2}\)</span> es una base, se tiene que <span class="math inline">\(\lambda_{1}=\lambda_{2}=\cdots=\lambda_{k}=\delta_{1}=\delta_{2}=\cdots=\delta_{n}=0\)</span>, en particular <span class="math inline">\(\delta_{i}=0\)</span> para cada <span class="math inline">\(1 \leq i\leq n\)</span>. De este modo <span class="math display">\[\sum_{i=1}^{k} \lambda_{i}v_{i} + \sum_{i=1}^{m} \gamma_{i}u_{1}=0.\]</span> Como <span class="math inline">\(B_{1}\)</span> es base, <span class="math inline">\(\lambda_{1}=\lambda_{2}=\cdots=\lambda_{k}=\gamma_{1}=\gamma_{2}=\cdots=\gamma_{m}=0\)</span>, de donde se tiene que <span class="math inline">\(B_{1}\cup B_{2}\)</span> es base de <span class="math inline">\(W_{1}+W-{2}\)</span>. Finalmente, <span class="math inline">\(\dim W_{1}+\dim W_{2}=(k+m)+(k+n)=k+(k+m+n)=\dim (W_{1}\cup W_{2})+\dim (W_{1}+W_{2})\)</span>.</p>
</div>


<div class="definition">
<p><span id="def:unnamed-chunk-312" class="definition"><strong>Definición 4.17  </strong></span> El <em>rango fila</em> de una matriz <span class="math inline">\(A\)</span> de orden <span class="math inline">\(n\times m\)</span>, es la dimensión del espacio fila de la <span class="math inline">\(A\)</span>.</p>
</div>

<p>Note que, si <span class="math inline">\(P\in\mathcal{M}_{k\times m}(\mathbb{R})\)</span>, entonces <span class="math inline">\(B=PA\in\mathcal{M}_{k\times n}(\mathbb{R})\)</span> por lo tanto sus vectores fila son <span class="math inline">\(v=P_{i1}u_{1}+P_{i2}u_{2}+\cdots+P_{im}u_{m}\)</span>, donde <span class="math inline">\(P_{ij}=[P]_{i1}\)</span> y <span class="math inline">\(u_{j}=A_{j\ast}=(a_{j1}, a_{j2},\cdots, a_{jn})\)</span>. Entonces, el espacio fila de <span class="math inline">\(B\)</span> es un subespacio del espacio fila de <span class="math inline">\(A\)</span>. Si <span class="math inline">\(P\)</span> es una matriz invertible <span class="math inline">\(m\times m\)</span>, entonces <span class="math inline">\(A=P^{-1}B\)</span> y de forma análoga, el espacio fila de la matriz <span class="math inline">\(A\)</span> es subespacio del espacio fila de la matriz <span class="math inline">\(B\)</span>. Esto lo describimos en el siguiente teorema.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-313" class="theorem"><strong>Teorema 4.19  </strong></span> Las matrices equivalentes por filas tienen el mismo espacio fila.</p>
</div>

<p>Así solo basta estudiar el espacio fila de la matriz escalón reducida por filas.</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-314" class="theorem"><strong>Teorema 4.20  </strong></span> Sea <span class="math inline">\(R\)</span> una matriz escalón reducida por filas, no nula. Entonces los vectores filas no nulas de <span class="math inline">\(R\)</span> forman una base del espacio fila de <span class="math inline">\(R\)</span>.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sea <span class="math inline">\(R\in\mathcal{M}_{n}(\mathbb{R})\)</span> como en la hipótesis. Sean <span class="math inline">\(s_{1}, s_{2},\cdots, s_{r}\)</span> los vectores filas no nulos de <span class="math inline">\(R\)</span>, esto es, <span class="math inline">\(r_{i}=(s_{i1}, s_{i2},\cdots, s_{in})\)</span> para todo <span class="math inline">\(1\leq i\leq r\)</span>. Es claro que <span class="math inline">\(s_{1}, s_{2},\cdots, s_{r}\)</span> generan el espacio fila de <span class="math inline">\(R\)</span>. Como <span class="math inline">\(R\)</span> es escalón reducida por filas, se tiene que existen <span class="math inline">\(k_{1}&lt;k_{2}&lt;\cdots&lt;k_{r}\in\mathbb{N}\)</span> tales que <span class="math inline">\([R]_{ij}=0\)</span> si <span class="math inline">\(j&lt;k_{i}\)</span> y <span class="math inline">\([R]_{ik_{j}}=\delta_{ij}\)</span>. Consideremos <span class="math inline">\(\lambda_{1}s_{1}+ \lambda_{2}s_{2}+\cdots+ \lambda_{r}s_{r}=0\)</span> si y solo si <span class="math inline">\(\lambda_{1}=\lambda_{2}=\cdots=\lambda_{r}\)</span> ya que <span class="math inline">\(\lambda_{1}s_{1}+ \lambda_{2}s_{2}+\cdots+ \lambda_{r}s_{r}=(b_{1}, b_{2}, \cdots, b_{n})\)</span> si y solo si <span class="math inline">\(b_{k_{j}}=\sum_{i=1}^{r} \lambda_{i}s_{ik_{j}}=\lambda_{j}s_{jk_{j}}=\lambda_{j}\)</span> por lo tanto <span class="math inline">\(s_{1}, s_{2},\cdots, s_{r}\)</span> son l.i.</p>
</div>


<div class="theorem">
<p><span id="thm:unnamed-chunk-316" class="theorem"><strong>Teorema 4.21  </strong></span> Sean <span class="math inline">\(m\)</span> y <span class="math inline">\(n\)</span> enteros positivos y sea <span class="math inline">\(W\)</span> un subespacio de <span class="math inline">\(\mathbb{R}^{n}\)</span> tal que <span class="math inline">\(\dim W\leq m\)</span>. Entonces existe una única matriz escalón reducida por filas <span class="math inline">\(m\times n\)</span> que tiene a <span class="math inline">\(W\)</span> como espacio fila.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Sean <span class="math inline">\(v_{1}, v_{2}, \cdots, v_{m}\in\mathbb{R}^{n}\)</span> vectores generadores de <span class="math inline">\(W\)</span>. Sea <span class="math inline">\(A\)</span> una matriz <span class="math inline">\(m\times n\)</span> cuyas filas son los vectores <span class="math inline">\(v_{i}\)</span>, es decir, <span class="math inline">\(A_{i\ast}=v_{i}\)</span> para cada <span class="math inline">\(1\leq i\leq m\)</span>. Si <span class="math inline">\(R\)</span> es una matriz escalón reducida por filas equivalente a la matriz <span class="math inline">\(A\)</span>, entonces <span class="math inline">\(W\)</span> es el espacio fila de la matriz <span class="math inline">\(R\)</span> y por lo tanto de la matriz <span class="math inline">\(A\)</span>.</p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-318" class="corollary"><strong>Corolario 4.11  </strong></span> Cada matriz <span class="math inline">\(A\)</span> es equivalente por filas a una y solo una mtriz escalón reducida por filas.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Si dos matrices escalón reducida por filas tienen el mismo espacio fila, entonces ellas son identicas.</p>
</div>


<div class="corollary">
<p><span id="cor:unnamed-chunk-320" class="corollary"><strong>Corolario 4.12  </strong></span> Dos matrices <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> son equivalentes por filas si y solo si <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> tienen el mismo espacio fila.</p>
</div>


<div class="proof">
<p> <span class="proof"><em>Demostración. </em></span>  Es claro que si <span class="math inline">\(A\sim B\)</span>, entonces <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> tienen el mismo espacio fila. Supongamos que <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> tienen el mismo espacio fila. Sean <span class="math inline">\(R\)</span> y <span class="math inline">\(S\)</span> dos matrices escalonadas reducidas por fila, equivalentes a <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span> respectivamente. Como <span class="math inline">\(R\sim A\)</span> y <span class="math inline">\(B\sim S\)</span>, entonces <span class="math inline">\(R\)</span> y <span class="math inline">\(S\)</span> tienen el mismo espacio fila, luego <span class="math inline">\(R=S\)</span> de donde se tiene que <span class="math inline">\(A\sim B\)</span>.</p>
</div>

<div id="espacio-cociente" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Espacio cociente</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-322" class="definition"><strong>Definición 4.18  </strong></span> Sea <span class="math inline">\(W\)</span> un subespacio de un espacio vectorial <span class="math inline">\(V\)</span>. Si <span class="math inline">\(u\)</span> y <span class="math inline">\(v\)</span> son vectores en <span class="math inline">\(V\)</span>, diremos que <em><span class="math inline">\(u\)</span> es congruente con <span class="math inline">\(v\)</span> módulo <span class="math inline">\(W\)</span></em> si el vector <span class="math inline">\(v-u\in W\)</span>. Esto lo denotaremos <span class="math inline">\(u\cong v\mbox{ m\&#39;od }W\)</span>.</p>
</div>

<p>La congruencia módulo <span class="math inline">\(W\)</span>, es una relación de equivalencia sobre <span class="math inline">\(V\)</span>:</p>
<pre><code>(a) $u\cong u\mbox{ mód }W$, ya que $u-u=0\in W$.
(b) $u\cong v\mbox{ mód }W$ si y solo si $-1(u-v)\in W$ si y solo si $v\cong u\mbox{ mód }W$.
(c) Si $u\cong v\mbox{ mód }W$ y $v\cong w\mbox{ mód }W$, entonces $w-u=w-v+v-u\in W$ si y solo si $u\cong w\mbox{ mód } W$.

Las clases de equivalencia de un vector $v\in V$ es el conjunto $[v]:=\{u\in V: u\cong v\mbox{ mód } W \}=\{u\in V: v-u\in W \}=v+W$. A los conjuntos $v+W$ los llamaremos *clases laterales de $W$*.</code></pre>

<div class="remark">
<p> <span class="remark"><em>Nota. </em></span>  Podemos pensar que cada clase laterale <span class="math inline">\(v+W\)</span> es una traslaciÓn del subespacio <span class="math inline">\(W\)</span> por el vector <span class="math inline">\(v\)</span>.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-324" class="example"><strong>Ejemplo 4.28  </strong></span> Sea <span class="math inline">\(L\)</span> una recta en el plano <span class="math inline">\(\mathbb{R}^{2}\)</span> que pasa por el origen. Entonces <span class="math inline">\(L\)</span> es un subespacio de <span class="math inline">\(\mathbb{R}^{2}\)</span> y las clases laterales de la congruencia mÓdulo <span class="math inline">\(L\)</span> son <span class="math inline">\((x,y)+L\)</span>, para cada <span class="math inline">\((x,y)\in\mathbb{R}^{2}\)</span> y esto no es más que una recta paralela a <span class="math inline">\(L\)</span> que pasa por <span class="math inline">\((x,y)\)</span>.</p>
</div>

<p>La colección de todas las clases laterales de <span class="math inline">\(W\)</span> se denota <span class="math inline">\(V/W\)</span>. En este conjunto podemos definir la suma de clases y el producto de una clase por un escalar de la forma siguiente:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\((v+W)+(u+W)=((v+u)+W)\)</span>.</li>
<li><span class="math inline">\(\lambda(v+W)=(\lambda v)+W\)</span>.</li>
</ol>
<p>Estas operaciones están bien definidas, en efecto: Sean <span class="math inline">\(v_{1}\)</span>, y <span class="math inline">\(v_{2}\in V\)</span> tales que <span class="math inline">\(v_{1}\equiv v_{2}\mbox{ m\&#39;od }W\)</span> y sean <span class="math inline">\(u_{1}\)</span> y <span class="math inline">\(u_{2}\in V\)</span> tales que <span class="math inline">\(u_{1}\equiv u_{2}\mbox{ m\&#39;od }W\)</span>, <span class="math inline">\((v_{1}+u_{1})-(v_{2}+u_{2})=(v_{1}-v_{2})+(u_{1}-u_{2})\in W\)</span> por lo tanto <span class="math inline">\(v_{1}+u_{1}\equiv v_{2}+u_{2} \mbox{ m\&#39;od } W\)</span>. Sean <span class="math inline">\(\lambda\)</span> un escalar, <span class="math inline">\(\lambda v_{1}-\lambda v_{2}=\lambda(v_{1}-v_{2})\in W\)</span> por lo que <span class="math inline">\(\lambda v_{1}\equiv \lambda v_{2} \mbox{ m\&#39;od } W\)</span>. De esta forma, el conjunto <span class="math inline">\(V/W\)</span> junto a estas dos operaciones, es un espacio vectorial (sobre el mismo cuerpo de <span class="math inline">\(V\)</span>). La suma de clases es nuevamente una clase, así como el producto por un escalar. Las propiedades de ambas operaciones se heredan de las respectivas propiedades de <span class="math inline">\(V\)</span>. Note que el vector cero (nulo para la suma) es la clase <span class="math inline">\(0+W=W\)</span>. Daremos el nombre de <em>espacio cociente</em> al espacio <span class="math inline">\(V/W\)</span> con estas dos operaciones.</p>

<div class="definition">
<p><span id="def:unnamed-chunk-325" class="definition"><strong>Definición 4.19  </strong></span> Sean <span class="math inline">\(W_{1}\)</span> y <span class="math inline">\(W_{2}\)</span> subespacios de un espacio vectorial <span class="math inline">\(V\)</span>. La <em>suma directa</em> de <span class="math inline">\(W_{1}\)</span> y <span class="math inline">\(W_{2}\)</span> es el espacio dado por los vectores de la forma <span class="math inline">\(v_{1}+v_{2}\)</span> donde <span class="math inline">\(v_{1}\in W_{1}\)</span> y <span class="math inline">\(v_{2}\in W_{2}\)</span>, donde <span class="math inline">\(W_{1}\cap W_{2}=\{0\}\)</span>, este se denota por <span class="math inline">\(W_{1}\oplus W_{2}\)</span>.</p>
</div>


<div class="remark">
<p> <span class="remark"><em>Nota. </em></span>  El hecho que los subespacios sean disjuntos salvo por el vector nulo, es decir <span class="math inline">\(W_{1}\cap W_{2}=\{0\}\)</span>, implica que la representación de cualquier vector de <span class="math inline">\(W_{1}\oplus W_{2}\)</span> como la suma <span class="math inline">\(v_{1}+v_{2}\)</span> donde <span class="math inline">\(v_{1}\in W_{1}\)</span> y <span class="math inline">\(v_{2}\in W_{2}\)</span>, es única.</p>
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-327" class="example"><strong>Ejemplo 4.29  </strong></span><br />
(1) En <span class="math inline">\(\mathbb{R}^{2}\)</span>, dados los subespacios <span class="math inline">\(V_{i}=\{(x_{i},x_{2}): x_{j}\in \mathbb{R} y x_{j}=\delta_{ij} \}\)</span>, para <span class="math inline">\(i,j\leq 2\)</span> se tiene que <span class="math inline">\(\mathbb{R}^{2}=V_{1}\oplus V_{2}\)</span>. (2) En <span class="math inline">\(\mathcal{M}_{2}(\mathbb{R})\)</span>, dados <span class="math inline">\(V_{1}\)</span> el subespacio formado por las matrices de la forma <span class="math inline">\(\left[ \begin{array}{cc}  a &amp; b\\  0 &amp; 0  \end{array}\right]\)</span> y <span class="math inline">\(V_{2}\)</span> el subespacio formado por las matrices de la forma <span class="math inline">\(\left[ \begin{array}{cc}  0 &amp; 0\\  c &amp; d  \end{array}\right]\)</span> entonces <span class="math inline">\(\mathcal{M}_{2}(\mathbb{R})=V_{1}\oplus V_{2}\)</span>. (3) En <span class="math inline">\(\mathbb{R}^{3}\)</span>, dados los subespacios <span class="math inline">\(V_{1}\)</span> y <span class="math inline">\(V_{2}\)</span> los planos <span class="math inline">\(XY\)</span> y <span class="math inline">\(YZ\)</span> respectivamente, se tiene que <span class="math inline">\(\mathbb{R}^{3}=V_{1}+V_{2}\)</span>. (4) dados <span class="math inline">\(W_{1}\)</span> y <span class="math inline">\(W_{2}\)</span> las matrices de la forma $ a,b $ y <span class="math inline">\(\left[ \begin{array}{cc}  a &amp; b\\  c &amp; d  \end{array}\right]\mbox{ con } a,b,c,d \in\mathbb{R}\)</span>, se tiene que <span class="math inline">\(W_{1}\)</span> y <span class="math inline">\(W_{2}\)</span> son subespacios de <span class="math inline">\(\mathcal{M}_{2}(\mathbb{R})\)</span> pero <span class="math inline">\(\mathcal{M}_{2}(\mathbb{R})\neq W_{1}+W_{2}\)</span>. Esto es porque <span class="math inline">\(W_{1}\cap W_{2}\neq \{0\}\)</span>.</p>
</div>

</div>
</div>
<div id="ejercicios-3" class="section level2">
<h2><span class="header-section-number">4.5</span> Ejercicios:</h2>
<pre><code>(1) Muestre que el conjunto de las matrices hermíticas $n\times n$ es un espacio vectorial sobre el conjunto de los números reales $\mathbb{R}$.
Respuesta: Sea $V$ el conjunto de las matrices $n\times n$ de la forma 
$$\left[ \begin{array}{cccc}
    x_{11}+iy_{11} &amp; x_{12}+iy_{12} &amp; \cdots &amp; x_{1n}+iy_{1n}\\
        x_{21}+iy_{21} &amp; x_{22}+iy_{22} &amp; \cdots &amp; x_{2n}+iy_{2n}\\
        \vdots        &amp; \vdots        &amp; \ddots &amp; \vdots       \\
        x_{n1}+iy_{n1} &amp; x_{n2}+iy_{n2} &amp; \cdots &amp; x_{nn}+iy_{nn}
\end{array}\right] $$
tal que $a_{jk}=x_{jk}+iy_{jk}=x_{kj}-iy_{kj}=\overline{x_{kj}+iy_{kj}}=\overline{a_{kj}}$; sea el conjunto de los números reales $\mathbb{R}$, el conjunto de escalares y consideremos las operaciones de suma y producto por un escalar las usuales para las matrices. Entonces se tiene que:
(i) La suma de matrices en $V$ es cerrada: Sean $A, B\in V$, como $\overline{a_{jk}+b_{jk}}=\overline{a_{kj}}+\overline{b_{kj}}$, entonces $\forall A,B\in V$, $A+B\in V$.
(ii) La suma en $V$ es conmutativa: ya que la suma de números complejos es conmutativa.
(iii) La suma en $V$ es asociativa: se sigue de la propiedad asociativa de los números complejos.
(iv) Existe el elemento neutro: la matriz nula es hermítica.
(v) Existencia del elemento opuesto: dada la una matriz $A\in V$, $-A$ es hermítica.
(vi) El producto de una matriz por un escalar, es cerrada: como $\lambda \overline{x_{jk}+iy_{jk}}=\lambda (x_{jk}-iy_{jk})=\lambda x_{jk}-\lambda iy_{jk}$, se tiene que $\lambda A$ es hermítica, si $A$ es hermítica.
(vii) Claramente $1A=A$ para todo $A\in V$, ya que $1(x_{jk}+iy_{jk})=x_{jk}+iy_{jk}$.
(viii) Sean $\lambda,\gamma\in\mathbb{R}$ y $A\in V$, se tiene que $(\lambda\gamma) a_{jk}=(\lambda\gamma)(x_{jk}+iy_{jk})=(\lambda\gamma) x_{jk}+(\lambda\gamma) iy_{jk}=\lambda(\gamma x_{jk})+\lambda(\gamma iy_{jk})=\lambda(\gamma (x_{jk}+ iy_{jk}))=\lambda(\gamma a_{jk})$, por lo tanto $(\lambda\gamma)A=\lambda(\gamma A)$.
(ix) Para todo $A, B\in V$ y todo $\lambda\in\mathbb{R}$, $\lambda(a_{jk}+b_{jk})=\lambda a_{jk} + \lambda b_{jk}$ ya que el producto por un escalar es distributivo respecto de la suma de números complejos.
(x) Para todo $A\in V$ y todo $\lambda,\gamma\in\mathbb{R}$, $(\lambda+\gamma)a_{jk}=\lambda a_{jk} + \gamma a_{jk}$ ya que el producto por un escalar es distributivo respecto de la suma de números complejos.
De donde se sigue que $(V,\mathbb{R},+,\cdot)$ es un espacio vectorial.</code></pre>

</div>
</div>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-78759535-1', 'auto');
ga('send', 'pageview');  
</script>
            </section>

          </div>
        </div>
      </div>
<a href="vectores.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="matrices-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/synergyvision/Algebra/edit/master/bookdown/040-espacios-vectoriales.Rmd",
"text": "Edit"
},
"download": ["Algebra-Lineal.pdf"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
