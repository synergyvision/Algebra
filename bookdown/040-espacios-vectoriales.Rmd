# Espacios vectoriales
En este capítulo estudiaremos los espacios vectoriales, un concepto abstrato de una estructura cerrada bajo combinaciones lineales. Veremos que las matrices son objetos muy útiles para trabajar en espacios vectoriales. Por esta razón estudiaremos algunos aspectos básicos de estos objetos. Como motivación para el estudio de las matrices iniciaremos el capítulo estudiando las ecuaciones lineales y poco a poco iremos acercándonos al estudio de los espacios vectoriales.

## Ecuaciones lineales
En este sección estudiaremos las ecuaciones lineales como motivación para el estudio de los conceptos iniciales de matrices. Además estudiaremos las operaciones básicas de suma y producto por un escalar así como el producto de matrices. Luego estudiaremos el concepto de invertibilidad de matrices, para ello definiremos las operaciones elementales por filas. 

Sea $\mathbb{F}$ un cuerpo. Supongamos que queremos hallar $n$ escalares (elementos del cuerpo $\mathbb{F}$) $x_{1},x_{2}, \cdots, x_{n}$ que satisfagan las condiciones:
\begin{equation}
	\begin{array}{ccccc}
		A_{11}x_{1}+&A_{12}x_{2}+&\cdots +&A_{1n}x_{n}=&b_{1}\\
		A_{21}x_{1}+&A_{22}x_{2}+&\cdots +&A_{2n}x_{n}=&b_{2}\\
		\vdots& &\ddots& \vdots& \vdots	\\
		A_{m1}x_{1}+&A_{m2}x_{2}+&\cdots +&A_{mn}x_{n}=&b_{m}
	\end{array}
	(\#eq:sistemalineal)
\end{equation}
donde $b_{i}\in\mathbb{F}$  así como $A_{ij}\in \mathbb{F}$ para todo $1\leq i\leq m$ y $1\leq j\leq n$. Al conjunto de ecuaciones expresadas en \@ref(eq:sistemalineal) se le llama *sistema de $m$ ecuaciones lineales con $n$ incognitas*. A los elementos $A_{ij}$ se les conoce como *coeficientes* del sistema de ecuaciones, siendo específicamente el coeficiente de la $i$-ésima fila y la $j$-ésima columna. Una *solución* del sistema es una $n$-tupla $(x_{1},x_{2},\cdots,x_{n})$ (un vector del espacio $\mathbb{R}^{n}$) que satisfaga las ecuaciones \@ref(eq:sistemalineal). Cuando $b_{1}=b_{2}=\cdots b_{m}=0$ se dice que el *sistema de ecuaciones es homogéneo* (cada ecuación es homogénea).

Una forma de resolver un sistema de ecuaciones es con la técnica de eliminación de incógnitas, el cual consiste en multiplicar algunas de las ecuaciones por un escalar de forma que al sumar las ecuaciones se elimine algunas de las incógnitas. Veamos esto con un ejemplo.

```{example}
	Dado el siguiente sistema homogéneo, sobre el cuerpo de los números reales $\mathbb{R}$
	\begin{eqnarray*}
	\begin{array}{cc}
		x_{1}-4x_{2}+x_{3}&=0\\
		3x_{1}-11x_{2}+2x_{3}&=0
	\end{array}
	\end{eqnarray*}
	Multiplicamos la primera ecuación por el escalar $-3$ y la sumamos a la segunda ecuación, para obtener $-3(x_{1}-4x_{2}+x_{3})+(3x_{1}-11x_{2}+2x_{3})=0$, lo que queda como la siguiente $-3x_{1}+12x_{2}-3x_{3}+3x_{1}-11x_{2}+2x_{3}=0$, sumando términos independientes, obtenemos $x_{2}-x_{3}=0$ por lo tanto $x_{2}=x_{3}$. Ahora, multiplicando por $-2$ la primera ecuación y sumándola a la segunda, se obtiene $-2(x_{1}-4x_{2}+x_{3})+(3x_{1}-11x_{2}+2x_{3})=0$ lo que equivale a $-2x_{1}+8x_{2}-2x_{3}+3x_{1}-11x_{2}+2x_{3}=0$ al sumar los términos semejantes se obtiene que $x_{1}-3x_{2}=0$ por lo que $x_{1}=3x_{2}$. Luego cualquier vector de la forma $(3\lambda,\lambda,\lambda)$ con $\lambda\in\mathbb{R}$ es una solución del sistema homogéneo.

```

En general, este método para resolver un sistema de ecuaciones como \@ref(eq:sistemalineal) consiste en multiplicar por $m$ escalares $c_{1}, c_{2},\cdots ,c_{m}$ cada ecuación del sistema y sumarlas entre si para obtener una *combinación lineal* de las ecuaciones:
\begin{equation}
\begin{array}{ccccccc}
&c_{1}(A_{11}x_{1}&+&\cdots &+&A_{1n}x_{n})=&c_{1}b_{1}\\
&c_{2}(A_{21}x_{1}&+&\cdots &+&A_{2n}x_{n})=&c_{2}b_{2}\\
+&\vdots& &\ddots& &\vdots& \vdots	\\
&c_{m}(A_{m1}x_{1}&+&\cdots &+&A_{mn}x_{n})=&c_{m}b_{m}\\
\cline{2-7}
&(c_{1}A_{11}+\cdots+c_{m}A_{m1})x_{1}&+&\cdots&+&(c_{1}A_{1n}+\cdots +c_{m}A_{mn})x_{n}&=c_{1}b_{1}+\cdots+c_{m}b_{m}
\end{array}
\end{equation}

Es claro que cualquier solución del sistema \@ref(eq:sistemalineal) es solución de la combinación lineal antes descrita.
Ahora bien, si formamos un sistema de $k$ ecuaciones lineales en las que cada una de ellas es una combinación lineal de las $m$ ecuaciones del sistema original, como sigue:
\begin{equation}
\begin{array}{ccccc}
B_{11}x_{1}+&B_{12}x_{2}+&\cdots +&B_{1n}x_{n}=&d_{1}\\
B_{21}x_{1}+&B_{22}x_{2}+&\cdots +&B_{2n}x_{n}=&d_{2}\\
\vdots& &\ddots& \vdots& \vdots	\\
B_{k1}x_{1}+&B_{k2}x_{2}+&\cdots +&B_{kn}x_{n}=&d_{k}
\end{array}
\end{equation}(\#eq:sistema2)

se tiene que $(x_{1},x_{2},\cdots, x_{n})$ es solución de \@ref(eq:sistema2) si lo es del sistema \@ref(eq:sistemalineal). Lo contrario no es necesariamente cierto, sin embargo si las ecuaciones de \@ref(eq:sistemalineal) son combinación lineal de las ecuaciones \@ref(eq:sistema2) entonces podemos estar seguros que toda solución del sistema \@ref(eq:sistema2) es también solución de \@ref(eq:sistemalineal). En este caso diremos que *son sistemas de ecuaciones equivalentes*. Y la observación podemos señalarla así:

```{theorem}
	Sistemas de ecuaciones lineales equivalentes tienen exactamente las mismas soluciones.
```

Lo anterior nos permite buscar las soluciones de cualquier sistema de ecuaciones lineales, buscando un sistema equivalente que sea mas fácil de resolver (por excelencia, trivial). Este método lo explicaremos en la siguiente sección.

## Matrices

En la sección anterior vimos que cuando realizamos combinaciones lineales de ecuaciones lineales, lo que importa son los coeficientes de las ecuaciones originales, siendo las incógnitas prescindibles. Esto nos permite trabajar directamente con los coeficientes para hallar un nuevo sistema lineal equivalente mas sencillo. Por esta razón arreglaremos tales coeficientes en forma rectángular para trabajar con ellos de forma directa. Estos objetos se llaman *matrices*. En la primera parte de esta sección se dará una definición formal y mas general de estos objetos, se definiran algunas operaciones con estos objetos. Para luego volver al problema original, la resolución de sistemas de ecuaciones lineales.

```{definition}
	Para enteros positivos $m$ y $n$, *una matriz $m\times n$ sobre el cuerpo $\mathbb{F}$* es una función $A$ del conjunto de los pares $(i,j)\in\{1,2,\cdots m\}\times\{1,2,\cdots n\}$ en el cuerpo $\mathbb{F}$. El *orden de la matriz $A$* es $m\times n$. Los *elementos de la matriz $A$* son los escalares $A(i,j)=a_{ij}$. Suele representarse como un arreglo rectángular de $m$ filas y $n$ columnas, donde el elemento $a_{ij}$ ocupa el puesto en la fila $i$ y la columna $j$ del arreglo, como sigue 
	\begin{equation*}
		\left[ \begin{array}{cccc}
		a_{11}&a_{12}&\cdots &a_{1n}\\
		a_{21}&a_{22}&\cdots &a_{2n}\\
		\vdots& \ddots& \vdots& \vdots\\
		a_{m1}&a_{m2}&\cdots &a_{mn}
		\end{array}\right] 
	\end{equation*}
	La *$i$-ésima fila de la matriz $A$* es el arreglo $A_{i*}=[a_{i1}\; a_{i2}\;\cdots\; a_{in}]$ (puede entenderse como un vector de $\mathbb{R}^{n}$) y la \textit{$j$-ésima columna de la matriz $A$} es el arreglo $A_{*j}=\left[ \begin{array}{c}a_{1j}\\ 
	a_{2j}\\
	\vdots\\
	a_{mj}
	\end{array}\right]$.

```

```{definition}
	Dadas las matrices $A$ y $B$ de orden $n\times m$, sobre el cuerpo $\mathbb{F}$, *la suma de las matrices $A$ y $B$*, es la matriz $A+B$ formada por los elementos $(a+b)_{ij}=a_{ij}+b_{ij}$ (la función suma $(A+B)(i,j)=A(i,j)+B(i,j)$). También se puede expresar como:
	\begin{equation*}
		\left[ \begin{array}{cccc}
			a_{11} & a_{12} & \cdots & a_{1n}\\
			a_{21} & a_{22} & \cdots & a_{2n}\\
			\vdots & \vdots & \ddots & \vdots\\
			a_{m1} & a_{m2} & \cdots & a_{mn}\\
		\end{array}\right]+
		\left[ \begin{array}{cccc}
		b_{11} & b_{12} & \cdots & b_{1n}\\
		b_{21} & b_{22} & \cdots & b_{2n}\\
		\vdots & \vdots & \ddots & \vdots\\
		b_{m1} & b_{m2} & \cdots & b_{mn}\\
		\end{array}\right]=
		\left[ \begin{array}{cccc}
		a_{11} + b_{11}& a_{12} + b_{12}& \cdots & a_{1n}+ b_{1n}\\
		a_{21} + b_{21} & a_{22} + b_{22} & \cdots & a_{2n} + b_{2n}\\
		\vdots & \vdots & \ddots & \vdots\\
		a_{m1} + b_{m1} & a_{m2} + b_{m2} & \cdots & a_{mn} + b_{mn}\\
		\end{array}\right]
	\end{equation*}

```
```{definition}
	Dada una matriz $A$ de orden $m\times n$ sobre el cuerpo $\mathbb{F}$ y un escalar $\lambda\in\mathbb{F}$. El *producto de la matriz $A$ por el escalar $\lambda$* es la matriz $\lambda A$, donde cada elemento $[\lambda A]_{ij}=\lambda a_{ij}$, es decir:
	\begin{equation*}
		\lambda A=
		\lambda \left[ \begin{array}{cccc}
			a_{11} & a_{12} & \cdots & a_{1n}\\
			a_{21} & a_{22} & \cdots & a_{2n}\\
			\vdots & \vdots & \ddots & \vdots\\
			a_{m1} & a_{m2} & \cdots & a_{mn}\\
		\end{array}\right]=
		\left[ \begin{array}{cccc}
			\lambda a_{11}& \lambda a_{12}& \cdots & \lambda a_{1n}\\
			\lambda a_{21}& \lambda a_{22}& \cdots & \lambda a_{2n}\\
			\vdots & \vdots & \ddots & \vdots\\
			\lambda a_{m1}& \lambda a_{m2}& \cdots & \lambda a_{mn}\\
		\end{array}\right]
	\end{equation*}

```

```{definition}
	Sean $A$ y $B$ matrices de orden $m\times n$ y $n\times p$ respectivamente, el *producto $AB$* es la matriz $C$ de orden $m\times p$ cuyos elementos $ij$ son $[C]_{ij}=\sum_{k=1}^{n} A_{ik}B_{kj}$.

```

```{example}
	Dadas las matrices $A=\left[ \begin{array}{cc}
	1 & 0 \\
	-3 & 1
	\end{array}\right]$, $B=\left[ \begin{array}{ccc}
	5 & -1 & 2 \\
	15 & 4 & 8
	\end{array}\right]$ y $C=\left[ \begin{array}{ccc}
	-2 & 1 & 5\\
	-1 & -5 & 1
	\end{array}\right]$, entonces el producto de la matriz $C$ por el escalar $-2$ es: 
	$$-2 C=\left[ \begin{array}{ccc}
	(-2)(-2) & (-2)(1) & (-2)(5)\\
	(-2)(-1) & (-2)(-5) & (-2)(1)
	\end{array}\right]=\left[ \begin{array}{ccc}
	4 & -2 & -10\\
	2 & 10 & -2
	\end{array}\right]$$.
	El producto $AB$ es la matriz: 
	$$\left[ \begin{array}{cc}
	1 & 0 \\
	-3 & 1
	\end{array}\right]\left[ \begin{array}{ccc}
	5 & -1 & 2 \\
	15 & 4 & 8
	\end{array}\right]=\left[ \begin{array}{ccc}
		5 & -1 & 2\\
		0 & 7 & 2
	\end{array}\right]$$
	Y la suma $$AB+C=\left[ \begin{array}{ccc}
		3 & 0 & 7\\
		-1 & 2 & 3
	\end{array}\right]$$
	Note que no podemos realizar el producto $BA$, pues no está definido. Para realizar un producto de matrices, el número de columnas del primer factor debe ser igual al número de filas del segundo factor.

```

```{example}
	Dadas las matrices $A=\left[ \begin{array}{cc}
	1 & -2 \\
	0 & 1 \\
	-2 & 0
	\end{array}\right]$ y $B=\left[ \begin{array}{ccc}
	1 & 0 & 1 \\
	0 & 2 & 1
	\end{array}\right]$, entonces los productos:
	$AB=\left[ \begin{array}{ccc}
	1 & -4 & -1\\
	0 & 2 & 1 \\
	-2 & 0 & -2
	\end{array}\right]$ y $BA=\left[ \begin{array}{cc}
	-1 & -2  \\
	-2 & 2  
	\end{array}\right]$.\\
	Por otro lado, si se tienen las matrices $C=\left[ \begin{array}{cc}
	1 & -2 \\
	3 & -1 \\
	\end{array}\right]$ y 
	$D=\left[ \begin{array}{cc}
	1 & -2  \\
	-1 & 2 
	\end{array}\right]$, entonces los productos:
	$CD=\left[ \begin{array}{cc}
	3 & -6 \\
	4 & -7 
	\end{array}\right]$ y $DC=\left[ \begin{array}{cc}
	-5 & 0  \\
	5 & 0  
	\end{array}\right]$

```

```{remark}
	En los casos en que expresemos el producto $AB$ sin detallar el orden de las matrices, supondremos que el producto está bien definido. De los ejemplos anteriores podemos ver que aunque los productos $AB$ y $BA$ esten bien definidos, no necesariamente se tiene que $AB=BA$, esto es, el producto de matrices no es conmutativo.

```

Ahora estudiaremos las *operaciones elementales de filas* que pueden aplicarse a una matriz, el fin de aplicar estas operaciones es obtener una matriz equivalente a la original (para obtener sistemas de ecuaciones lineales equivalentes) que corresponda a los coeficientes de un sistema lineal sencillo de resolver. 

```{definition}
	Una matriz $R$ de orden $m\times n$ se llama una *matriz escalón reducida por filas* si:

		(1) El primer elemento no nulo de cada fila no nula es igual a $1$, al cual llamaremos *pivote*.

		(2) las columnas que contienen a un pivote (de cualquier fila), tienen el restos de sus elemento igual a cero.

		(3) toda fila nula de $R$, está debajo de las filas con elementos no nulos.

		(4) suponiendo que las filas no nulas de $R$ son las filas $1,2,\cdots, r$ y que el pivote de la fila $i\leq r$ está en la columna $k_{i}$, entonces $k_{1}< k_{2}< \cdots < k_{r}$.

```

```{remark}
	Una matriz que cumpla las primeras dos condiciones se llama *matriz reducida por filas*.

```

```{example}
	La *matriz identidad* $n\times n$ (cuadrada), definida por la función $$[I]_{ij}=delta_{ij}=\left\{ \begin{array}{cc}
	1 &\mbox{ si } i=j\\
	0 &\mbox{ si } i\neq j
	\end{array}\right. $$
	La función $\delta_{ij}$ es conocida como la *delta de Kronecker*. Es de hacer notar que la matriz identidad se define como una matriz de cualquier orden, mientras que sea cuadrada, es decir, el número de filas es igual al número de columnas.Por ejemplo la matriz identidad de orden $4\times 4$ luce así:
	$$\left[\begin{array}{cccc}
	1 & 0 & 0 & 0\\
	0 & 1 & 0 & 0\\
	0 & 0 & 1 & 0\\
	0 & 0 & 0 & 1
	\end{array} \right]$$
	Las matrices $$\left[\begin{array}{cccc}
	0 & 1 & 0 & 4\\
	0 & 0 & 1 & 5\\
	0 & 0 & 0 & 0
	\end{array} \right] \mbox{ y }
	\left[\begin{array}{ccccc}
	0 & 1 & -3 & 0 & \frac{1}{2}\\
	0 & 0 & 0 & 1 & 2\\
	0 & 0 & 0 & 0 & 0
	\end{array} \right] $$  son matrices escalonadas reducidas.
	Pero estas matrices  
	$$\left[\begin{array}{cccc}
	0 & 1 & 0 & 4\\
	0 & 0 & 1 & 5\\
	0 & 0 & 0 & 2
	\end{array} \right] \mbox{ y }\left[\begin{array}{cccc}
	0 & 1 & 0 & \frac{4}{3}\\
	0 & 0 & 1 & 5\\
	1 & 0 & 0 & -1
	\end{array} \right]$$ no lo son. La primera no lo es ya que el primer elemento no nulo de la última fila no es $1$. La segunda matriz no cumple la definición, el pivote de la tercera fila está en la columna $1$, mientras que el de la primera primera fila está en la columna $2$ ($2\nless 1$).

```

Podremos ver mas adelante que los sistemas de ecuaciones asociadas a matrices escalonadas reducidas son mas fáciles de resolver. Entonces es conveniente hallar una  matriz escalonada reducida que tenga un sistema de ecuaciones equivalente al original para así resolver el sistema fácilmente. 

## Ecuaciones lineales y matrices

Dado un sistema de ecuaciones lineales como \@ref(eq:sistemalineal)
\begin{equation}
\begin{array}{ccccc}
A_{11}x_{1}+&A_{12}x_{2}+&\cdots +&A_{1n}x_{n}=&b_{1}\\
A_{21}x_{1}+&A_{22}x_{2}+&\cdots +&A_{2n}x_{n}=&b_{2}\\
\vdots& &\ddots& \vdots& \vdots	\\
A_{m1}x_{1}+&A_{m2}x_{2}+&\cdots +&A_{mn}x_{n}=&b_{m}
\end{array}
\end{equation}
Podemos representar este sistema de ecuaciones como un sistema matricial $AX=B$, donde $A$ es la matriz de los coeficientes del sistema de ecuaciones, $X$ es una matriz de incógnitas y $B$ una matriz de términos independientes de la siguiente forma:
\begin{equation}
	\left[ \begin{array}{cccc}
		A_{11}&A_{12}&\cdots &A_{1n}\\
		A_{21}&A_{22}&\cdots &A_{2n}\\
		\vdots& \ddots& \vdots& \vdots\\
		A_{m1}&A_{m2}&\cdots &A_{mn}
	\end{array}\right] 
	\cdot
	\left[ \begin{array}{c}
		x_{1}\\
		x_{2}\\
		\vdots\\
		x_{n}
	\end{array}\right] =
	\left[ \begin{array}{c}
		b_{1}\\
		b_{2}\\
		\vdots\\
		b_{n}
	\end{array}\right] 
\end{equation}

```{theorem}
	Sean $A$, $B$ y $C$ matrices sobre el cuerpo de escalares $\mathbb{F}$. Supongamos que los productos $BC$ y $A(BC)$ están definidos. Entonces, $AB$ y $(AB)C$ también están definidos y $A(BC)=(AB)C$.

```

```{proof}
	Como $BC$ y $A(BC)$ están definidos, se tiene que el número de columnas de $B$ es igual al número de filas de $C$ y que el número de columnas de $A$ es igual al número de filas de $BC$ (y por lo tanto, igual al número de filas de $B$). Supongamos que $B$ es de orden $n\times p$, $C$ es de orden $p\times q$ y $A$ de orden $m\times n$, así $A(BC)$ es de orden $m\times q$. Claramente, $AB$ está definida y será de orden $m\times p$, como $C$ es de orden $p\times q$, $(AB)C$ está bien definido y será de orden $m\times q$. Ahora veamos que los productos $A(BC)$ y $(AB)C$ además de tener el mismo orden, coinciden en cada elemento.
	$\begin{array}{rl}
	[A(BC)]_{ij} & =\sum_{r=1}^{n} [A]_{ir}[BC]_{rj}\\
	             & =\sum_{r=1}^{n} [A]_{ir} \sum_{s=1}^{p} [B]_{rs} [C]_{sj}\\
	             & =\sum_{r=1}^{n}\sum_{s=1}^{p} [A]_{ir}[B]_{rs} [C]_{sj}\\
	             & =\sum_{s=1}^{p}\sum_{r=1}^{n} [A]_{ir}[B]_{rs} [C]_{sj}\\
	             & =\sum_{s=1}^{p}(\sum_{r=1}^{n} [A]_{ir}[B]_{rs}) [C]_{sj}\\
	             & =\sum_{s=1}^{p} [AB]_{is} [C]_{sj}\\
	             & =[(AB)C]_{ij}
	\end{array}$

```

Ahora veremos las operaciones elementales por filas que corresponden a hacer combinaciones lineales entre las filas de la matriz de coeficientes (equivalente a hacerlo con las ecuaciones del sistema). Las *\textit{*operaciones elementales por filas* son tres:

	(1) Multiplicar una fila de la matriz $A$ por un escalar no nulo $\lambda$.
	
	(2) Intercambio de dos filas de la matriz $A$.
	
	(3) Sustituír la $i$-\'esima fila de la matriz $A$, por la suma de la fila $r$ mas un múltiplo de la fila $s$-ésima.

Podemos denotar en forma de función (entre fila) las operaciones elementales por fila del siguiente modo.
Si $A$ es una matriz $m\times n$, una operación elemental de filas es una función $e$ que se le aplica a la matriz $A$, asociándole la matriz $e(A)$, que corresponde al resultado de alguna de las operaciones antes descritas. esto es:

	(1) Denotaremos $\lambda e_{r}$ a la operación 
	$[e(A)]_{ij}=\left\{ \begin{array}{cc}
	\lambda [A]_{ij} & \mbox{ si } i=r \\
	\left[A\right]_{ij} & \mbox{ si } i\neq r
	\end{array}\right.$, con $r\leq m$ y $\lambda\neq 0$.
	
	(2) Denotaremos $e_{rs}$ a la operación 
	$[e(A)]_{ij}=\left\{ \begin{array}{cc}
	[A]_{sj} & \mbox{ si } i=r \\
	\left[ A\right]_{rj} & \mbox{ si } i=s \\
	\left[A\right]_{ij} & \mbox{ en otro caso } 
	\end{array}\right.$, con $r\neq s \leq m$.
	
	(3) Denotaremos $\lambda e_{rs}$ a la operación 
	$[e(A)]_{ij}=\left\{ \begin{array}{cc}
	[A]_{ij}+\lambda [A]_{sj} & \mbox{ si } i=r \\
	\left[A\right]_{ij} & \mbox{ si } i\neq r
	\end{array}\right.$, con $r\neq s \leq m$.

Note que cualquiera de las tres operaciones elementales por filas se puede "revertir" con una operación del mismo tipo. Para el primer tipo, basta con multiplicar la misma fila por el inverso de $\lambda$, $\frac{1}{\lambda}$. Para el intercambio de las filas $r$ y $s$ basta volver a intercambiar las filas. Para el tercer tipo de operación, $\lambda A_{rs}$, debemos aplicar $-\lambda A_{rs}$ y regresaremos a la matriz original. Esto es la demostración del siguiente teorema.

```{theorem}
	Para cada operación elemental de filas $e$ existe una operación elemental de filas $e_{1}$ del mismo tipo tal que $e_{1}(e(A))=e(e_{1}(A))=A$. Es decir, cada operación elemental de filas, tiene una operación inversa del mismo tipo.

```

```{definition}
	Si $A$ y $B$ son dos matrices del mismo orden sobre el mismo cuerpo de escalares y $B$ se obtiene de aplicar una cantidad finita de operaciones elementales por filas a la matriz $A$, entonces decimos que *$B$ es equivalente por filas a $A$*.

```

```{remark}
	Del teorema anterior se puede verificar que si una matriz $B$ es equivalente por filas a otra matriz $A$, entonces $A$ es equivalente por filas con $B$. También se puede ver que toda matriz es equivalente por filas a si misma. Por último se puede demostrar que si $A$ es equivalente por filas a $B$ y $B$ es equivalente por filas a $C$, entonces $A$ es equivalente por filas a $C$. De lo anterior, se tiene que la equivalencia por filas es una relación de equivalencia.

```
Como ya lo hemos mencionado, aplicar una operación elemental por filas es equivalente a hacer combinaciones lineales con las ecuaciones del sistema, por lo tanto al obtener matrices equivalentes por filas tendremos sistemas de ecuaciones equivalentes.

```{theorem}
	Si $A$ y $B$ son matrices equivalentes por filas, los sistemas homogeneos de ecuaciones lineales $AX=0$ y $BX=0$ tinen exactamente las mismas soluciones.

```
```{proof}
	Basta suponer que $B$ se obtiene de aplicar una operación elemental $e$ a la matriz $A$. Luego, las ecuaciones del sistema $BX=0$ son combinaciones lineales de las ecuaciones del sistema $AX=0$, por lo que cada solución de $AX=0$ es solución de $BX=0$. Análogamente,  cada solución de $BX=0$ es solución de $AX=0$, ya que $A$ se obtiene al aplicar la operación elemental inversa de $e$ a la matriz $B$.

```
```{example}
	Dada la matriz de coeficientes 
$A=\left[  \begin{array}{cccc}
	2 & -1 & 3 & 2 \\
	1 & 4 & 0 & -1 \\
	2 & 6 & -1 & 5
	\end{array}\right]$
podemos hallar una matriz escalonada reducida por fila equivalente a $A$, de la siguiente forma:
	
	$\left[  \begin{array}{cccc}
	2 & -1 & 3 & 2 \\
	1 & 4 & 0 & -1 \\
	2 & 6 & -1 & 5
	\end{array}\right]   \stackrel{-2 e_{21}}{\longrightarrow}
	\left[	\begin{array}{cccc}
	0 & -9 & 3 & 4 \\
	1 & 4 & 0 & -1 \\
	2 & 6 & -1 & 5
	\end{array}\right]  \stackrel{-2 e_{23}}{\longrightarrow}
	\left[ 
	\begin{array}{cccc}
	0 & -9 & 3 & 4 \\
	1 & 4 & 0 & -1 \\
	0 & -2 & -1 & 7
	\end{array}\right] \stackrel{\frac{-1}{2} e_{3}}{\longrightarrow}
	\left[ 
	\begin{array}{cccc}
		0 & -9 & 3 & 4 \\
		1 & 4 & 0 & -1 \\
		0 & 1 & \frac{1}{2} & \frac{-7}{2}
	\end{array}\right] \stackrel{-4 e_{32}}{\longrightarrow}
	\left[ 
	\begin{array}{cccc}
		0 & -9 & 3 & 4 \\
		1 & 0 & -2 & 13 \\
		0 & 1 & \frac{1}{2} & \frac{-7}{2}
	\end{array}\right] \stackrel{9 e_{31}}{\longrightarrow}
	\left[ 
	\begin{array}{cccc}
		0 & 0 & \frac{15}{2} & \frac{-55}{2} \\
		1 & 0 & -2 & 13 \\
		0 & 1 & \frac{1}{2} & \frac{-7}{2}
	\end{array}\right] \stackrel{\frac{2}{15} e_{1}}{\longrightarrow}
  \left[ 
	\begin{array}{cccc}
		0 & 0 & 1 & \frac{-11}{3} \\
		1 & 0 & -2 & 13 \\
		0 & 1 & \frac{1}{2} & \frac{-7}{2}
	\end{array}\right] \stackrel{2 e_{12}}{\longrightarrow}
	\left[ 
	\begin{array}{cccc}
		0 & 0 & 1 & \frac{-11}{3} \\
		1 & 0 & 0 & \frac{17}{3} \\
		0 & 1 & \frac{1}{2} & \frac{-7}{2}
	\end{array}\right] \stackrel{\frac{-1}{2} e_{13}}{\longrightarrow}
	\left[ 
	\begin{array}{cccc}
	0 & 0 & 1 & \frac{-11}{3} \\
	1 & 0 & 0 & \frac{17}{3} \\
	0 & 1 & 0 & \frac{-5}{3}
	\end{array}\right] \stackrel{e_{12}}{\longrightarrow}
  \left[ 
	\begin{array}{cccc}
	1 & 0 & 0 & \frac{17}{3} \\
	0 & 0 & 1 & \frac{-11}{3} \\
	0 & 1 & 0 & \frac{-5}{3}
	\end{array}\right] \stackrel{e_{32}}{\longrightarrow}
	\left[ 
	\begin{array}{cccc}
	1 & 0 & 0 & \frac{17}{3} \\
	0 & 1 & 0 & \frac{-5}{3} \\
	0 & 0 & 1 & \frac{-11}{3}
	\end{array}\right]$
	
De donde se tiene que los sistemas de ecuaciones lineales

 
$\left\lbrace 
\begin{array}{ccccc}
2x_{1} & -x_{2} & +3x_{3} & +2x_{4} & =0\\
x_{1} & +4x_{2} &  & -x_{4} &=0\\
2x_{1} & +6x_{2} & -x_{3} & +5x_{4} & =0
\end{array}
\right.  $
y

$
\left\lbrace 
\begin{array}{ccccc}
	x_{1} &  &  & +\frac{17}{3}x_{4} &=0 \\
 & x_{2} &  & +\frac{-5}{3}x_{4} &=0 \\
  &  & x_{3} & +\frac{-11}{3}x_{4} &=0 
\end{array} 
\right. $ 
son equivalentes y por lo tanto tienen las mismas soluciones. Del segundo sistema salta a la vista que una solución es de la forma $(\frac{-17}{3}\lambda,\frac{5}{3}\lambda,\frac{11}{3}\lambda,\lambda)$ para cualquier número real $\lambda$.

```

```{example}
	Dada la ecuación con coeficientes en el cuerpo de los números complejos $\mathbb{C}$:
	$ 
	\left\lbrace 
	\begin{array}{ccc}
	-x_{1} & +ix_{2} &=0\\
	-ix_{1} & +3x_{2} &=0\\
	x_{1} & +2x_{2} & =0
	\end{array}
	\right.  $
	
	La matriz de coeficientes es $A=\left[  \begin{array}{cc}
	-1 & i \\
	-i & 3 \\
	1 & 2 
	\end{array}\right]$$ 
	Hallemos una matriz escalonada reducida por fila equivalente a $A$:
	$$\left[  \begin{array}{cc}
	-1 & i \\
	-i & 3 \\
	1 & 2 
	\end{array}\right]   \stackrel{e_{13}}{\longrightarrow}
	\left[	\begin{array}{cc}
	1 & 2 \\
	-i & 3 \\
	-1 & i 
	\end{array}\right]  \stackrel{i e_{12}}{\longrightarrow}
	\left[ 
	\begin{array}{cc}
	1 & 2 \\
	0 & 3+2i \\
	-1 & i 
	\end{array}\right] \stackrel{1 e_{13}}{\longrightarrow}
	\left[ 
	\begin{array}{cc}
	1 & 2 \\
	0 & 3+2i \\
	0 & 2+i 
	\end{array}\right] \stackrel{\frac{1}{3+2i} e_{2}}{\longrightarrow}$
	
	$\left[ 
	\begin{array}{cc}
	1 & 2 \\
	0 & 1 \\
	0 & 2+i 
	\end{array}\right] \stackrel{-2 e_{21}}{\longrightarrow}
	\left[ 
	\begin{array}{cc}
	1 & 0 \\
	0 & 1 \\
	0 & 2+i 
	\end{array}\right] \stackrel{-2-i e_{23}}{\longrightarrow}
	\left[ 
	\begin{array}{cc}
	1 & 0 \\
	0 & 1 \\
	0 & 2+i 
	\end{array}\right]$
	
De donde se tiene que la solución del sistema $AX=0$ es la trivial $(0,0,0)$, ya que $AX=0$ es equivalente al sistema $\left[ \begin{array}{cc}
1 & 0 \\
0 & 1 \\
0 & 2+i 
\end{array}\right] \cdot \left[ \begin{array}{c}
x_{1} \\
x_{2} 
\end{array}\right] = \left[ \begin{array}{c}
0 \\
0 
\end{array}\right]$

```

Dada cualquier matriz $A$ de orden $m\times n$, podemos hallar una matriz equivalente por filas que sea escalonda reducida por filas, realizando un número finito de operaciones por filas según el siguiente algoritmo. Toda fila nula de la matriz se mueven hacia abajo de la matriz por medio de la operación intercambio de filas, de forma que todas ellas queden en las últimas filas de la matriz, es decir, supongamos que existen $r\leq m$ filas no nulas, entonces las últimas filas $r+1, r+2, \cdots, m$ serán las filas de ceros, de forma que el bloque superior $1,2,\cdots, r$ serán las filas no nulas. Luego, considerando esta nueva matriz (la llamaremos $A$, por comodidad). Sea $a_{1k_{1}}$ el primer elemento no nulo de la primera fila ($k_{1}$ es la columna donde aparece el primer elemento no nulo de la fila $1$), si $a_{1k_{1}}=1$, se cumple la condición (1), si no es así, aplicamos la operación $\frac{1}{a_{1k_{1}}} e_{1}$ para hacer que el pivote sea $1$. Ahora, debemos hacer que todo elemento en esa columna sea cero  si está en otra fila (condición (2)), para esto aplicamos la operación $-a_{ik_{1}}e_{i}$ para cada fila $1\neq i\leq r$. Pasamos a la siguiente fila, $A_{2\ast}$, consideramos el primer elemento no nulo de dicha fila, $a_{2k_{2}}$, donde $k_{2}$ es la columna que ocupa. Si $a_{2k_{2}}=1$, no haremos nada, en otro caso, aplicamos la operación $\frac{1}{a_{2k_{2}}} e_{2}$, con esto se cumple la condición (1) para esta fila. Ahora aplicamos $-a_{ik_{2}}e_{i}$ para cada fila $2\neq i\leq r$, con esto se cumple la condición (2). Repetimos este proceso para cada una de las filas no nulas de $A$, es decir, para las filas $1,2,\cdots, r$. Ahora ordenamos las filas no nulas intercambiando filas para lograr que se cumpla la condición (4), es decir, que se cumpla que $p_{1}< p_{2}< \cdots < p_{r}$, llamando $p_{i}$ a la columna del pivote de la fila $i$ luego de aplicar las operaciones elementales por filas. Es claro que estas operaciones son siempre posibles de aplicar a cualquier matriz, en un número finito de pasos.
De lo anterior podemos concluir que siempre podemos reducir una matriz a una escalonada. Podemos expresarlo como un teorema, cuya demostración es el algoritmo anterior.

```{theorem}
	Toda matriz $m\times n$ sobre el cuerpo $\mathbb{F}$ es equivalente por filas a una matriz escalonada reducida por filas.

```

```{theorem}
	Si $A$ es una matriz $m\times n$ con $m<n$, el sistema homogéneo de ecuaciones lineales $AX=0$ tiene una solución no trivial.

```
```{proof}
	Sea $R$ una matriz escalón reducida por filas equivalente por filas a la matriz $A$. Entonces $AX=0$ y $RX=0$ tienen exactamente las mismas soluciones. Supongamos que $R$ tiene $r$ filas no nulas, luego $r<n$, luego el sistema de ecuaciones $RX=0$ consta de $r$ ecuaciones no triviales, a saber, suponiendo que $x_{k_{i}}$ es la incógnita que aparece en la posición del pivote de la fila $i$,
	$$\begin{array}{ccc}
	x_{k_{1}}+& \sum_{j=1}^{n-r}c_{1j}u_{j}=&0\\
	\vdots & & \vdots\\
	x_{k_{r}}+& \sum_{j=1}^{n-r}c_{rj}u_{j}=&0
	\end{array}$$ 
	donde las $n-r$ incógnitas diferentes de $x_{k_{1}}, x_{k_{2}},\cdots, x_{k_{r}}$ las denotamos $u_{1}, u_{2}, \cdots, u_{n-r}$. Note que la incógnita $x_{k_{i}}$ aparece solo en la $i$-ésima ecuación. De esta forma, podemos dar valores arbitrarios a $u_{1}, u_{2}, \cdots, u_{n-r}$ y así hallar una solución no trivial, que a su vez es solución del sistema $AX=0$.

```

Todos los teoremas anteriores hacen referencia a sistemas homogéneos $AX=0$, el cual siempre tiene solución, la solución trivial $X=0$. Cabe preguntar que sucede con los sistemas no homogéneos $AX=B$. Un sistema no homogéneo no tiene necesariamente solución. Estudiemos esto a continuación.
Dado el sistema no homogéneo $AX=B$, con $A$ de orden $m\times n$; consideramos la *matriz aumentada*, $\hat{A}$ de orden $m\times (n+1)$, cuyas primeras $n$ columnas son iguales a las columnas de $A$ y la columna $n+1$ corresponde a $B$, es decir, $\hat{A}_{\ast j}=A_{\ast j}$ para $j\leq n$ y $\hat{A}_{\ast n+1}=B_{1}$. Se aplicará a esta matriz, $\hat{A}$ las mismas operaciones elementales por filas que se le aplican a la matriz $A$ para llevarla a una matriz escalonada reducidas por filas $R$, y así se obtendrá una matriz $\hat{R}$ cuya última fila son los escalares $z_{1}, z_{2},\cdots z_{m}$ (que serán combinaciones lineales de los coeficientes $b_{1}, b_{2},\cdots, b_{m}$). Es claro que los sistemas $AX=B$ y $RX=Z$ tienen las mismas soluciones (la demostración es análoga al de los sistemas homogéneos). Es fácil ver cuando el sistema $\hat{R}X=Z$ tiene solución. Si $\hat{R}$ tiene $r$ filas no nulas, donde el pivote de la fila $i$ está en la columna $k_{i}$ entonces las primeras $r$ ecuaciones expresarán las primeras $r$ incógnitas, $x_{k_{1}}, x_{k_{2}}, \cdots, x_{k_{r}}$ por las $n-r$ incógnitas restantes, $x_{j}$ y los escalares $z_{1}, z_{2},\cdots, z_{r}$. Y las últimas $m-r$ ecuaciones son:
$$\begin{array}{cc}
0= & z_{r+1}\\
\vdots & \vdots\\
0= & z_{m}
\end{array}$$
Por lo tanto, para que el sistema tenga solución debe suceder que $z_{r+1}=z_{r+2}=\cdots=z_{m}=0$. Si esto ocurre entonces las soluciones del sistema se obtienen dando valores arbitrarios a la $n-j$ incógnitas $x_{j}$.

```{example}
	Sea $$A=\left[\begin{array}{ccc}
	1 & -2 & 1\\
	2 & 1 & 1\\
	0 & 5 & -1
	\end{array} \right] $$ la matriz de coeficientes del sistema $AX=B$, donde $B=\left[\begin{array}{c}
	b_{1}\\
	b_{2}\\
	b_{3}
	\end{array} \right]$. Luego la matriz extendida es 
  $\hat{A}=\left[\begin{array}{ccc|c}
	1 & -2 & 1 & b_{1}\\
	2 & 1 & 1 & b_{2}\\
	0 & 5 & -1 & b_{3}
	\end{array} \right]$
  
	Reducimos la matriz:
  
	$\left[\begin{array}{ccc|c}
	1 & -2 & 1 & b_{1}\\
	2 & 1 & 1 & b_{2}\\
	0 & 5 & -1 & b_{3}
	\end{array} \right] \stackrel{-2 e_{12}}{\longrightarrow} 
	\left[\begin{array}{ccc|c}
	1 & -2 & 1 & b_{1}\\
	0 & 5 & -1 & b_{2}-2b_{1}\\
	0 & 5 & -1 & b_{3}
	\end{array} \right] \stackrel{\frac{1}{5} e_{2}}{\longrightarrow}
	\left[\begin{array}{ccc|c}
	1 & -2 & 1 & b_{1}\\
	0 & 1 & -\frac{1}{5} & \frac{1}{5}(b_{2}-2b_{1})\\
	0 & 5 & -1 & b_{3}
	\end{array} \right] \stackrel{ 2e_{21}}{\longrightarrow}
  \left[\begin{array}{ccc|c}
	1 & 0 & \frac{3}{5} & \frac{1}{5}(b_{1}+2b_{2})\\
	0 & 1 & -\frac{1}{5} & \frac{1}{5}(b_{2}-2b_{1})\\
	0 & 5 & -1 & b_{3}
	\end{array} \right] \stackrel{ -5e_{23}}{\longrightarrow}
	\left[\begin{array}{ccc|c}
	1 & 0 & \frac{3}{5} & \frac{1}{5}(b_{1}+2b_{2})\\
	0 & 1 & -\frac{1}{5} & \frac{1}{5}(b_{2}-2b_{1})\\
	0 & 0 & 0 & b_{3}-b_{2}+2b_{1}
	\end{array} \right]$
	Luego, el sistema tiene solución solo si $2b_{1}-b_{2}+b_{3}=0$. Si esta condición se cumple, entonces una solución para el sistema es de la forma:
	$\begin{array}{cc}
	x_{1}= &-\frac{3}{5}x_{3}+\frac{1}{5}(b_{1}+2b_{2})\\
	x_{2}= &\frac{1}{5}x_{3}+\frac{1}{5}(b_{2}-2b_{1})
	\end{array}$ para cualquier valor de $x_{3}$.

```
